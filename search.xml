<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数仓知识</title>
      <link href="2018/12/23/2020-12-18-quannnxu-xiang-mu-4/"/>
      <url>2018/12/23/2020-12-18-quannnxu-xiang-mu-4/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓知识</title>
      <link href="2018/12/16/2020-12-16-quannnxu-xiang-mu-1/"/>
      <url>2018/12/16/2020-12-16-quannnxu-xiang-mu-1/</url>
      
        <content type="html"><![CDATA[<h3 id="一-ERP介绍"><a href="#一-ERP介绍" class="headerlink" title="一.ERP介绍"></a>一.ERP介绍</h3><p>可以理解为一个复杂的丰富的进销存管理系统</p><h4 id="1-1日志管理-pv-uv"><a href="#1-1日志管理-pv-uv" class="headerlink" title="1.1日志管理:pv    uv"></a>1.1日志管理:pv    uv</h4><p>【智能分单】：如收货地址会进行分区域发货，但是收货地址规格不统一，需要预先清洗再智能分单</p><p>清洗：</p><p>甲：上海市    100W</p><p>乙：上海        10W</p><p>四级地址库    五级地址库</p><h4 id="1-2-为什么像京东会布局一线城市的仓库呢？"><a href="#1-2-为什么像京东会布局一线城市的仓库呢？" class="headerlink" title="1.2 为什么像京东会布局一线城市的仓库呢？"></a>1.2 为什么像京东会布局一线城市的仓库呢？</h4><h4 id="1-3-商品类别"><a href="#1-3-商品类别" class="headerlink" title="1.3 商品类别"></a>1.3 商品类别</h4><p>一级类别    二级类别</p><p><a href="https://www.sohu.com/a/222240429_787408">https://www.sohu.com/a/222240429_787408</a></p><p>城市搜索商品类别差异</p><p>如：在酒类、蔬菜、生鲜等各个城市的差异</p><h4 id="1-4-商品-sku"><a href="#1-4-商品-sku" class="headerlink" title="1.4 商品    sku"></a>1.4 商品    sku</h4><h4 id="1-6-采购"><a href="#1-6-采购" class="headerlink" title="1.6 采购"></a>1.6 采购</h4><p>采购订单（做计划）==&gt; 转采购入库</p><p>采购入库信息：</p><p>采购订单==》采购入库</p><h4 id="1-7-销售"><a href="#1-7-销售" class="headerlink" title="1.7 销售"></a>1.7 销售</h4><p>销售订单  (做计划) ==&gt;  转销售出库</p><h4 id="1-8-其他"><a href="#1-8-其他" class="headerlink" title="1.8 其他"></a>1.8 其他</h4><p>其他入库</p><p>其他出库</p><p>产地仓</p><h4 id="1-9-报表分析"><a href="#1-9-报表分析" class="headerlink" title="1.9 报表分析"></a>1.9 报表分析</h4><h4 id="1-10-盘点"><a href="#1-10-盘点" class="headerlink" title="1.10 盘点"></a>1.10 盘点</h4><p>账务与实物之间的衔接点,当账务和实物出现差异时需要查明原因,修正账面库存</p><p>库存表是动态变化的</p><p>0点    封账</p><p>create table 库存_20181216 as select * from 库存表;</p><p>库存_20181216</p><p>库存_20181217</p><p>库存_20181218</p><p>库存_20181219</p><p>库存_20181220表导出到hive数仓</p><p>统计分析 上海仓近七天的库存量</p><h3 id="二-数据仓库"><a href="#二-数据仓库" class="headerlink" title="二.数据仓库"></a>二.数据仓库</h3><h4 id="2-1-数据仓库-DW"><a href="#2-1-数据仓库-DW" class="headerlink" title="2.1 数据仓库    DW"></a>2.1 数据仓库    DW</h4><p>data warehouse</p><p>是为企业所有级别的决策制定的过程,提供所有类型的数据支持的战略集合.</p><h4 id="2-2-数据集市-DM"><a href="#2-2-数据集市-DM" class="headerlink" title="2.2 数据集市    DM"></a>2.2 数据集市    DM</h4><p>data mart</p><p>是满足某个部门某个用户的需求,按照多维的方式进行存储,包括定义维度和计算指标等,生成面向决策分析需求的数据立方体.</p><p>区别:</p><table><thead><tr><th align="center"></th><th align="center">数据仓库</th><th>数据集市</th></tr></thead><tbody><tr><td align="center">数据来源</td><td align="center">业务系统数据库、日志</td><td>数据仓库</td></tr><tr><td align="center">范围</td><td align="center">企业</td><td>部门</td></tr></tbody></table><p>3.数据仓库分层</p><p>mysql.order–&gt;Sqoop–&gt;Hive中的第一层</p><p>每天全量抽取    hive分区表</p><p>流水表–&gt;Sqoop–&gt;Hive第一层</p><p>每天抽取只抽当天的流水</p><p>分层的好处:</p><p>1.复杂的问题简单化:将一个复杂的任务分解多个步骤来完成,每一次只处理单一的步骤,比较简单,且方便定位问题.</p><p>2.减少重复开发:规范数据分层,通过中间层到的数据,能够减少重复计算.</p><p>3.隔离原始数据:数据的敏感,是的真实数据与统计数据解耦.</p><p>离线数据仓库:</p><p>ODS层:原始数据层(业务表结构一致),存放原始数据,不做任何处理</p><p>DWD(data warehouse detail)层:明细层(业务表结构一致,数据量差异,汇总),对ODS层数据进行清洗(脏数据,空值处理),则根据【主题】定义好【事实表】和【维度表】，保存最细粒度的数据，可以等价于业务数据库的表。</p><p>DWS(data warehouse service)层：轻度汇总层，以DWD层为基础轻度汇总，做成宽表，如订单表</p><p>ADS(APP application data store)层：应用层，按主题提供统计数据.</p><p>DWD层相当于业务系统的分布式表</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓知识之数仓分层建模</title>
      <link href="2018/12/16/2020-12-17-quannnxu-xiang-mu-2/"/>
      <url>2018/12/16/2020-12-17-quannnxu-xiang-mu-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1-离线分层"><a href="#1-离线分层" class="headerlink" title="1.离线分层"></a>1.离线分层</h4><p>ODS</p><p>DWD</p><p>DWS</p><p>ADS</p><p>数据分层的数据量其实是一个倒三角型,越到后面数据量越精简.</p><h4 id="2-命名规范-表的命名-字段的命名"><a href="#2-命名规范-表的命名-字段的命名" class="headerlink" title="2.命名规范(表的命名,字段的命名)"></a>2.命名规范(表的命名,字段的命名)</h4><h5 id="2-1-1个database的情况"><a href="#2-1-1个database的情况" class="headerlink" title="2.1 1个database的情况"></a>2.1 1个database的情况</h5><p>表前缀    ods    dwd    dws    ads</p><p>二级业务     dbname.ods_oa    OA系统</p><p>​                    dbname.ods_erp    ERP系统</p><p>​                    dbname.ods_crm    CRM系统</p><h5 id="2-2-多个database的情况"><a href="#2-2-多个database的情况" class="headerlink" title="2.2 多个database的情况"></a>2.2 多个database的情况</h5><p>ods.oa_    ods.erp_</p><p>dwd.oa_    dwd.erp_</p><p>dws.根据业务需求</p><p>ads.</p><p>目的就是很明确的知道这个表处于哪个分层,哪个系统</p><h5 id="2-3-统一字段名称"><a href="#2-3-统一字段名称" class="headerlink" title="2.3 统一字段名称"></a>2.3 统一字段名称</h5><p>对于同一的含义的字段名称,如orderno、orderid</p><p>在<strong>dws层</strong>统一规范，比如订单号就叫order_no</p><h4 id="3-主表明细表"><a href="#3-主表明细表" class="headerlink" title="3.主表明细表"></a>3.主表明细表</h4><p>如订单主表和订单明细表,同一单订单会有多个商品,但是主表只展示订单ID,创建时间,订单金额等信息,而订单明细表则保存该订单的具体商品信息,如数量,单价,名称等详细信息.</p><p><strong>表的三范式</strong>:</p><p>第一范式：确保每一列的原子性 具体来说就是将数据库表中的所有字段都设置为不可拆分的原子值</p><p>第二范式：确保每列都和主键相关</p><p>具体来说就是数据库表中的每一列都和主键想关而不是部分相关，比如一张表中是以商品编号和订单编号作为联合主键的，而商品价格和数量，单位等信息都仅仅和商品编号相关而和订单编号并不相关，这样就不满足数据库建表的三范式</p><p>第三范式：确保每列都和主键直接相关而不是间接相关</p><p>比如两张数据库表是以一个外键进行关联的，则第二张表中的信息字段不能出现在第一张表中去。</p><h4 id="4-数据建模"><a href="#4-数据建模" class="headerlink" title="4.数据建模"></a>4.数据建模</h4><p>数据仓库模型的设计,其中的模型层次并不是越多越好.</p><p>我们经常会构建多层的数据结构,预先对不同的粒度的数据做预先汇总,以方便在使用时,以最小的计算代价获得计算结果,反之会造成数据处理流程太长,步骤较多,问题追溯困难.</p><p>合理的层次设计,以及计算成本、人力成本之间的平衡是一个好的数仓架构的表现.</p><h4 id="5-维度建模"><a href="#5-维度建模" class="headerlink" title="5.维度建模:"></a>5.维度建模:</h4><p><strong>事实表</strong>:who where when what（事实发生的数据,如下了一个订单的订单表:time,id,商品）</p><p><strong>维度表</strong>:用户信息、客户信息、商品信息（基本不变/少量变化的数据）</p><p><strong>星型模型</strong>:一个事实表及多个维度表（非正规化描述）构成</p><p><img src="https://i.loli.net/2020/12/22/mEdMn5wxQqP9olY.png"></p><p><strong>雪花模型</strong>：是星型模型的扩展，区别是将非正规化维度表规范化，进一步分解到附加表</p><p><img src="https://i.loli.net/2020/12/22/9l6QRFSBeHomWPy.png"></p><p><strong>星座模型</strong>：是星型模型的延伸，是基于多个事实表，共享维度信息的</p><p><img src="https://i.loli.net/2020/12/22/ivXgcYqFanj8EWJ.png"></p><h4 id="6-离线数仓架构"><a href="#6-离线数仓架构" class="headerlink" title="6.离线数仓架构"></a>6.离线数仓架构</h4><p>Oracle/MySQL        –&gt;sqoop/datax/kettle        –&gt;HDFS/Hive        –&gt;hql+UDF函数        –&gt;Superset BI</p><p>业务数据库                    抽数工具                                数仓                        业务指标处理            BI展示</p><p>调度工具:rundeck</p><h4 id="7-环境准备"><a href="#7-环境准备" class="headerlink" title="7.环境准备"></a>7.环境准备</h4><p>hadoop</p><p>hive</p><p>sqoop</p><p>erp+mysql erp</p><h4 id="8-分层流程图"><a href="#8-分层流程图" class="headerlink" title="8.分层流程图"></a>8.分层流程图</h4><p><strong>全量表</strong>    每天都抽取全量 ods  dwd</p><p><strong>流水表</strong>    ods每天都抽取当天的数据 ，dwd进行 union合并  </p><p><strong>拉链表</strong>    i/u/d(逻辑删除)+主键+更新时间字段<br>                ods每天都抽取当天变化的数据 ，dwd 进行 union合并  ，比较复杂 进行拉链写法</p><p>DWS:<br>1.事实表每天调度只取【业务周期】数据，<br>比如电商下订单发货 签收，最多是3天，那么取最近1个月的数据，极大包含业务周期数据：开始–》结束。<br>因为结束后数据基本不会变更删除，无需再把历史数据再一天天重复join group复杂计算,无谓的计算。<br>hive：默认不开启update，不支持update ，且本来hive应用场景就是离线<br>hbase：支持update，应用场景就是实时场景 </p><p>2.维度表每天全量抽取</p><p>3.dws表应该如何设计？<br>3.1 假如数据源是insert（物流表  log表），<br>按天分区，每个分区就截止当天的全量数据</p><p>3.2 假如数据源是dml类型<br>按天分区，每个分区就截止当天的全量数据<br>insert overwrite …<br>select  .. from a<br>left join  b<br>left join c</p><p>4.场景 :给ADS供数据 和 ADHOC场景</p><p>ADS：<br>1.根据指标，从3 DWS层(+2 DWD层)直接再次汇总<br>2.场景：BI 、rest api</p><p>总结思考：<br>1.表类型： i、dml(物理删除、逻辑删除)<br>2.数据抽取方式: 全量、增量(流水表、拉链表)<br>3.无论离线还是实时数仓，指标加工要考虑本身的业务生命周期<br>4.是否要考虑数据延时的情况？<br>假如定在0点准时去离线抽取，是否合理？<br>订单主表       订单明细表<br>19号23:59:59   20号00:00:03</p><p>建议：0:5min</p><p>5.hive默认不开启update delete操作，请问既然有这个特性，为什么<br>业界各大公司都不会去开启？</p><p>6.如果开启update delete操作，是不是可以先删除匹配的旧数据，再插入</p><p>7.如果效仿hbase put语法，就完美了，因为这个语法很强大<br>如果库里没有数据就执行插入insert，如果有数据就执行覆盖更新update。<br>这样会极大简化我们的设计和操作。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓知识</title>
      <link href="2018/12/16/2020-12-18-quannnxu-xiang-mu-3/"/>
      <url>2018/12/16/2020-12-18-quannnxu-xiang-mu-3/</url>
      
        <content type="html"><![CDATA[<p>1.表设计</p><p>1.1 主键字段</p><p>1.2 更新时间字段</p><p>1.3 逻辑删除</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础之数据导出Sqoop</title>
      <link href="2018/12/13/2020-12-13-quannnxu-hive5/"/>
      <url>2018/12/13/2020-12-13-quannnxu-hive5/</url>
      
        <content type="html"><![CDATA[<h4 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a><strong>Sqoop</strong></h4><p>RDBMS    ==&gt;    Hadoop(HDFS/Hive/HBase…)</p><p>Hadoop(HDFS/Hive/HBase…)    ==&gt;    RDBMS</p><p>以Hadoop作为参照物,如果数据是进Hadoop的,那就是导入,反之就是导出</p><p>import导入</p><p>export导出</p><p>1.4.7 版本  Sqoop1<br>1.99.7 版本 Sqoop2</p><p><strong>配置Sqoop:</strong></p><p>$SQOOP_HOME/conf/sqoop-env.sh<br>export HADOOP_COMMON_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2<br>export HADOOP_MAPRED_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.16.2<br>export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.16.2</p><p>拷贝MySQL驱动包到$SQOOP_HOME/lib/目录下</p><p><strong>系统环境变量:</strong><br>export SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.16.2<br>export PATH=$SQOOP_HOME/bin:$PATH<br>export SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.16.2<br>export PATH=$SQOOP_HOME/bin:$PATH</p><p>[hadoop@warehouse001 bin]$ sqoop help</p><p>Available commands:<br>  codegen            Generate code to interact with database records<br>  create-hive-table  Import a table definition into Hive<br>  <strong>eval</strong>               Evaluate a SQL statement and display the results<br>  <strong>export             Export an HDFS directory to a database table</strong><br>  help               List available commands<br>  <strong>import             Import a table from a database to HDFS</strong><br>  import-all-tables  Import tables from a database to HDFS<br>  import-mainframe   Import datasets from a mainframe server to HDFS<br>  <strong>job</strong>                Work with saved jobs<br>  <strong>list-databases     List available databases on a server</strong><br>  <strong>list-tables        List available tables in a database</strong><br>  merge              Merge results of incremental imports<br>  metastore          Run a standalone Sqoop metastore<br>  version            Display version information</p><p>查看mysql数据库</p><pre><code>sqoop list-databases \--connect jdbc:mysql://localhost:3306 \--username root \--password &#39;xxxxx&#39;</code></pre><p>查看表:</p><pre><code>sqoop list-tables \--connect jdbc:mysql://localhost:3306/hive_metadata \--username root \--password &#39;xxxxx&#39;</code></pre><p>sqoop import <br>–connect jdbc:mysql://localhost:3306/hive_metadata \    jdbc的URL<br>–username root \        –用户<br>–password ‘xxxxx’ <br>–delete-target-dir \        –删除目标文件夹<br>–columns “EMPNO,ENAME,JOB,SAL,COMM” \        –指定字段<br>–mapreduce-job-name EmpFromMySQL2HDFS \    –修改mapreduce任务名称<br>–table emp <br>–null-string ‘’ \        –指定字符串替换null<br>–null-non-string 0 \        –指定字符替换非string类型字段<br>–fields-terminated-by ‘\t’ \    –指定分隔符<br>–where ‘SAL&gt;2000’ <br>–target-dir EMP_COLUMN</p><p>-e   “sql语句” 指定导出条件</p><p>-e和–table不能同时使用</p><p>-e sql语句后需要加$CONDITIONS</p><p>如果是””加sql语句则需要转义\$CONDITIONS”</p><p>导出没有主键的表,则需要设置参数–split-by 字段或者-m 1</p><p>原因sqoop默认开启的是4个map,默认是按主键切分任务的</p><p>–options-file 文件</p><p>文件格式:</p><p>import<br>–connect<br>jdbc:mysql://localhost:3306/mydatabase<br>–username<br>root<br>–password<br>‘xxxx’<br>–delete-target-dir<br>–columns<br>“EMPNO,ENAME,JOB,SAL,COMM”<br>–mapreduce-job-name<br>EmpFromMySQL2HDFS<br>–table<br>emp<br>–target-dir<br>EMP_COLUMN</p><p><strong>sqoop job –create封装到job中</strong></p><p>sqoop job –create my_job  <br>–import –connect jdbc:mysql://localhost:3306/mydatabase <br>–username root <br>–password ‘xxxx’ <br>–delete-target-dir <br>–table product_info <br>–split-by product_id</p><p>查看job</p><p>sqoop job -list </p><p>执行job</p><p>sqoop job -exec ruozedata_job</p><p>删除job</p><p>sqoop job -delete ruozedata_job</p><p><strong>RDBMS导入到Hive中:</strong></p><p>sqoop import <br>–connect jdbc:mysql://localhost:3306/mydatabase <br>–username root <br>–password ‘xxxxx’ <br>–delete-target-dir <br>–hive-database hive<br>–hive-import <br>–hive-overwrite <br>–hive-table emp_column <br>–columns “EMPNO,ENAME,JOB,SAL,COMM” <br>–mapreduce-job-name EmpFromMySQL2Hive <br>–table emp <br>–null-string ‘’ <br>–null-non-string 0 <br>–fields-terminated-by ‘\t’ </p><p>需提前创建好表结构</p><p>分区表需要加:</p><p>–hive-partition-key ‘day’ \            分区字段<br>–hive-partition-value ‘yyyyMMdd’ \        分区字段值</p><p>导出数据</p><p>sqoop export <br>–connect jdbc:mysql://localhost:3306/product <br>–username root <br>–password ‘xxxxx’ <br>–table city_info <br>–fields-terminated-by ‘\t’ <br>–export-dir /user/hadoop/emp            –导出文件的路径</p><p><strong>案例:</strong></p><p>city_info.sql  product_info.sql  user_click.txt</p><p>城市表：MySQL<br>商品表：MySQL<br>用户行为日志：Hive</p><p>create table user_click(<br>user_id string,<br>session_id string,<br>time string,<br>city_id int,<br>product_id int<br>) partitioned by(day string)<br>row format delimited fields terminated by ‘,’;    </p><p>按照区域求最受欢迎商品的TopN</p><p>1.首先使用sqoop导出数据到Hive中</p><pre><code>sqoop import \--connect jdbc:mysql://localhost:3306/product  \--username root --password &#39;xxxxx&#39; \--table city_info \--delete-target-dir \--hive-import  --hive-database product --hive-table city_info --hive-overwrite  \--fields-terminated-by &#39;\t&#39; \--split-by city_id \-m 2 \--verbose</code></pre><pre><code>sqoop import --connect jdbc:mysql://localhost:3306/product \--username root --password &#39;xxxxx&#39; \--table product_info \--delete-target-dir \--hive-import --hive-database product --hive-table product_info --hive-overwrite \--fields-terminated-by &#39;\t&#39; \--split-by product_id \-m 2</code></pre><p>city_info.city_id       city_info.city_name     city_info.area<br>1       BEIJING NC<br>2       SHANGHAI        EC<br>3       NANJING EC</p><p>product_info.product_id product_info.product_name       product_info.extend_info<br>1       product1        {“product_status”:1}<br>2       product2        {“product_status”:1}<br>3       product3        {“product_status”:1}</p><p>user_click.user_id      user_click.session_id   user_click.time user_click.city_id      user_click.product_id    user_click.day<br>95      2bf501a7637549c89cf55342331b15db        2016-05-05 21:01:56     1       72      2016-05-05<br>95      2bf501a7637549c89cf55342331b15db        2016-05-05 21:52:26     1       68      2016-05-05<br>95      2bf501a7637549c89cf55342331b15db        2016-05-05 21:17:03     1       40      2016-05-05</p><p>作业:分组TopN的结果export到mysql</p><p>一.获取商品信息及城市区域信息</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">table</span> tmp_product_basic_info<span class="token punctuation">;</span><span class="token keyword">create</span> <span class="token keyword">table</span> tmp_product_basic_info <span class="token keyword">as</span><span class="token keyword">select</span>u<span class="token punctuation">.</span>product_id<span class="token punctuation">,</span>u<span class="token punctuation">.</span>city_id<span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">.</span>city_name<span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">.</span>area<span class="token keyword">from</span> user_click u<span class="token keyword">join</span> <span class="token punctuation">(</span><span class="token keyword">select</span> city_id<span class="token punctuation">,</span>city_name<span class="token punctuation">,</span>area <span class="token keyword">from</span> city_info<span class="token punctuation">)</span> <span class="token number">c</span><span class="token keyword">on</span> <span class="token number">c</span><span class="token punctuation">.</span>city_id<span class="token operator">=</span>u<span class="token punctuation">.</span>city_id<span class="token punctuation">;</span></code></pre><p><img src="https://i.loli.net/2020/12/21/8ygt4VsP31irkxv.png"></p><p>二.获取区域的商品点击次数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">table</span> tmp_area_product_click_cnt<span class="token punctuation">;</span><span class="token keyword">create</span> <span class="token keyword">table</span> tmp_area_product_click_cnt <span class="token keyword">as</span><span class="token keyword">select</span> t<span class="token punctuation">.</span>product_id<span class="token punctuation">,</span>t<span class="token punctuation">.</span>area<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> click_cnt<span class="token keyword">from</span> tmp_product_basic_info t <span class="token keyword">join</span> user_click <span class="token number">c</span> <span class="token keyword">on</span><span class="token number">c</span><span class="token punctuation">.</span>product_id<span class="token operator">=</span>t<span class="token punctuation">.</span>product_id<span class="token keyword">group</span> <span class="token keyword">by</span> t<span class="token punctuation">.</span>product_id<span class="token punctuation">,</span>t<span class="token punctuation">.</span>area<span class="token punctuation">;</span></code></pre><p><img src="https://i.loli.net/2020/12/21/poYCckqfmvT42rK.png"></p><p>三.获取完整的商品点击次数信息</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">table</span> tmp_product_full_click_cnt<span class="token punctuation">;</span><span class="token keyword">create</span> <span class="token keyword">table</span> tmp_product_full_click_cnt <span class="token keyword">as</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>product_id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>area<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>click_cnt<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>product_name<span class="token punctuation">,</span><span class="token keyword">if</span><span class="token punctuation">(</span>get_json_object<span class="token punctuation">(</span>extend_info<span class="token punctuation">,</span><span class="token string">'$.product_status'</span><span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">=</span> <span class="token string">'0'</span><span class="token punctuation">,</span><span class="token string">'self'</span><span class="token punctuation">,</span><span class="token string">'third'</span> <span class="token punctuation">)</span> <span class="token keyword">status</span><span class="token keyword">from</span> tmp_area_product_click_cnt <span class="token number">a</span><span class="token keyword">join</span> product_info <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>product_id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>product_id<span class="token punctuation">;</span></code></pre><p><img src="https://i.loli.net/2020/12/21/hUnDfFR8ljNGCXQ.png"><br>获取json格式的value</p><p>select extend_info,get_json_object(extend_info,’$.product_status’) from product_info limit 5;</p><p>四.分组topN</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">table</span> product_click_top<span class="token punctuation">;</span><span class="token keyword">create</span> <span class="token keyword">table</span> product_click_top <span class="token keyword">as</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>product_id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>area<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>click_cnt<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>product_name<span class="token punctuation">,</span><span class="token keyword">status</span><span class="token punctuation">,</span>row_number<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">over</span><span class="token punctuation">(</span><span class="token keyword">partition</span> <span class="token keyword">by</span> area <span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">.</span>click_cnt <span class="token keyword">desc</span><span class="token punctuation">)</span> rank<span class="token keyword">from</span> tmp_product_full_click_cnt <span class="token number">a</span><span class="token punctuation">)</span> <span class="token number">b</span><span class="token keyword">where</span> <span class="token number">b</span><span class="token punctuation">.</span>rank<span class="token operator">&lt;=</span><span class="token number">3</span><span class="token punctuation">;</span></code></pre><p><img src="https://i.loli.net/2020/12/21/vmclfgHBVFwKpTh.png"></p><p>导出数据到mysql:</p><pre><code>sqoop export \--connect jdbc:mysql://localhost:3306/product \--username root \--password &#39;ruozedata&#39; \--table product_click_top \--fields-terminated-by &#39;\001&#39; \--export-dir /user/hive/warehouse/product.db/product_click_top \--verbose</code></pre><p>注意指定分隔符,hive默认是\001,mysql表需要自己预先创建好.</p><p><img src="https://i.loli.net/2020/12/21/58bQW7XHYUTOVks.png"></p><p>常见的sqoop报错:</p><p><a href="http://blog.itpub.net/25854343/viewspace-2565234/">http://blog.itpub.net/25854343/viewspace-2565234/</a></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础之Hive DML语法</title>
      <link href="2018/12/10/2020-12-07-quannnxu-hive3/"/>
      <url>2018/12/10/2020-12-07-quannnxu-hive3/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML</a></p><h4 id="Loading-files-into-tables"><a href="#Loading-files-into-tables" class="headerlink" title="Loading files into tables"></a>Loading files into tables</h4><p>语法:</p><pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT &#39;inputformat&#39; SERDE &#39;serde&#39;] (3.0 or later)</code></pre><p>​    LOAD DATA:加载数据</p><p>​    LOCAL:可选    从本地加载到Hive表</p><p>​    INPATH:加载数据的路径</p><p>​    OVERWRITE:可选</p><p>​        有参数就是覆盖,没有参数就是追加</p><p>​    INTO TABLE:加载到那张表</p><p>如:LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE EMP;</p><h4 id="Inserting-data-into-Hive-Tables-from-queries"><a href="#Inserting-data-into-Hive-Tables-from-queries" class="headerlink" title="Inserting data into Hive Tables from queries"></a>Inserting data into Hive Tables from queries</h4><p>语法:</p><pre><code>INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;</code></pre><p>如:INSERT OVERWRITE TABLE emp3 select * from emp;</p><p>hive (hive)&gt; INSERT OVERWRITE TABLE emp3 select * from emp;<br>FAILED: SemanticException [Error 10001]: Line 1:23 Table not found ‘emp3’</p><p><strong>这种插入方式的前提是表得存在</strong></p><p>create table emp3 like emp;快速创建表结构</p><p>create table dept (</p><p>deptno int,</p><p>dname string,</p><p>loc string</p><p>)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/dept.txt’ OVERWRITE INTO TABLE dept;</p><p>insert into table Dept values (999,’DEV’,’BJ’),(1000,’QA’,’SH’);</p><p><img src="https://i.loli.net/2020/12/11/RJw6pxDNIjiasWg.png"></p><p><strong>这种插入会出现小文件,所以我们一般不用insert values的方式</strong></p><h4 id="Writing-data-into-the-filesystem-from-queries"><a href="#Writing-data-into-the-filesystem-from-queries" class="headerlink" title="Writing data into the filesystem from queries"></a>Writing data into the filesystem from queries</h4><pre><code>INSERT OVERWRITE [LOCAL]   directory1  [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0)  SELECT ... FROM ...</code></pre><p>指定分隔符:</p><p>INSERT OVERWRITE  LOCAL DIRECTORY ‘/home/hadoop/data/‘ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ select * from dept ;</p><p>不指定分隔符:</p><p>INSERT OVERWRITE  LOCAL DIRECTORY ‘/home/hadoop/data/‘  select * from dept ;</p><p>get HDFS文件:</p><p>hdfs dfs -get /user/hive/warehouse/hive.db/dept/*.txt /home/hadoop/data/</p><p>hive -e:</p><p>hive -e “select * from hive.dept” &gt; dept.txt</p><h4 id="Import-Export"><a href="#Import-Export" class="headerlink" title="Import/Export"></a>Import/Export</h4><h5 id="Export-Syntax"><a href="#Export-Syntax" class="headerlink" title="Export Syntax"></a>Export Syntax</h5><pre><code>EXPORT TABLE tablename [PARTITION (part_column=&quot;value&quot;[, ...])]  TO &#39;export_target_path&#39; [ FOR replication(&#39;eventid&#39;) ]</code></pre><h5 id="Import-Syntax"><a href="#Import-Syntax" class="headerlink" title="Import Syntax"></a>Import Syntax</h5><pre><code>IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=&quot;value&quot;[, ...])]]  FROM &#39;source_path&#39;  [LOCATION &#39;import_target_path&#39;]</code></pre><p>适合做库表迁移,导出包含元数据的数据.</p><p>EXPORT TABLE hive.emp TO “/home/hadoop/data”;</p><p>IMPORT TABLE hive.emp_import FROM ‘/home/hadoop/data’;</p><h4 id="Drop-Truncate-Table"><a href="#Drop-Truncate-Table" class="headerlink" title="Drop/Truncate Table"></a>Drop/Truncate Table</h4><p><strong>truncate只删除元数据,不删除数据文件.</strong></p><p><strong>外部表不能truncate.</strong></p><h4 id="Select-Syntax"><a href="#Select-Syntax" class="headerlink" title="Select Syntax"></a>Select Syntax</h4><pre><code>[WITH CommonTableExpression (, CommonTableExpression)*]    (Note: Only available starting with Hive 0.13.0)SELECT [ALL | DISTINCT] select_expr, select_expr, ...  FROM table_reference  [WHERE where_condition]  [GROUP BY col_list]  [ORDER BY col_list]  [CLUSTER BY col_list    | [DISTRIBUTE BY col_list] [SORT BY col_list]  ] [LIMIT [offset,] rows]</code></pre><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-SelectSyntax">Select Syntax</a><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-WHEREClause">WHERE Clause</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-ALLandDISTINCTClauses">ALL and DISTINCT Clauses</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-PartitionBasedQueries">Partition Based Queries</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-HAVINGClause">HAVING Clause</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-LIMITClause">LIMIT Clause</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-REGEXColumnSpecification">REGEX Column Specification</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-MoreSelectSyntax">More Select Syntax</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础之Hive分区和常用函数</title>
      <link href="2018/12/10/2020-12-12-quannnxu-hive4/"/>
      <url>2018/12/10/2020-12-12-quannnxu-hive4/</url>
      
        <content type="html"><![CDATA[<h4 id="1-PARTITIONED-BY"><a href="#1-PARTITIONED-BY" class="headerlink" title="1.PARTITIONED BY"></a>1.PARTITIONED BY</h4><h5 id="1-1-分区表"><a href="#1-1-分区表" class="headerlink" title="1.1 分区表"></a>1.1 分区表</h5><p>​    如:操作日志表    需记录时间,事件等信息,数据量大</p><p>​    所以就需要分区</p><p>语法:</p><p>[PARTITIONED BY (col_name data_type [COMMENT col_comment], …)]</p><h5 id="1-2-建表"><a href="#1-2-建表" class="headerlink" title="1.2 建表:"></a>1.2 建表:</h5><p>create table dept_partition(</p><p>deptno int,</p><p>dname string,</p><p>loc string</p><p>)partitioned by (day string)</p><p>row format delimited fields terminated by ‘\t’;</p><h5 id="1-3-加载数据"><a href="#1-3-加载数据" class="headerlink" title="1.3 加载数据"></a>1.3 加载数据</h5><p>load data local inpath ‘/home/hadoop/data/dept_20300901.txt’ overwrite into table dept_partition partition (day=20300901);</p><p>hive (hive)&gt; select * from dept_partition;<br>OK<br>dept_partition.deptno   dept_partition.dname    dept_partition.loc      dept_partition.day<br>10      ACCOUNTING      NEW YORK        20300901<br>20      RESEARCH        DALLAS  20300901<br>30      SALES   CHICAGO 20300901<br>40      OPERATIONS      BOSTON  20300901<br>Time taken: 0.17 seconds, Fetched: 4 row(s)</p><p><strong>20300901这一列的数据是不存在HDFS上的</strong></p><p>分区表查询一般都要加分区字段:</p><p>select * from dept_partition where day = ‘20300901’;</p><p>分区表的分区元数据信息维护在partitions表中:</p><p>tbls表信息:</p><p><img src="https://i.loli.net/2020/12/14/q7eRKVshm16StPE.png"></p><p>partitions表信息:</p><p><img src="https://i.loli.net/2020/12/14/YSHcVox27wjsMrB.png"></p><h5 id="1-4-也可以先创建hdfs路径-再数据put到指定目录下"><a href="#1-4-也可以先创建hdfs路径-再数据put到指定目录下" class="headerlink" title="1.4 也可以先创建hdfs路径,再数据put到指定目录下"></a>1.4 也可以先创建hdfs路径,再数据put到指定目录下</h5><p>hdfs dfs -mkdir -p /user/hive/warehouse/hive.db/dept_partition/day=20300904</p><p>hdfs dfs -cp /user/hive/warehouse/hive.db/dept_partition/day=20300903/* /user/hive/warehouse/hive.db/dept_partition/day=20300904/</p><p>但是这种操作插入数据需要刷新元数据.</p><p>语法:</p><pre><code>MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];</code></pre><p>msck repair table dept_partition;</p><p>这种操作会非常消耗资源,一般不建议用,可以使用下面的方式去修改分区信息:</p><pre><code>ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION &#39;location&#39;][, PARTITION partition_spec [LOCATION &#39;location&#39;], ...];</code></pre><h5 id="1-5-删除分区"><a href="#1-5-删除分区" class="headerlink" title="1.5 删除分区"></a>1.5 删除分区</h5><pre><code>ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]  [IGNORE PROTECTION] [PURGE];            -- (Note: PURGE available in Hive 1.2.0 and later, IGNORE PROTECTION not available 2.0.0 and later)</code></pre><p>alter table dept_partition drop partition (day=’20300904’);</p><h5 id="1-6-查看分区信息"><a href="#1-6-查看分区信息" class="headerlink" title="1.6 查看分区信息:"></a>1.6 查看分区信息:</h5><p>show partitions dept_partition;</p><p>insert在分区别里也可以使用</p><p>语法:</p><p>insert overwrite table dept_partiton partition(day = ‘20300904’);</p><h5 id="1-7-多级分区"><a href="#1-7-多级分区" class="headerlink" title="1.7 多级分区"></a>1.7 多级分区</h5><p>在数据量特别大的时候坑需要多级分区,在分区关键字后面再加其他字段即可</p><p>如:partitioned by (day string,hour string)</p><h5 id="1-8-静态分区vs动态分区"><a href="#1-8-静态分区vs动态分区" class="headerlink" title="1.8 静态分区vs动态分区"></a>1.8 静态分区vs动态分区</h5><p>上述都是静态分区</p><p>动态分区:</p><p>案例:将emp表中的数据按照deptno将数据插入到emp对应的分区表中</p><p>create table emp_partition(<br>  <code>empno</code> int,<br>  <code>ename</code> string,<br>  <code>job</code> string,<br>  <code>mgr</code> int,<br>  <code>hiredate</code> string,<br>  <code>sal</code> double,<br>  <code>comm</code> double)<br>partitioned by(deptno int)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>插入数据:</p><p>insert into table emp_partition partition(deptno=30)<br>select empno,ename,job,mgr,hiredate,sal,comm from emp where deptno=30;</p><p>如果部门很多很多时,这样执行肯定是有问题的,所以就引进了动态分区.</p><p>create table emp_dynamic_partition(<br>  <code>empno</code> int,<br>  <code>ename</code> string,<br>  <code>job</code> string,<br>  <code>mgr</code> int,<br>  <code>hiredate</code> string,<br>  <code>sal</code> double,<br>  <code>comm</code> double)<br>partitioned by(deptno int)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>插入数据:</p><p>insert into table emp_dynamic_partition partition(deptno) select empno,ename,job,mgr,hiredate,sal,comm, deptno from emp;</p><p>但是执行时报错:</p><p>FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict</p><p>只需执行set hive.exec.dynamic.partition.mode=nonstrict;再执行加载操作即可</p><p>hive (hive)&gt; show partitions emp_dynamic_partition<br>           &gt; ;<br>OK<br>partition<br>deptno=10<br>deptno=20<br>deptno=30<br>deptno=__HIVE_DEFAULT_PARTITION__</p><h4 id="2-Hive中的函数"><a href="#2-Hive中的函数" class="headerlink" title="2.Hive中的函数"></a>2.Hive中的函数</h4><p>内置函数</p><p>UDF函数</p><h5 id="2-1-查看内置函数"><a href="#2-1-查看内置函数" class="headerlink" title="2.1 查看内置函数:"></a>2.1 查看内置函数:</h5><p>show functions;</p><h5 id="2-2查看函数使用方法"><a href="#2-2查看函数使用方法" class="headerlink" title="2.2查看函数使用方法:"></a>2.2查看函数使用方法:</h5><p>desc function extended xxx;</p><h5 id="2-3-常用内置函数"><a href="#2-3-常用内置函数" class="headerlink" title="2.3 常用内置函数"></a>2.3 常用内置函数</h5><h6 id="日期相关"><a href="#日期相关" class="headerlink" title="日期相关"></a>日期相关</h6><p><strong>current_date</strong></p><p>2018-12-14</p><p><strong>current_timestamp</strong></p><p>2018-12-14 22:26:06.679</p><p><strong>unix_timestamp</strong></p><p>select unix_timestamp(“2018-12-14 22:26:06.679”);</p><p>1544797566</p><p>非标准格式可以转化:</p><p>select unix_timestamp(“20181214 222606”,”yyyyMMdd HHmmss”);</p><p>1544797566</p><p><strong>from_unixtime(时间戳转time)</strong></p><p>select from_unixtime(1544797566);</p><p>2018-12-14 22:26:06</p><p><strong>to_date(转化成日期格式)</strong></p><p><strong>year</strong></p><p><strong>day</strong></p><p><strong>hour</strong></p><p><strong>minuite</strong></p><p><strong>second</strong></p><pre><code>hive (hive)&gt; select year(&quot;2018-12-14 22:26:06&quot;);OK_c02018Time taken: 0.035 seconds, Fetched: 1 row(s)hive (hive)&gt; select month(&quot;2018-12-14 22:26:06&quot;);OK_c012Time taken: 0.025 seconds, Fetched: 1 row(s)hive (hive)&gt; select day(&quot;2018-12-14 22:26:06&quot;);OK_c014Time taken: 0.024 seconds, Fetched: 1 row(s)hive (hive)&gt; select hour(&quot;2018-12-14 22:26:06&quot;);OK_c022Time taken: 0.025 seconds, Fetched: 1 row(s)hive (hive)&gt; select minute(&quot;2018-12-14 22:26:06&quot;);OK_c026Time taken: 0.03 seconds, Fetched: 1 row(s)hive (hive)&gt; select second(&quot;2018-12-14 22:26:06&quot;);OK_c06Time taken: 0.023 seconds, Fetched: 1 row(s)</code></pre><p><strong>weekofyear(一年的第几周)</strong></p><p><strong>dayofmonth</strong></p><p><strong>month_between(两个时间相差几个月)</strong></p><p><strong>add_month(加…月)</strong></p><p><strong>datediff(两个时间相差多少天)</strong></p><p><strong>date_add(加..天)</strong></p><p><strong>date_sub(减..天)</strong></p><p><strong>last_day(本月的最后一天)</strong></p><h6 id="算术相关"><a href="#算术相关" class="headerlink" title="算术相关:"></a>算术相关:</h6><p><strong>round(四舍五入)</strong></p><p><strong>ceil(向上取整)</strong></p><p><strong>floor(向下取整)</strong></p><h6 id="字符串相关"><a href="#字符串相关" class="headerlink" title="字符串相关"></a>字符串相关</h6><p><strong>upper</strong></p><p><strong>lower</strong></p><p><strong>length(求字符串长度)</strong></p><p><strong>trim(去空格)</strong></p><p><strong>lpad(指定长度向左用指定字符补齐)</strong></p><p><strong>rpad</strong></p><p><strong>regexp_replace(用指定字符串替换字符串)</strong></p><p>select regexp_replace(“2030/09/01”,”/“,”-“);</p><p><strong>substr(从某个位置开始截取几位)</strong></p><p><strong>concat(拼接)</strong></p><p><strong>concat_ws(用第一个参数拼接后面的字符串)</strong></p><p>select concat_ws(“.”,”192”,”168”,”1”,”1”);</p><p><strong>split(根据指定字符拆分)</strong></p><p>select split(“192.168.1.1”,”\.”);(需要转义特定字符)</p><p><strong>json_tuple(拆分json字符串)</strong></p><p><strong>parse_url_tuple</strong></p><p><a href="http://www.google.com/test/film?param1=value1&amp;param2=value2">http://www.google.com/test/film?param1=value1&amp;param2=value2</a></p><p>http/https: 协议<br><a href="http://www.google.com/">www.google.com</a>: domain<br>test/film: path<br>param1=value1&amp;param2=value2: param</p><h4 id="3-案例"><a href="#3-案例" class="headerlink" title="3.案例:"></a>3.案例:</h4><p><strong>案例一</strong>:</p><p>有一张部门表,求出每个部门的男女人数</p><p>create table emp_info(<br>id string,<br>name string,<br>dept string,<br>sex string<br>)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;</p><p>数据:</p><p>1,ZHANGSAN,RD,1<br>2,LISI,RD,1<br>3,XIAOHEI,RD,2<br>4,XIAOBAI,QA,1<br>5,ERGOU,QA,2<br>6,DAZHUANG,QA,2</p><p>方法一:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> dept<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token keyword">case</span> <span class="token keyword">when</span> dept<span class="token operator">=</span><span class="token string">"1"</span> <span class="token keyword">then</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> m_cnt<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token keyword">case</span> <span class="token keyword">when</span> dept<span class="token operator">=</span><span class="token string">"2"</span> <span class="token keyword">then</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f_cnt<span class="token keyword">from</span> emp_info<span class="token keyword">group</span> <span class="token keyword">by</span> dept<span class="token punctuation">;</span></code></pre><p>方法二:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> dept<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token keyword">if</span><span class="token punctuation">(</span>dept<span class="token operator">=</span><span class="token string">"1"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> m_cnt<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token keyword">if</span><span class="token punctuation">(</span>dept<span class="token operator">=</span><span class="token string">"2"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f_cnt<span class="token keyword">from</span> emp_info<span class="token keyword">group</span> <span class="token keyword">by</span> dept<span class="token punctuation">;</span></code></pre><p><strong>案例二(行转列)</strong>:</p><p>相同部门、性别的人合在一起两列展示部门性别和姓名：</p><p>RD,1 PK|RUOZE<br>RD,2 XIAOHONG<br>QA,1 XIAOZHANG<br>QA,2 XIAOLI|XIAOFANG</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> dept_sex<span class="token punctuation">,</span>concat_ws<span class="token punctuation">(</span><span class="token string">'|'</span><span class="token punctuation">,</span>collect_set<span class="token punctuation">(</span>t<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> name<span class="token punctuation">,</span>concat_ws<span class="token punctuation">(</span><span class="token string">','</span><span class="token punctuation">,</span>dept<span class="token punctuation">,</span>sex<span class="token punctuation">)</span> dept_sex<span class="token keyword">from</span> emp_info<span class="token punctuation">)</span> t<span class="token keyword">group</span> <span class="token keyword">by</span> dept_sex<span class="token punctuation">;</span></code></pre><p><strong>案例三(列转行)</strong>:</p><p>数据:</p><p>PK MapReduce<br>PK Hive<br>PK Spark<br>PK Flink<br>J Hadoop<br>J HBase<br>J Kafka</p><p>最终转化为:</p><p>PK      MapReduce,Hive,Spark,Flink<br>J       Hadoop,HBase,Kafka</p><p>建表:</p><p>create table column2row(name string, courses string)<br>row format delimited fields terminated by ‘\t’;</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>name<span class="token punctuation">,</span>course<span class="token keyword">from</span>column2rowlateral <span class="token keyword">view</span> explode<span class="token punctuation">(</span>split<span class="token punctuation">(</span>courses<span class="token punctuation">,</span><span class="token string">','</span><span class="token punctuation">)</span><span class="token punctuation">)</span> course_tmp <span class="token keyword">as</span> course<span class="token punctuation">;</span></code></pre><p><strong>案例四</strong></p><p>按月统计每个网站的点击次数:</p><p>create table user_click (</p><p>domain string,</p><p>time string,</p><p>traffic int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;</p><p>数据:</p><p>gifshow.com,2019/01/01,5<br>yy.com,2019/01/01,4<br>huya.com,2019/01/01,1<br>gifshow.com,2019/01/20,6<br>gifshow.com,2019/02/01,8<br>yy.com,2019/01/20,5<br>gifshow.com,2019/02/02,7</p><p>最终展示为:</p><p>域名               月份    小计   累计<br>gifshow.com     2019-01  11    11<br>gifshow.com     2019-02  15    26<br>yy.com          2019-01  9     9<br>huya.com        2019-01  1     1</p><p>1.先求出每个月的小计</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> t_user_click <span class="token keyword">as</span> <span class="token keyword">select</span>domain<span class="token punctuation">,</span>month<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>traffic<span class="token punctuation">)</span> sum<span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span>domain<span class="token punctuation">,</span>substr<span class="token punctuation">(</span>regexp_replace<span class="token punctuation">(</span>time<span class="token punctuation">,</span><span class="token string">"/"</span><span class="token punctuation">,</span><span class="token string">"-"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span> month<span class="token punctuation">,</span>traffic<span class="token keyword">from</span>user_click<span class="token punctuation">)</span> <span class="token number">c</span> <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">c</span><span class="token punctuation">.</span>domain<span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">.</span>month<span class="token punctuation">;</span></code></pre><p>2.求出当前月的累计数量</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>t<span class="token punctuation">.</span>domain<span class="token punctuation">,</span>t<span class="token punctuation">.</span>month<span class="token punctuation">,</span>sum<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>u<span class="token punctuation">.</span>sum<span class="token punctuation">)</span><span class="token keyword">from</span> t_user_click t<span class="token keyword">inner</span> <span class="token keyword">join</span> t_user_click u<span class="token keyword">on</span> t<span class="token punctuation">.</span>domain<span class="token operator">=</span>u<span class="token punctuation">.</span>domain<span class="token keyword">where</span> t<span class="token punctuation">.</span>month<span class="token operator">>=</span>u<span class="token punctuation">.</span>month<span class="token keyword">group</span> <span class="token keyword">by</span> t<span class="token punctuation">.</span>domain<span class="token punctuation">,</span>t<span class="token punctuation">.</span>month<span class="token punctuation">,</span>t<span class="token punctuation">.</span>sum<span class="token punctuation">;</span></code></pre><p><strong>案例五</strong>:</p><p>请用Hive完成wc统计<br>hello,hello,hello<br>world,world<br>welcome</p><p>建表:</p><p>create table hive_wc(sentence string);</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>word<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> cnt<span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> explode<span class="token punctuation">(</span>split<span class="token punctuation">(</span>sentence<span class="token punctuation">,</span><span class="token string">','</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> word<span class="token keyword">from</span> hive_wc<span class="token punctuation">)</span> t<span class="token keyword">group</span> <span class="token keyword">by</span> word<span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础之Hive参数配置及语法</title>
      <link href="2018/12/08/2020-12-06-quannnxu-hive2/"/>
      <url>2018/12/08/2020-12-06-quannnxu-hive2/</url>
      
        <content type="html"><![CDATA[<h4 id="1-参数修改"><a href="#1-参数修改" class="headerlink" title="1.参数修改"></a>1.参数修改</h4><h5 id="1-1获取hive执行引擎"><a href="#1-1获取hive执行引擎" class="headerlink" title="1.1获取hive执行引擎:"></a>1.1获取hive执行引擎:</h5><p>set hive.execution.engine;</p><h5 id="1-2修改hive执行引擎"><a href="#1-2修改hive执行引擎" class="headerlink" title="1.2修改hive执行引擎:"></a>1.2修改hive执行引擎:</h5><p>set hive.execution.engine=tez;(看情况修改)</p><h5 id="1-3修改hive-site-xml"><a href="#1-3修改hive-site-xml" class="headerlink" title="1.3修改hive-site.xml"></a>1.3修改hive-site.xml</h5><p>打印头信息:<br>    <property><br>      <name>hive.cli.print.header</name><br>      <value>true</value><br>    </property><br>显示最近使用的库:<br>    <property><br>      <name>hive.cli.print.current.db</name><br>      <value>true</value><br>    </property></p><p>显示如下:</p><p>hive (default)&gt; select * from test1;<br>OK<br>test1.id<br>100<br>Time taken: 0.035 seconds, Fetched: 1 row(s)</p><h4 id="2-Hive中参数设置的几种方式"><a href="#2-Hive中参数设置的几种方式" class="headerlink" title="2.Hive中参数设置的几种方式:"></a>2.Hive中参数设置的几种方式:</h4><p>​    hive-site.xml    全局</p><p>​    hive –hiveconf</p><p>​    命令行set a=b;</p><p>优先级:hive-site.xml &lt; hive –hiveconf &lt; set a=b</p><h4 id="3-Hive命令行"><a href="#3-Hive命令行" class="headerlink" title="3.Hive命令行"></a>3.Hive命令行</h4><p>​    -e：SQL语句<br>​    -f：SQL文件<br>​    –hiveconf：设置参数<br>​    -i：定义UDF函数<br>​    exit</p><h5 id="3-1-hive-e使用"><a href="#3-1-hive-e使用" class="headerlink" title="3.1 hive -e使用:"></a>3.1 hive -e使用:</h5><p>[hadoop@warehouse001 bin]$ hive -e “select * from test1”;<br>OK<br>test1.id<br>100<br>Time taken: 3.248 seconds, Fetched: 1 row(s)</p><p><strong>hive -e以后shell封装脚本使用</strong></p><h5 id="3-2-hive-f使用"><a href="#3-2-hive-f使用" class="headerlink" title="3.2 hive -f使用:"></a>3.2 hive -f使用:</h5><p>[hadoop@warehouse001 bin]$ vi 1.sql</p><p>select * from test1;</p><p>[hadoop@warehouse001 bin]$ hive -f 1.sql<br>OK<br>test1.id<br>100<br>Time taken: 3.363 seconds, Fetched: 1 row(s)</p><h4 id="4-Hive-SQL"><a href="#4-Hive-SQL" class="headerlink" title="4.Hive SQL"></a>4.Hive SQL</h4><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL%E5%AE%98%E6%96%B9%E8%AF%AD%E6%B3%95,%E6%9C%ACblog%E8%AF%AD%E6%B3%95%E6%9D%A5%E8%87%AA%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL官方语法,本blog语法来自官方文档</a></p><p><strong>Create Database</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token operator">NOT</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> database_name  <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> database_comment<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">--描述信息</span>  <span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">--自定义HDFS路径</span>  <span class="token punctuation">[</span>MANAGEDLOCATION hdfs_path<span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token keyword">WITH</span> DBPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span></code></pre><p>如:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">database</span> hive<span class="token punctuation">;</span></code></pre><p>HDFS路径:/user/hive/warehouse/hive.db</p><p>元数据库DBS表信息</p><p><img src="https://i.loli.net/2020/12/08/Tp7k1thB6cSVN5R.png"></p><p><strong>Drop Database</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">DROP</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> database_name <span class="token punctuation">[</span><span class="token keyword">RESTRICT</span><span class="token operator">|</span><span class="token keyword">CASCADE</span><span class="token punctuation">]</span><span class="token punctuation">;</span></code></pre><p>hive (hive)&gt; show tables;<br>OK<br>tab_name<br>test<br>Time taken: 0.018 seconds, Fetched: 1 row(s)<br>hive (hive)&gt; drop database hive;<br>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database hive is not empty. One or more tables exist.)</p><p><strong>当数据库有表时不能直接删除,可使用CASCADE强制级联删除,但不推荐使用.</strong></p><p><strong>Alter Database</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">ALTER</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> database_name <span class="token keyword">SET</span> DBPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">-- (Note: SCHEMA added in Hive 0.14.0)</span><span class="token keyword">ALTER</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> database_name <span class="token keyword">SET</span> OWNER <span class="token punctuation">[</span><span class="token keyword">USER</span><span class="token operator">|</span>ROLE<span class="token punctuation">]</span> user_or_role<span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">-- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><span class="token keyword">ALTER</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> database_name <span class="token keyword">SET</span> LOCATION hdfs_path<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- (Note: Hive 2.2.1, 2.4.0 and later)</span><span class="token keyword">ALTER</span> <span class="token punctuation">(</span><span class="token keyword">DATABASE</span><span class="token operator">|</span><span class="token keyword">SCHEMA</span><span class="token punctuation">)</span> database_name <span class="token keyword">SET</span> MANAGEDLOCATION hdfs_path<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">-- (Note: Hive 4.0.0 and later)</span></code></pre><p><strong>Use Database</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">USE</span> database_name<span class="token punctuation">;</span><span class="token keyword">USE</span> <span class="token keyword">DEFAULT</span><span class="token punctuation">;</span></code></pre><p><strong>Create Table</strong></p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token punctuation">[</span><span class="token keyword">TEMPORARY</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>EXTERNAL<span class="token punctuation">]</span> <span class="token keyword">TABLE</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token operator">NOT</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>db_name<span class="token punctuation">.</span><span class="token punctuation">]</span>table_name    <span class="token comment" spellcheck="true">-- (Note: TEMPORARY available in Hive 0.14.0 and later)</span>  <span class="token punctuation">[</span><span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>column_constraint_specification<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span>constraint_specification<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> table_comment<span class="token punctuation">]</span>  <span class="token punctuation">[</span>PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name<span class="token punctuation">,</span> col_name<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">[</span>SORTED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name <span class="token punctuation">[</span><span class="token keyword">ASC</span><span class="token operator">|</span><span class="token keyword">DESC</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">INTO</span> num_buckets BUCKETS<span class="token punctuation">]</span>  <span class="token punctuation">[</span>SKEWED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name<span class="token punctuation">,</span> col_name<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>                  <span class="token comment" spellcheck="true">-- (Note: Available in Hive 0.10.0 and later)]</span>     <span class="token keyword">ON</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>col_value<span class="token punctuation">,</span> col_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>col_value<span class="token punctuation">,</span> col_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>     <span class="token punctuation">[</span>STORED <span class="token keyword">AS</span> DIRECTORIES<span class="token punctuation">]</span>  <span class="token punctuation">[</span>   <span class="token punctuation">[</span><span class="token keyword">ROW</span> FORMAT row_format<span class="token punctuation">]</span>    <span class="token punctuation">[</span>STORED <span class="token keyword">AS</span> file_format<span class="token punctuation">]</span>     <span class="token operator">|</span> STORED <span class="token keyword">BY</span> <span class="token string">'storage.handler.class.name'</span> <span class="token punctuation">[</span><span class="token keyword">WITH</span> SERDEPROPERTIES <span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">-- (Note: Available in Hive 0.6.0 and later)</span>  <span class="token punctuation">]</span>  <span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>  <span class="token punctuation">[</span>TBLPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true">-- (Note: Available in Hive 0.6.0 and later)</span>  <span class="token punctuation">[</span><span class="token keyword">AS</span> select_statement<span class="token punctuation">]</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">-- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span></code></pre><table><thead><tr><th align="left">Storage Format</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">STORED AS TEXTFILE</td><td align="left">Stored as plain text files. TEXTFILE is the default file format, unless the configuration parameter <a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.default.fileformat">hive.default.fileformat</a> has a different setting.Use the DELIMITED clause to read delimited files.Enable escaping for the delimiter characters by using the ‘ESCAPED BY’ clause (such as ESCAPED BY ‘&#39;)  Escaping is needed if you want to work with data that can contain these delimiter characters.   A custom NULL format can also be specified using the ‘NULL DEFINED AS’ clause (default is ‘\N’). (Hive 4.0) All BINARY columns in the table are assumed to be base64 encoded. To read the data as raw bytes:TBLPROPERTIES (“hive.serialization.decode.binary.as.base64”=”false”)</td></tr><tr><td align="left">STORED AS SEQUENCEFILE</td><td align="left">Stored as compressed Sequence File.</td></tr><tr><td align="left">STORED AS ORC</td><td align="left">Stored as <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntax">ORC file format</a>. Supports ACID Transactions &amp; Cost-based Optimizer (CBO). Stores column-level metadata.</td></tr><tr><td align="left">STORED AS PARQUET</td><td align="left">Stored as Parquet format for the <a href="https://cwiki.apache.org/confluence/display/Hive/Parquet">Parquet</a> columnar storage format in <a href="https://cwiki.apache.org/confluence/display/Hive/Parquet#Parquet-Hive0.13andlater">Hive 0.13.0 and later</a>;  Use ROW FORMAT SERDE … STORED AS INPUTFORMAT … OUTPUTFORMAT syntax … in <a href="https://cwiki.apache.org/confluence/display/Hive/Parquet#Parquet-Hive0.10-0.12">Hive 0.10, 0.11, or 0.12</a>.</td></tr><tr><td align="left">STORED AS AVRO</td><td align="left">Stored as Avro format in <a href="https://issues.apache.org/jira/browse/HIVE-6806">Hive 0.14.0 and later</a> (see <a href="https://cwiki.apache.org/confluence/display/Hive/AvroSerDe">Avro SerDe</a>).</td></tr><tr><td align="left">STORED AS RCFILE</td><td align="left">Stored as <a href="https://en.wikipedia.org/wiki/RCFile">Record Columnar File</a> format.</td></tr><tr><td align="left">STORED AS JSONFILE</td><td align="left">Stored as Json file format in Hive 4.0.0 and later.</td></tr><tr><td align="left">STORED BY</td><td align="left">Stored by a non-native table format. To create or link to a non-native table, for example a table backed by <a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration">HBase</a> or <a href="https://cwiki.apache.org/confluence/display/Hive/Druid+Integration">Druid</a> or <a href="https://cwiki.apache.org/confluence/display/Hive/AccumuloIntegration">Accumulo</a>.  See <a href="https://cwiki.apache.org/confluence/display/Hive/StorageHandlers">StorageHandlers</a> for more information on this option.</td></tr><tr><td align="left">INPUTFORMAT and OUTPUTFORMAT</td><td align="left">in the file_format to specify the name of a corresponding InputFormat and OutputFormat class as a string literal.  For example, ‘org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat’.   For LZO compression, the values to use are  ‘INPUTFORMAT “com.hadoop.mapred.DeprecatedLzoTextInputFormat”  OUTPUTFORMAT “<a href="http://org.apache.hadoop.hive.ql.io/">org.apache.hadoop.hive.ql.io</a>.HiveIgnoreKeyTextOutputFormat”‘   (see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LZO">LZO Compression</a>).</td></tr></tbody></table><p>如:</p><p>create table table_name(</p><p>id int,name string,age int</p><p>)</p><p>CREATE TABLE：指定要创建的表的名字<br>col_name data_type：列名以及对应的数据类型，多个列之间使用逗号分隔<br>PARTITIONED BY：指定分区<br>CLUSTERED BY： 排序、分桶<br>ROW FORMAT：指定数据的分隔符等信息<br>STORED AS：指定表存储的数据格式：textfile rc orc parquet等<br>LOCATION：指定表在文件系统上的存储路径<br>AS select_statement: 通过select的sql语句的结果来创建表</p><p><strong>创建员工表:</strong></p><p>create table emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p><strong>load本地文件:</strong></p><p>load data local inpath ‘/home/hadoop/data/emp.txt’ overwrite into table emp;</p><p>Time taken: 0.155 seconds<br>hive (hive)&gt; select * from emp;<br>OK<br>emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno<br>7369    SMITH   CLERK   7902    1980-12-17 00:00:00     800.0   NULL    20<br>7499    ALLEN   SALESMAN        7698    1981-02-20 00:00:00     1600.0  300.0   30<br>7521    WARD    SALESMAN        7698    1981-02-22 00:00:00     1250.0  500.0   30<br>7566    JONES   MANAGER 7839    1981-04-02 00:00:00     2975.0  NULL    20<br>7654    MARTIN  SALESMAN        7698    1981-09-28 00:00:00     1250.0  1400.0  30<br>7698    BLAKE   MANAGER 7839    1981-05-01 00:00:00     2850.0  NULL    30<br>7782    CLARK   MANAGER 7839    1981-06-09 00:00:00     2450.0  NULL    10<br>7788    SCOTT   ANALYST 7566    1982-12-09 00:00:00     3000.0  NULL    20<br>7839    KING    PRESIDENT       NULL    5000.00 NULL    10.0    NULL<br>7844    TURNER  SALESMAN        7698    1981-09-08 00:00:00     1500.0  0.0     30<br>7876    ADAMS   CLERK   7788    1983-01-12 00:00:00     1100.0  NULL    20<br>7900    JAMES   CLERK   7698    1981-12-03 00:00:00     950.0   NULL    30<br>7902    FORD    ANALYST 7566    1981-12-03 00:00:00     3000.0  NULL    20<br>7934    MILLER  CLERK   7782    1982-01-23 00:00:00     1300.0  NULL    10<br>Time taken: 0.258 seconds, Fetched: 14 row(s)</p><p>只建表结构:</p><p>create table emp2 like emp;</p><p><strong>内部表和外部表的区别:</strong></p><p>​    内部表:创建语法create table xxxx;</p><p>​    <strong>内部表删除表之后HDFS和元数据都被删除了</strong></p><p>​    外部表:创建语法create external table xxx;</p><p>​    <strong>外部表删除之后只删除元数据,HDFS文件依旧还存在</strong></p><p>创建内部表:</p><p>create table emp_managed(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><pre><code>hive (hive)&gt; desc formatted emp_managed;OKcol_name        data_type       comment# col_name              data_type               comment             empno                   int                                         ename                   string                                      job                     string                                      mgr                     int                                         hiredate                string                                      sal                     double                                      comm                    double                                      deptno                  int                                         # Detailed Table Information             Database:               hive                     OwnerType:              USER                     Owner:                  hadoop                   CreateTime:             Tue Dec 08 22:33:37 CST    LastAccessTime:         UNKNOWN                  Protect Mode:           None                     Retention:              0                        Location:               hdfs://warehouse001:9000/user/hive/warehouse/hive.db/emp_managed         Table Type:             MANAGED_TABLE      --内部表类型是MANAGED_TABLE 是交给hive内部管理的    Table Parameters:                        transient_lastDdlTime   1607438017          # Storage Information            SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       InputFormat:            org.apache.hadoop.mapred.TextInputFormat         OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       Compressed:             No                       Num Buckets:            -1                       Bucket Columns:         []                       Sort Columns:           []                       Storage Desc Params:                     field.delim             \t                          serialization.format    \t                  Time taken: 0.06 seconds, Fetched: 35 row(s)</code></pre><p>创建外部表:</p><p>create external table emp_external(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><pre><code>hive (hive)&gt; desc formatted emp_external;OKcol_name        data_type       comment# col_name              data_type               comment             empno                   int                                         ename                   string                                      job                     string                                      mgr                     int                                         hiredate                string                                      sal                     double                                      comm                    double                                      deptno                  int                                         # Detailed Table Information             Database:               hive                     OwnerType:              USER                     Owner:                  hadoop                   CreateTime:             Tue Dec 08 22:39:49 CST      LastAccessTime:         UNKNOWN                  Protect Mode:           None                     Retention:              0                        Location:               hdfs://warehouse001:9000/user/hive/warehouse/hive.db/emp_external        Table Type:             EXTERNAL_TABLE  -- 外部表只删除元数据,不能删除HDFS文件        Table Parameters:                        EXTERNAL                TRUE                        transient_lastDdlTime   1607438389          # Storage Information            SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       InputFormat:            org.apache.hadoop.mapred.TextInputFormat         OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       Compressed:             No                       Num Buckets:            -1                       Bucket Columns:         []                       Sort Columns:           []                       Storage Desc Params:                     field.delim             \t                          serialization.format    \t                  Time taken: 0.053 seconds, Fetched: 36 row(s)</code></pre><p><strong>内部表和外部表的相互转换</strong></p><p>ALTER TABLE emp_external SET TBLPROPERTIES (‘EXTERNAL’ = ‘false’);</p><p><strong>EXTERNAL</strong>参数必须为大写</p><p>内部表同理转换.</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础之Hive简介及搭建</title>
      <link href="2018/12/05/2020-12-05-quannnxu-hive1/"/>
      <url>2018/12/05/2020-12-05-quannnxu-hive1/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Hive是基于Hadoop之上的"><a href="#1-Hive是基于Hadoop之上的" class="headerlink" title="1.Hive是基于Hadoop之上的"></a><strong>1.Hive是基于Hadoop之上的</strong></h4><p>​    Hadoop广义:</p><p>​        大数据生态圈</p><p>​        Hive:使用sql完成大数据统计分析</p><p>​    Hadoop狭义:</p><p>​        HDFS    YARN    MR</p><h4 id="2-下载路径"><a href="#2-下载路径" class="headerlink" title="2.下载路径:"></a>2.下载路径:</h4><p>Apache的顶级项目或者是孵化项目路径</p><p>xxx.apache.org</p><p>GitHub:github.com/apache/xxx    (源码地址)</p><p>CDH版本:<a href="http://archive.cloudera.com/cdh5/cdh/5">http://archive.cloudera.com/cdh5/cdh/5</a></p><p>本次使用的是<strong>hive-1.1.0-cdh5.16.2</strong>版本</p><p>Hadoop、Hive、Sqoop、Hbase等    版本一定要一致（兼容性） Spark除外</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5">http://archive.cloudera.com/cdh5/cdh/5</a></p><p>补充一个JDBC的概念:</p><p>Java数据库连接，（Java Database Connectivity，简称JDBC）是<a href="https://baike.baidu.com/item/Java%E8%AF%AD%E8%A8%80">Java语言</a>中用来规范客户端程序如何来访问数据库的<a href="https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3/10418844">应用程序接口</a>，提供了诸如查询和更新数据库中数据的方法。JDBC也是Sun Microsystems的商标。我们通常说的JDBC是面向关系型数据库的。</p><h4 id="3-distributed-storage-分布式存储系统"><a href="#3-distributed-storage-分布式存储系统" class="headerlink" title="3.distributed storage(分布式存储系统):"></a><strong>3.distributed storage(分布式存储系统):</strong></h4><p>​    HDFS</p><p>​    S3(亚马逊)</p><p>​    OSS(阿里云)</p><p>​    COS(腾讯云)</p><p>​    …</p><h4 id="4-Hive简介"><a href="#4-Hive简介" class="headerlink" title="4.Hive简介:"></a>4.Hive简介:</h4><p>​    FaceBook    解决海量的结构化日志的统计问题</p><p>​    Hive是构建在Hadoop之上的数据仓库</p><p>​        HDFS:Hive的数据存放在HDFS</p><p>​        MR:Hive作业（sql）是通过hive的框架翻译成MR</p><p>​        YARN:Hive的作业是提交到YARN上面去运行的</p><p>​    Hadoop:开发可以使用单机,生产上肯定都是分布式的</p><p>​    Hive其实就是一个客户端(提交机器)而已,没有集群概念</p><p>​        SQL==&gt;Hive==&gt;MR==&gt;YARN</p><p>​    Hive职责:将SQL翻译成底层对应的执行引擎作业</p><p>​    统一的元数据(metadata)管理==&gt;metastore</p><p>​        元数据:描述数据的数据</p><p>​        源数据:采集来的数据文件</p><p>​    Spark    SQL/Impala/Presto…统一使用metastore</p><p>Hive的数据是存放在distribute的storage上的,元数据是存储在metastore所对应的底层数据库中的(MySQL)</p><p>Hive架构:</p><p><img src="https://i.loli.net/2020/12/06/PijA9nQplqDLUXF.png"></p><h4 id="5-Hive架构"><a href="#5-Hive架构" class="headerlink" title="5.Hive架构"></a>5.Hive架构</h4><p><img src="https://i.loli.net/2020/12/07/Ct2pr6bKSjJonc7.png"></p><h5 id="5-1-Hive与RDBMS对比"><a href="#5-1-Hive与RDBMS对比" class="headerlink" title="5.1 Hive与RDBMS对比"></a>5.1 Hive与RDBMS对比</h5><p>可以从以下几点来对比:</p><p>​    分布式</p><p>​    节点数</p><p>​    成本</p><p>​    数据量</p><p>​    事务</p><p>​    hive也支持update操作,但一般不推荐用.</p><p>​    延时性</p><h5 id="5-2-Hive的适用场景"><a href="#5-2-Hive的适用场景" class="headerlink" title="5.2 Hive的适用场景"></a>5.2 Hive的适用场景</h5><p>​    批处理/离线处理</p><p>​    延时性大</p><p>​    尽量少涉及到update或者delete操作,虽然是支持的(走底层MR)</p><h5 id="5-2-Hive的优缺点"><a href="#5-2-Hive的优缺点" class="headerlink" title="5.2 Hive的优缺点"></a>5.2 Hive的优缺点</h5><p>​    优点:易上手、比MR使用起来简单的多</p><p>​    缺点:高延时</p><p><strong>Hive的执行速度对比MySQL的数据谁快?</strong></p><p>​    这两个是使用不同场景的,不能拿来比较</p><h5 id="5-3-Hive安装目录"><a href="#5-3-Hive安装目录" class="headerlink" title="5.3 Hive安装目录"></a>5.3 Hive安装目录</h5><p>auxlib  UDF函数编程</p><p>conf    Hive相关的配置文件</p><p>bin        jar包</p><p>lib         相关脚本</p><h5 id="5-4-添加到系统环境变量"><a href="#5-4-添加到系统环境变量" class="headerlink" title="5.4 添加到系统环境变量"></a>5.4 添加到系统环境变量</h5><p>[hadoop@warehouse001 ~]$ vi .bashrc</p><p>​        export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.16.2<br>​        export PATH=$HIVE_HOME/bin:$PATH</p><p><strong>查看数据库</strong></p><p>hive&gt; show databases;<br>OK<br>default<br>Time taken: 3.702 seconds, Fetched: 1 row(s)</p><p><strong>创建表</strong></p><p>hive&gt; create table test(id int);<br>OK<br>Time taken: 4.293 seconds</p><p><strong>查询表</strong></p><p>hive&gt; select * from test;<br>OK<br>Time taken: 0.394 seconds</p><p>重新开启一个窗口重新进入Hive:</p><p>hive&gt; show tables;<br>FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</p><p>原因:Derby是单Session的</p><p>Hive日志配置:</p><p>hive&gt; [hadoop@warehouse001 bin]$ cd ../conf/<br>[hadoop@warehouse001 conf]$ ll<br>total 20<br>-rw-r–r– 1 hadoop hadoop 1196 Jun  3  beeline-log4j.properties.template<br>-rw-r–r– 1 hadoop hadoop 2378 Jun  3  hive-env.sh.template<br>-rw-r–r– 1 hadoop hadoop 2662 Jun  3  hive-exec-log4j.properties.template<br>-rw-r–r– 1 hadoop hadoop 3505 Jun  3  hive-log4j.properties.template<br>-rw-r–r– 1 hadoop hadoop 2060 Jun  3  ivysettings.xml<br>[hadoop@warehouse001 conf]$ cp hive-log4j.properties.template hive-log4j.properties</p><p>Hive默认日志路径hive.log.dir=${java.io.tmpdir}/${user.name}</p><p>查看/tmp/hadoop/hive.log</p><p>Caused by: ERROR XJ040: Failed to start database ‘metastore_db’ with class loader sun.misc.Launcher$AppClassLoader@7cca494b, see the next exception for details.<br>        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)<br>        at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)<br>        … 86 more<br>Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/hadoop/app/hive-1.1.0-cdh5.16.2/bin/metastore_db.</p><p>由此可见derby的单session的</p><h5 id="5-5-配置MySQL作为元数据库"><a href="#5-5-配置MySQL作为元数据库" class="headerlink" title="5.5 配置MySQL作为元数据库"></a>5.5 配置MySQL作为元数据库</h5><p>[hadoop@warehouse001 conf]$ vi hive-site.xml</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://localhost:3306/hive_metadata?createDatabaseIfNotExist=true<span class="token entity" title="&amp;">&amp;amp;</span>useSSL=false<span class="token entity" title="&amp;">&amp;amp;</span>useUnicode=true<span class="token entity" title="&amp;">&amp;amp;</span>characterEncoding=UTF-8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>xxxx<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>再开启hive就可以多个窗口访问了</p><p><strong>元数据表:</strong></p><p>mysql&gt; use hive_metadata;<br>Database changed<br>mysql&gt; show tables;<br>+—————————+<br>| Tables_in_hive_metadata   |<br>+—————————+<br>| bucketing_cols            |<br>| cds                       |<br>| columns_v2                |<br>| database_params           |<br>| dbs                       |<br>| func_ru                   |<br>| funcs                     |<br>| global_privs              |<br>| partition_keys            |<br>| roles                     |<br>| sd_params                 |<br>| sds                       |<br>| sequence_table            |<br>| serde_params              |<br>| serdes                    |<br>| skewed_col_names          |<br>| skewed_col_value_loc_map  |<br>| skewed_string_list        |<br>| skewed_string_list_values |<br>| skewed_values             |<br>| sort_cols                 |<br>| table_params              |<br>| tbls                      |<br>| version                   |<br>+—————————+</p><p>​    DBS: databases<br>​    TBLS: Tables<br>​    COLUMNS_V2: column </p><h5 id="5-6-Hive默认的存放路径"><a href="#5-6-Hive默认的存放路径" class="headerlink" title="5.6 Hive默认的存放路径"></a>5.6 Hive默认的存放路径</h5><p>/user/hive/warehouse</p><p>默认库的表则是在/user/hive/warehouse/下</p><p>用户自定义表/user/hive/warehouse/库名.db/下</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop06之Yarn参数调优</title>
      <link href="2018/12/02/2020-12-02-quannnxu-hadoop6/"/>
      <url>2018/12/02/2020-12-02-quannnxu-hadoop6/</url>
      
        <content type="html"><![CDATA[<h4 id="1-container容器"><a href="#1-container容器" class="headerlink" title="1.container容器"></a>1.container容器</h4><p>关于yarn的调优,其实就是调整container</p><p>虚拟化    是memory+cpu vcore组成的 是专门运行任务</p><p>生产上应如何调优container参数?</p><p>128G    16物理core</p><h5 id="1-1-系统装完消耗1G"><a href="#1-1-系统装完消耗1G" class="headerlink" title="1.1 系统装完消耗1G"></a>1.1 系统装完消耗1G</h5><h5 id="1-2-系统要预留20-内存"><a href="#1-2-系统要预留20-内存" class="headerlink" title="1.2 系统要预留20%内存"></a>1.2 系统要预留20%内存</h5><p>给当前进程服务,防止出现oom-kill机制</p><p>Linux系统防止夯住</p><p>给未来部署软件预留空间</p><p>128G服务器</p><p>128G*20%=26G </p><p>剩余102G</p><h5 id="1-3-DN-NM节点"><a href="#1-3-DN-NM节点" class="headerlink" title="1.3 DN NM节点"></a>1.3 DN NM节点</h5><p>存储和计算一体,数据本地化,节省网络IO</p><p>DN=2G(受限磁盘IO)</p><p>NM=4G    NM进程本身的内存</p><p>剩余内存96G    是真正container容器总内存,才是真正计算的内存</p><h5 id="1-4-container内存"><a href="#1-4-container内存" class="headerlink" title="1.4 container内存"></a>1.4 container内存</h5><p>yarn.nodemanager.resource.memory-mb        -1</p><p>​    <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/</a></p><p>yarn-default.xml</p><p><strong>yarn.nodemanager.resource.memory-mb    96G</strong></p><p><strong>yarn.scheduler.minimum-allocation-mb  1G</strong>    极限情况    96个container容器 内存1G</p><p><strong>yarn.scheduler.maximum-allocation-mb  96G</strong>    极限情况    1个container容器 内存96G</p><p>container容器会不断的字段增加内存1G,cdh有这个参数    默认不动</p><h5 id="1-5-container虚拟core"><a href="#1-5-container虚拟core" class="headerlink" title="1.5 container虚拟core"></a>1.5 container虚拟core</h5><p>这个概念是yarn自己引入的.设计初衷是考虑不同服务器的cpu性能不一样,每个cpu计算能力不一样.</p><p>比如某个物理cpu是另外一个物理cpu的2倍,这时通过设置第一个物理cpu的虚拟core来弥补差异</p><p>但是后来大家都是使用虚拟core，默认值一般都不会去修改，就是2</p><p>yarn.nodemanager.resource.pcores-vcores-multiplier  2 </p><p>生产调整</p><p><strong>yarn.nodemanager.resource.cpu-vcores    32</strong>(16*2,也可以适当预留几个物理core)</p><p><strong>yarn.scheduler.minimum-allocation-vcores  1</strong>    极限情况,是32个</p><p><strong>yarn.scheduler.maximum-allocation-vcores  32</strong>    极限情况,是1个</p><p>补充:</p><table><thead><tr><th>yarn.nodemanager.pmem-check-enabled(检查物理内存)</th><th>true</th></tr></thead><tbody><tr><td><strong>yarn.nodemanager.vmem-check-enabled(检查虚拟内存)</strong></td><td><strong>true</strong></td></tr><tr><td><strong>yarn.nodemanager.vmem-pmem-ratio(物理虚拟内存比例,一般不动)</strong></td><td><strong>2.1</strong></td></tr></tbody></table><h5 id="1-6-生产上如何设置"><a href="#1-6-生产上如何设置" class="headerlink" title="1.6 生产上如何设置"></a>1.6 生产上如何设置</h5><p>突破口yarn.scheduler.maximum-allocation-vcores</p><p>CPU    内存</p><p><strong>yarn.nodemanager.resource.cpu-vcores    32</strong></p><p><strong>yarn.scheduler.minimum-allocation-vcores  1</strong>    </p><p><strong>yarn.scheduler.maximum-allocation-vcores  4</strong>    极限情况下是有8个container</p><p>CDH官方经过大量验证,经验值,container容器最大分配vcore不要超过5,故一般生产设置4</p><p><strong>yarn.nodemanager.resource.memory-mb    96G</strong></p><p><strong>yarn.scheduler.minimum-allocation-mb  1G</strong>    </p><p><strong>yarn.scheduler.maximum-allocation-mb  12G</strong>    极限情况下,是有8个container</p><p>也就是说:1个container12G    </p><p>​                4个vcore    </p><p>​                1个vcore使用3G</p><p>正常来说,生产上yarn.scheduler.maximum-allocation-mb设置8G就OK</p><h5 id="1-7-参数如何配置"><a href="#1-7-参数如何配置" class="headerlink" title="1.7 参数如何配置"></a>1.7 参数如何配置</h5><p>假如该节点还有HBase RS节点=30G内存(关于内存指针压缩技术)</p><p><a href="https://blog.csdn.net/weixin_44641024/article/details/103248842">https://blog.csdn.net/weixin_44641024/article/details/103248842</a></p><p>服务器256G    32个物理core</p><p>如上6个参数如何设计?</p><p>预留256*20%=51    RS=30G</p><p>剩余175G</p><p>DN=3G    NM=6G</p><p>剩余166G</p><p><strong>yarn.nodemanager.resource.cpu-vcores    60</strong></p><p><strong>yarn.scheduler.minimum-allocation-vcores  1</strong>    </p><p><strong>yarn.scheduler.maximum-allocation-vcores  4</strong>    极限情况下是有8个container</p><p><strong>yarn.nodemanager.resource.memory-mb    166G</strong></p><p><strong>yarn.scheduler.minimum-allocation-mb  1G</strong>    </p><p><strong>yarn.scheduler.maximum-allocation-mb  21G</strong></p><p>1个container21G    4个vcore    1个vcore5G</p><h4 id="2-调度器"><a href="#2-调度器" class="headerlink" title="2.调度器"></a>2.调度器</h4><p>理想情况下，我们应用对Yarn资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一</p><p>个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应</p><p>用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用</p><p>场景。为此，Yarn提供了多种调度器和可配置的策略供我们选择。</p><h5 id="yarn的三种调度器"><a href="#yarn的三种调度器" class="headerlink" title="yarn的三种调度器:"></a>yarn的三种调度器:</h5><p><strong>FIFO Scheduler</strong>    先进先出调度器</p><p>把应用按提交的顺序排成一个队列，这是一个<strong>先进先出</strong>队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p><p><img src="https://i.loli.net/2020/12/05/2yhWg1HrIQTZlPA.png"></p><p><strong>Capacity Scheduler</strong>    计算调度器</p><p>Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p><p><img src="https://i.loli.net/2020/12/05/7dhbfQwnW1uAycz.png"></p><p><strong>FairScheduler</strong>    公平调度器 </p><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p><p><img src="https://i.loli.net/2020/12/05/aEeKiY8oxVj3pL6.png"></p><p>当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；</p><p>当第二个小任务提交后，从提交到获得资源会有一定的延迟，<br>因为它需要等待第一个任务释放占用的Container。</p><p>小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。<br>最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop练习题01</title>
      <link href="2018/11/30/2020-11-30-quannnxu-zao-ke/"/>
      <url>2018/11/30/2020-11-30-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.hadoop广义狭义<br>广义: 以apache hadoop软件为主的生态圈 包含hive sqoop hbase kafka spark flink<br>狭义: apache hadoop软件</p><p>2.hadoop软件哪三个组成，分别做什么<br>HDFS        数据存储<br>MapReduce    计算框架<br>Yarn        资源调度</p><p>3.hdfs yarn  web默认端口号多少<br>8088</p><p>4.hdfs读写流程</p><p>5.outputstream是读还是写<br>写</p><p>6.NN是做什么的，一句话简述<br>管理文件系统的命名空间,其实就是维护文件系统树的文件和文件夹,也就是管理元数据</p><p>7.SNN是做什么的<br>备份,非实时的</p><p>8.副本放置策略，假如上传节点是有DN进程，好处是什么<br>假设设置为3个副本:<br>第一个副本: 1) 假如上传节点本身为DN节点，优先放置本节点； 2) 否则就随机挑选一台磁盘不太慢,CPU不太繁忙的节点；<br>第二个副本: 放置在与第一个副本的不同机架的节点上<br>第三个副本: 放置在与第二个副本的相同机架的不同节点上<br>减少网络IO</p><p>9.为什么机器上部署DN NM进程，一般都是两者一起</p><p>10.hadoop fs等价什么<br>hdfs dfs</p><p>11.hadoop开启回收站，回收站目录在哪?(微信群友，直接删除这个文件夹，想要恢复是不是很困难?)<br>/user/username/.Trash</p><p>12.hdfs dfs命令那么多，不会用怎么办<br>去bin目录下执行脚本查看命令</p><p>13.shuffle是什么?<br>洗牌,据key去分组</p><p>14.mr on yarn流程</p><p>15.map task个数一般由谁决定的<br>文件的大小</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop05之MapReduce/yarn架构</title>
      <link href="2018/11/30/2020-11-29-quannnxu-hadoop5/"/>
      <url>2018/11/30/2020-11-29-quannnxu-hadoop5/</url>
      
        <content type="html"><![CDATA[<p>MapReduce</p><p>Hive 执行引擎就是MapReduce</p><h4 id="1-分布式计算框架"><a href="#1-分布式计算框架" class="headerlink" title="1.分布式计算框架"></a>1.分布式计算框架</h4><p>map    映射</p><p>​            指的是一组数据按照规则映射为一组</p><p>​            string= a b c</p><p>​            (a,1)</p><p>​            (b,1)</p><p>​            (a,1)</p><p>reduce    规约    汇总</p><p>​            (a,2)</p><p>​            (b,1)</p><p>SQL:</p><table><thead><tr><th align="center">id</th><th align="center">name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">a</td></tr><tr><td align="center">2</td><td align="center">b</td></tr><tr><td align="center">3</td><td align="center">a</td></tr><tr><td align="center">4</td><td align="center">c</td></tr></tbody></table><p>select id,naem+’x’ from t;</p><table><thead><tr><th align="center">id</th><th align="center">name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">ax</td></tr><tr><td align="center">2</td><td align="center">bx</td></tr><tr><td align="center">3</td><td align="center">ax</td></tr><tr><td align="center">4</td><td align="center">cx</td></tr></tbody></table><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>  name<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span> <span class="token keyword">from</span><span class="token punctuation">(</span>    <span class="token keyword">select</span> id<span class="token punctuation">,</span>name<span class="token operator">+</span><span class="token string">'x'</span> <span class="token keyword">from</span> t<span class="token punctuation">)</span> <span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token punctuation">;</span></code></pre><p>【<strong>shuffle</strong>】    洗牌:数据根据key进行网络传输规整到一起,按规则计算</p><p>warehouse001        </p><p>id=1        name=a    100w条</p><p>id=2        name=b    10w条</p><p>warehouse002        </p><p>id=1        name=a    1w条</p><p>id=3        name=c    10w条</p><p>reduce:</p><p>​                id=1        name=a    101w</p><p>​                id=2        name=b    10w</p><p>​                id=3        name=c    10w</p><h4 id="2-架构设计"><a href="#2-架构设计" class="headerlink" title="2.架构设计"></a>2.架构设计</h4><p>mr on yarn提交流程</p><p>yarn架构设计</p><h5 id="2-1-container-容器-虚拟化概念"><a href="#2-1-container-容器-虚拟化概念" class="headerlink" title="2.1 container 容器 虚拟化概念"></a>2.1 container 容器 虚拟化概念</h5><p>是一定的内存和cpu的资源组合</p><p>100个箱子</p><p>1个箱子    1个人    需要100h</p><p>1个箱子    5个人    需要20h</p><p>在内存够的情况下,适当增加cpu vcore来提升计算的并行度,提高效率</p><p>256G的服务器</p><p><strong>预留 38G(15%)</strong>:</p><p>剩余218G</p><p>1.系统需要一部分+预留空间(未来新增新的服务)</p><p>2.为了防止oom killer的机制(超出内存会杀掉耗内存最多的进程,是一种linux的保护机制)【<strong>out of memory</strong>】</p><p><img src="https://i.loli.net/2020/11/30/PlVA7SEx68QszZk.png"></p><h5 id="2-2流程"><a href="#2-2流程" class="headerlink" title="2.2流程"></a>2.2流程</h5><p><img src="https://i.loli.net/2020/11/30/TSgKNLn27QYtJ9j.png"></p><p>1.Client 向RM提交应用程序，其中包含applicationmaster主程序和启动命令<br>2.applications manager 会为【应用程序分配第一个container容器】，来运行applicationmaster主程序<br>3.applicationmaster主程序就会向applications manager 注册，就可以做yarn的web界面上看到job的运行状态<br>4.applicationmaster主程序采取轮询的方式通过【rpc】协议向resourcescheduler，申请和领取资源(哪台机器 领取多少内存 多少cpu VCORE)</p><p>启动applicationmaster主程序，领取资源；</p><hr><p>5.一旦applicationmaster主程序拿到资源的列表，就和对应的nm进程进行通信，要求启动container来运行task任务<br>6.nm就为task任务设置好运行的环境（container容器）将任务启动命令写在脚本里，并且通过脚本启动任务task<br>7.各个container的task 任务(map task、reduce task任务)，通过【rpc】协议向applicationmaster主程序进行汇报进度和状态，以此让applicationmaster主程序随时掌握task的运行状态。<br>当task任务运行失败，也会重启container任务<br>8.当所有的task任务全部完成，applicationmaster主程序会向applications manager 申请注销和关闭作业，这时在web界面查看任务是  是否完成 ，是成功还是失败。</p><p><strong>主程序在哪个进程的所在节点运行？</strong><br>nm</p><p><strong>主程序要申请container容器吗？</strong></p><p>需要</p><p><strong>一个作业，第一个container容器运行什么？</strong></p><p>运行主程序</p><h4 id="3-wordcount案例"><a href="#3-wordcount案例" class="headerlink" title="3.wordcount案例"></a>3.wordcount案例</h4><p><img src="https://i.loli.net/2020/11/30/tKLXJmfkwcNHVQp.png"></p><p><strong>前3个部分属于Map阶段,后3个部分属于Reduce阶段</strong></p><p>代码:</p><pre class=" language-shell"><code class="language-shell">package org.apache.hadoop.examples;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount &#123;  public static class TokenizerMapper        extends Mapper<Object, Text, Text, IntWritable>&#123;    //继承Mapper方法    private final static IntWritable one = new IntWritable(1);    private Text word = new Text();    public void map(Object key, Text value, Context context                    ) throws IOException, InterruptedException &#123;      StringTokenizer itr = new StringTokenizer(value.toString());      while (itr.hasMoreTokens()) &#123;        word.set(itr.nextToken());        context.write(word, one);        &#125;      //car,1      //car,1      //river,1    &#125;  &#125;  public static class IntSumReducer        extends Reducer<Text,IntWritable,Text,IntWritable> &#123;    private IntWritable result = new IntWritable();    public void reduce(Text key, Iterable<IntWritable> values,                        Context context                       ) throws IOException, InterruptedException &#123;      int sum = 0;      for (IntWritable val : values) &#123;          //reduce计数        sum += val.get();      &#125;      result.set(sum);   //3      context.write(key, result);   //car,3    &#125;  &#125;  public static void main(String[] args) throws Exception &#123;    Configuration conf = new Configuration();    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();    if (otherArgs.length < 2) &#123;      System.err.println("Usage: wordcount <in> [<in>...] <out>");      System.exit(2);    &#125;    Job job = new Job(conf, "word count");    job.setJarByClass(WordCount.class);    job.setMapperClass(TokenizerMapper.class);    job.setCombinerClass(IntSumReducer.class);    job.setReducerClass(IntSumReducer.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(IntWritable.class);    for (int i = 0; i < otherArgs.length - 1; ++i) &#123;      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));    &#125;    FileOutputFormat.setOutputPath(job,      new Path(otherArgs[otherArgs.length - 1]));    System.exit(job.waitForCompletion(true) ? 0 : 1);  &#125;&#125;</code></pre><p><strong>小文件的危害:</strong></p><p>在存储上,小文件过多对NN压力太大(元数据)</p><p>在计算上也会造成资源的浪费(container的内存是固定的,如10个小文件都是10M,container固定是1G,则需要耗费10G内存,但是如果合并小文件则只需要1G的内存)</p><p><strong>tip1:</strong></p><p>A表:主表</p><p>id    name</p><p>1        a        1</p><p>2        b        1</p><p>3        c        1</p><p>4        c        1</p><p>null    x        1w</p><p>B表:明细表</p><p>id    name</p><p>1        a        1w</p><p>2        b        2w</p><p>3        c        3w</p><p>null    x        4w</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token keyword">from</span> <span class="token number">a</span><span class="token keyword">left</span> <span class="token keyword">join</span> <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id</code></pre><p>当null值join时会产生4wx1w数据</p><p>先要过滤null,否则会出现笛卡尔积</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span>id<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span> <span class="token number">c</span><span class="token keyword">from</span> <span class="token number">a</span><span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">)</span> <span class="token number">d</span> <span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token number">c</span> <span class="token keyword">desc</span> <span class="token keyword">limit</span> <span class="token number">100</span></code></pre><p>如果结果都是1则不需要担心,否则就要考虑这些值是否需要,如果需要的话只能慢慢跑,不需要的话就过滤掉.</p><p><strong>tip2:</strong></p><p>关于被挖矿后如何解决:</p><p><a href="https://blog.csdn.net/kevin_darkelf/article/details/46042739">https://blog.csdn.net/kevin_darkelf/article/details/46042739</a></p><p>分析脚本</p><p>一般给脚本权限 赋予 000</p><p>分析脚本，一般把目录删除掉，会又被下载，一般都是通过 wget命令</p><p>将wget命令 卸载掉</p><p>或者将wget重命名</p><p>最后一招，云主机的话联系客服备份文件,重置</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop04之hdfs命令</title>
      <link href="2018/11/29/2020-11-28-quannnxu-hadoop4/"/>
      <url>2018/11/29/2020-11-28-quannnxu-hadoop4/</url>
      
        <content type="html"><![CDATA[<p>1.SNN(检查点动作)</p><p><img src="https://i.loli.net/2020/11/29/yqU1YtOkIhZVxSn.png"></p><p>517是正在编辑的日志                                                                                                                                                                                                                                                                                                             </p><p>1.snn执行checkpoint动作的时候,nn会使用当前的edit文件5515-516,nn会暂时将读写操作记录到一个新的edit文件中5117</p><p>2.snn将nn的fsimage 514和edits文件515-516远程下载到本地</p><p>3.snn将fsimage 514加载到内存中,将edits文件515-516年内容之内存中从头到尾的执行一次,创建一个新的fsimage文件516</p><p>4.snn将新的fsimage 516推送给nn</p><p>5.nn接受到fsimage 516.ckpt滚动为edit 157</p><p>是一份最新的</p><p>SNN主要目的是为了备份,但是生产不用,用的是HA,仅学习过程了解</p><p>HA:</p><p>NN(active)        NN(standby)热备</p><h4 id="2-hdfs命令"><a href="#2-hdfs命令" class="headerlink" title="2.hdfs命令"></a>2.hdfs命令</h4><h5 id="1-hadoop命令"><a href="#1-hadoop命令" class="headerlink" title="1.hadoop命令"></a>1.hadoop命令</h5><p>[hadoop@warehouse001 bin]$ ./hadoop<br>Usage: hadoop [–config confdir] COMMAND<br>       where COMMAND is one of:<br>  fs                   run a generic filesystem user client<br>  version              print the version<br>  jar <jar>            run a jar file<br>  checknative [-a|-h]  check native hadoop and compression libraries availability<br>  distcp <srcurl> <desturl> copy file or directories recursively<br>  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive<br>  classpath            prints the class path needed to get the<br>  credential           interact with credential providers<br>                       Hadoop jar and the required libraries<br>  daemonlog            get/set the log level for each daemon<br>  s3guard              manage data on S3<br>  trace                view and modify Hadoop tracing settings<br> or<br>  CLASSNAME            run the class named CLASSNAME</p><p>Most commands print help when invoked w/o parameters.<br>[hadoop@warehouse001 bin]$ pwd<br>/home/hadoop/app/hadoop/bin</p><h5 id="2-fs命令"><a href="#2-fs命令" class="headerlink" title="2.fs命令"></a>2.fs命令</h5><p>[hadoop@warehouse001 bin]$ hadoop fs<br>Usage: hadoop fs [generic options]<br>        [-appendToFile <localsrc> … <dst>]<br>        [-cat [-ignoreCrc] <src> …]<br>        [-checksum <src> …]<br>        [-chgrp [-R] GROUP PATH…]<br>        [-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>        [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>        [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-count [-q] [-h] [-v] [-x] <path> …]<br>        [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>        [-createSnapshot <snapshotDir> [<snapshotName>]]<br>        [-deleteSnapshot <snapshotDir> <snapshotName>]<br>        [-df [-h] [<path> …]]<br>        [-du [-s] [-h] [-x] <path> …]<br>        [-expunge]<br>        [-find <path> … <expression> …]<br>        [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-getfacl [-R] <path>]<br>        [-getfattr [-R] {-n name | -d} [-e en] <path>]<br>        [-getmerge [-nl] <src> <localdst>]<br>        [-help [cmd …]]<br>        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> …]]<br>        [-mkdir [-p] <path> …]<br>        [-moveFromLocal <localsrc> … <dst>]<br>        [-moveToLocal <src> <localdst>]<br>        [-mv <src> … <dst>]<br>        [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-renameSnapshot <snapshotDir> <oldName> <newName>]<br>        [-rm [-f] [-r|-R] [-skipTrash] <src> …]<br>        [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[–set <acl_spec> <path>]]<br>        [-setfattr {-n name [-v value] | -x name} <path>]<br>        [-setrep [-R] [-w] <rep> <path> …]<br>        [-stat [format] <path> …]<br>        [-tail [-f] <file>]<br>        [-test -[defsz] <path>]<br>        [-text [-ignoreCrc] <src> …]<br>        [-touchz <path> …]<br>        [-usage [cmd …]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><p>检查支持压缩格式</p><p>[hadoop@warehouse001 bin]$ hadoop checknative<br>2018/11/29 18:31:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Native library checking:<br>hadoop:  false<br>zlib:    false<br>snappy:  false<br>lz4:     false<br>bzip2:   false<br>openssl: false<br>2018/11/29 18:31:58 INFO util.ExitUtil: Exiting with status 1</p><p>编译： <a href="https://blog.csdn.net/u010452388/article/details/99691421">https://blog.csdn.net/u010452388/article/details/99691421</a></p><p>尝试编译,编译后有更好的兼容性,二次开发</p><p>classpath</p><p>当前生效的目录,java运行的环境</p><p>[hadoop@warehouse001 bin]$ hadoop classpath<br>/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/</em>:/home/hadoop/app/hadoop/contrib/capacity-scheduler/*.jar</p><p><a href="http://cn.voidcc.com/question/p-tenieuea-bex.html">http://cn.voidcc.com/question/p-tenieuea-bex.html</a></p><h5 id="3-hdfs"><a href="#3-hdfs" class="headerlink" title="3.hdfs"></a>3.hdfs</h5><p>hadoop fs=hdfs dfs</p><p>[hadoop@warehouse001 bin]$ cat hadoop</p><p># the core commands<br>    if [ “$COMMAND” = “fs” ] ; then<br>      CLASS=org.apache.hadoop.fs.FsShell</p><p>[hadoop@warehouse001 bin]$ cat hdfs</p><p>elif [ “$COMMAND” = “dfs” ] ; then<br>  CLASS=org.apache.hadoop.fs.FsShell</p><p>底层同样调用org.apache.hadoop.fs.FsShell</p><h5 id="3-1-dfs命令-使用较多的参数"><a href="#3-1-dfs命令-使用较多的参数" class="headerlink" title="3.1 dfs命令:(*使用较多的参数)"></a>3.1 dfs命令:(*使用较多的参数)</h5><p>[hadoop@warehouse001 bin]$ hdfs dfs<br>Usage: hadoop fs [generic options]<br>        [-appendToFile <localsrc> … <dst>]<br>      *  [-cat [-ignoreCrc] <src> …]<br>        [-checksum <src> …]<br>      *  [-chgrp [-R] GROUP PATH…]<br>      *[-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>      *  [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>      * [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]    ==&gt;put<br>      *[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]    ==&gt;get<br>        [-count [-q] [-h] [-v] [-x] <path> …]<br>      *  [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>        [-createSnapshot <snapshotDir> [<snapshotName>]]<br>        [-deleteSnapshot <snapshotDir> <snapshotName>]<br>        [-df [-h] [<path> …]]<br>      *  [-du [-s] [-h] [-x] <path> …]<br>        [-expunge]<br>      *  [-find <path> … <expression> …]<br>      *  [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-getfacl [-R] <path>]<br>        [-getfattr [-R] {-n name | -d} [-e en] <path>]<br>        [-getmerge [-nl] <src> <localdst>]<br>      *  [-help [cmd …]]<br>      *  [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> …]]<br>      *  [-mkdir [-p] <path> …]<br>        [-moveFromLocal <localsrc> … <dst>]<br>        [-moveToLocal <src> <localdst>]<br>      * [-mv <src> … <dst>]    【生产不建议使用移动，原因是移动过程中假如有问题，会导致数据不全。建议使用cp到目标端，验证通过再删除源端】<br>      *  [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-renameSnapshot <snapshotDir> <oldName> <newName>]<br>      * [-rm [-f] [-r|-R] [-skipTrash] <src> …]    【不建议使用skipTrash参数，会跳过回收站直接删除】<br>      * [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[–set <acl_spec> <path>]]<br>        [-setfattr {-n name [-v value] | -x name} <path>]<br>        [-setrep [-R] [-w] <rep> <path> …]<br>        [-stat [format] <path> …]<br>        [-tail [-f] <file>]<br>        [-test -[defsz] <path>]<br>        [-text [-ignoreCrc] <src> …]<br>        [-touchz <path> …]<br>        [-usage [cmd …]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><h5 id="3-2-dfsadmin"><a href="#3-2-dfsadmin" class="headerlink" title="3.2 dfsadmin"></a>3.2 dfsadmin</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin<br>Usage: hdfs dfsadmin<br>Note: Administrative commands can only be run as the HDFS superuser.<br>        [-report [-live] [-dead] [-decommissioning]]    【可以检查节点状况】<br>        [-safemode &lt;enter | leave | get | wait&gt;]    【安全模式】<br>        [-saveNamespace]<br>        [-rollEdits]<br>        [-restoreFailedStorage true|false|check]<br>        [-refreshNodes]<br>        [-setQuota <quota> <dirname>…<dirname>]<br>        [-clrQuota <dirname>…<dirname>]<br>        [-setSpaceQuota <quota> <dirname>…<dirname>]<br>        [-clrSpaceQuota <dirname>…<dirname>]<br>        [-finalizeUpgrade]<br>        [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]<br>        [-refreshServiceAcl]<br>        [-refreshUserToGroupsMappings]<br>        [-refreshSuperUserGroupsConfiguration]<br>        [-refreshCallQueue]<br>        [-refresh <a href="host:ipc_port">host:ipc_port</a> <key> [arg1..argn]<br>        [-reconfig &lt;datanode|…&gt; <a href="host:ipc_port">host:ipc_port</a> &lt;start|status|properties&gt;]<br>        [-printTopology]<br>        [-refreshNamenodes datanode_host:ipc_port]<br>        [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]<br>        [-setBalancerBandwidth <bandwidth in bytes per second>]<br>        [-fetchImage <local directory>]<br>        [-allowSnapshot <snapshotDir>]<br>        [-disallowSnapshot <snapshotDir>]<br>        [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]    【shutdown节点】<br>        [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]<br>        [-metasave filename]<br>        [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]<br>        [-listOpenFiles [-blockingDecommission] [-path <path>]]<br>        [-help [cmd]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><hr><h5 id="3-3-dfsadmin"><a href="#3-3-dfsadmin" class="headerlink" title="3.3 dfsadmin"></a>3.3 dfsadmin</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -report<br>2018/11/29 19:03:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Configured Capacity: 42139451392 (39.25 GB)<br>Present Capacity: 26043494400 (24.25 GB)<br>DFS Remaining: 26043150336 (24.25 GB)<br>DFS Used: 344064 (336 KB)<br>DFS Used%: 0.00%<br>Under replicated blocks: 0<br>Blocks with corrupt replicas: 0<br>Missing blocks: 0<br>Missing blocks (with replication factor 1): 0</p><hr><p>Live datanodes (1):</p><p>Name: 172.23.75.57:50010 (warehouse001)<br>Hostname: warehouse001<br>Decommission Status : Normal<br>Configured Capacity: 42139451392 (39.25 GB)<br>DFS Used: 344064 (336 KB)<br>Non DFS Used: 13931802624 (12.98 GB)<br>DFS Remaining: 26043150336 (24.25 GB)<br>DFS Used%: 0.00%<br>DFS Remaining%: 61.80%<br>Configured Cache Capacity: 0 (0 B)<br>Cache Used: 0 (0 B)<br>Cache Remaining: 0 (0 B)<br>Cache Used%: 100.00%<br>Cache Remaining%: 0.00%<br>Xceivers: 1<br>Last contact: Sun Nov 29 19:03:57 CST 2018</p><hr><p>[hadoop@warehouse001 bin]$ hdfs haadmin<br>Usage: DFSHAAdmin [-ns <nameserviceId>]<br>    [-transitionToActive <serviceId> [–forceactive]]<br>    [-transitionToStandby <serviceId>]<br>    [-failover [–forcefence] [–forceactive] <serviceId> <serviceId>]<br>    [-getServiceState <serviceId>]<br>    [-checkHealth <serviceId>]<br>    [-help <command>]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><hr><h5 id="3-4-fsck"><a href="#3-4-fsck" class="headerlink" title="3.4 fsck"></a>3.4 fsck</h5><p>[hadoop@warehouse001 bin]$ hdfs fsck /    【健康检查】<br>2018/11/29 19:07:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Connecting to namenode via <a href="http://warehouse001:50070/fsck?ugi=hadoop&amp;path=/">http://warehouse001:50070/fsck?ugi=hadoop&amp;path=%2F</a><br>FSCK started by hadoop (auth:SIMPLE) from /172.23.75.57 for path / at Sun Nov 29 19:07:50 CST 2020<br>…………….Status: HEALTHY<br> Total size:    202382 B<br> Total dirs:    16<br> Total files:   16<br> Total symlinks:                0<br> Total blocks (validated):      14 (avg. block size 14455 B)<br> Minimally replicated blocks:   14 (100.0 %)<br> Over-replicated blocks:        0 (0.0 %)<br> Under-replicated blocks:       0 (0.0 %)<br> Mis-replicated blocks:         0 (0.0 %)<br> Default replication factor:    1<br> Average block replication:     1.0<br> Corrupt blocks:                0<br> Missing replicas:              0 (0.0 %)<br> Number of data-nodes:          1<br> Number of racks:               1<br>FSCK ended at Sun Nov 29 19:07:50 CST 2020 in 3 milliseconds</p><p>The filesystem under path ‘/‘ is HEALTHY</p><h4 id="4-安全模式"><a href="#4-安全模式" class="headerlink" title="4.安全模式"></a>4.安全模式</h4><h5 id="4-1-检查安全模式状态"><a href="#4-1-检查安全模式状态" class="headerlink" title="4.1 检查安全模式状态:"></a>4.1 检查安全模式状态:</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -safemode get<br>2018/11/29 19:12:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Safe mode is OFF</p><h5 id="4-2-进入安全模式"><a href="#4-2-进入安全模式" class="headerlink" title="4.2 进入安全模式"></a>4.2 进入安全模式</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -safemode enter<br>2018/11/29 19:13:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Safe mode is ON</p><h5 id="4-3-安全模式下是否可以读写"><a href="#4-3-安全模式下是否可以读写" class="headerlink" title="4.3 安全模式下是否可以读写"></a>4.3 安全模式下是否可以读写</h5><p>[hadoop@warehouse001 bin]$ echo 123 &gt; 3.log<br>[hadoop@warehouse001 bin]$ cat 3.log<br>123<br>[hadoop@warehouse001 bin]$ hdfs dfs -put 3.log /<br>2018/11/29 19:14:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>put: Cannot create file/3.log.<em>COPYING</em>. Name node is in safe mode.</p><hr><p>[hadoop@warehouse001 bin]$ hdfs dfs -ls /<br>2018/11/29 19:15:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Found 3 items<br>drwx——   - hadoop supergroup          0 2020-11-24 23:06 /tmp<br>drwxr-xr-x   - hadoop supergroup          0 2020-11-22 16:02 /user<br>drwxr-xr-x   - hadoop supergroup          0 2020-11-24 23:06 /wordcount</p><p>结论:安全模式下可读,但不可写</p><p>安全模式,未来必然hdfs查看日志出现安全模式的英文单词,必然说明你的hdfs集群的有问题的,相当于处于一个保护模式.</p><p>一般需要你尝试手动执行命令离开安全模式(有可能会失败,根据问题去解决问题)</p><h5 id="4-4-维护操作-主动进入安全模式-维护操作"><a href="#4-4-维护操作-主动进入安全模式-维护操作" class="headerlink" title="4.4 维护操作(主动进入安全模式,维护操作)"></a>4.4 维护操作(主动进入安全模式,维护操作)</h5><p>保证这个时间段hdfs不会有新的数据写入,注意通知相关部门.</p><h4 id="5-回收站"><a href="#5-回收站" class="headerlink" title="5.回收站"></a>5.回收站</h4><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml</a></p><h5 id="5-1-启用回收站"><a href="#5-1-启用回收站" class="headerlink" title="5.1 启用回收站"></a>5.1 启用回收站</h5><p>core-default.xml加入如下配置:(单位是分钟)</p><pre class=" language-xml"><code class="language-xml">   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10080<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001 sbin]$ hdfs dfs -rm /wordcount/input/1.log<br>2018/11/29 19:35:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>2018/11/29 19:35:57 INFO fs.TrashPolicyDefault: Moved: ‘hdfs://warehouse001:9000/wordcount/input/1.log’ to trash at: hdfs://warehouse001:9000/user/hadoop/.Trash/Current/wordcount/input/1.log</p><h5 id="5-2-回收站位置"><a href="#5-2-回收站位置" class="headerlink" title="5.2 回收站位置"></a>5.2 回收站位置</h5><p>hdfs://warehouse001:9000/user/hadoop/.Trash/Current/wordcount/input/1.log</p><p>生产必须要开启回收站,且回收站默认时间尽量长点.</p><p>涉及删除不要使用-skipTrash,以防万一.</p><p>恢复文件直接mv就可以了.</p><h4 id="6-各个节点平衡"><a href="#6-各个节点平衡" class="headerlink" title="6.各个节点平衡"></a>6.各个节点平衡</h4><p>开启节点平衡</p><p>[hadoop@warehouse001 sbin]$ ./start-balancer.sh<br>starting balancer, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.out<br>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</p><hr><p>[hadoop@warehouse001 sbin]$ cat  /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.out<br>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved<br>The cluster is balanced. Exiting…<br>Nov 29, 2020 7:41:28 PM           0                  0 B                 0 B               -1 B<br>Nov 29, 2020 7:41:28 PM  Balancing took 1.47 seconds<br>f) unlimited<br>pending signals                 (-i) 30128<br>max locked memory       (kbytes, -l) 64<br>max memory size         (kbytes, -m) unlimited<br>open files                      (-n) 65535<br>pipe size            (512 bytes, -p) 8<br>POSIX message queues     (bytes, -q) 819200<br>real-time priority              (-r) 0<br>stack size              (kbytes, -s) 8192<br>cpu time               (seconds, -t) unlimited<br>max user processes              (-u) 4096<br>virtual memory          (kbytes, -v) unlimited<br>file locks                      (-x) unlimited</p><hr><p>[hadoop@warehouse001 sbin]$ cat  /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.log<br>2020-11-29 19:41:27,417 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs://warehouse001:9000]<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, run during upgrade = false]<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []<br>2020-11-29 19:41:27,504 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)<br>2020-11-29 19:41:28,315 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)<br>2018-11-29 19:41:28,331 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.23.75.57:50010<br>2020-11-29 19:41:28,332 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 over-utilized: []<br>2020-11-29 19:41:28,332 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 underutilized: []</p><hr><p>threshold = 10.0【阈值】    </p><p>各个节点的使用率-平均磁盘使用率&lt;10%</p><p>生产上可以写个定时脚本,在业务低谷时定时执行.</p><p>dfs.datanode.balance.bandwidthPerSec             10m    【平衡带宽调节参数】</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>如何查看使用率:web,report等等</p><h4 id="7-单个节点多块磁盘平衡"><a href="#7-单个节点多块磁盘平衡" class="headerlink" title="7.单个节点多块磁盘平衡"></a>7.单个节点多块磁盘平衡</h4><p>磁盘挂载目录:</p><table><thead><tr><th>dfs.datanode.data.dir</th><th>file://${hadoop.tmp.dir}/dfs/data</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/data01/dfs/dn,/data02/dfs/dn,/data03/dfs/dn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>#具体看磁盘挂载路径</code></pre><p>hdfs-default.xml配置节点信息</p><p>Apache hadoop2.x 不支持    dfs.disk.balancer.enabled 搜索不到<br><a href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>Apache hadoop3.x 支持           dfs.disk.balancer.enabled 搜索到 是true<br><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>CDH    hadoop2.x 支持           dfs.disk.balancer.enabled 搜索到 是false<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>如何去执行呢?<br>文档：<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><pre class=" language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.disk.balancer.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001 hadoop]$ hdfs diskbalancer -plan warehouse001<br>20/11/28 22:37:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>20/11/28 22:37:02 INFO planner.GreedyPlanner: Starting plan for Node : warehouse001:50020<br>20/11/28 22:37:02 INFO planner.GreedyPlanner: Compute Plan for Node : warehouse001:50020 took 1 ms<br>20/11/28 22:37:03 INFO command.Command: No plan generated. DiskBalancing not needed for node: warehouse001 threshold used: 10.0</p><p>hdfs diskbalancer -execute warehouse001.plan.json 执行<br>hdfs diskbalancer -query warehouse001 </p><h4 id="遇到问题的解决方式"><a href="#遇到问题的解决方式" class="headerlink" title="遇到问题的解决方式:"></a>遇到问题的解决方式:</h4><p>先自己分析,必须找到log,找到错误</p><p>百度谷歌搜素</p><p>问老师,同事,群友</p><p>apache issue</p><p>源代码导入IDEA进行debug</p><h4 id="日志的查找方法"><a href="#日志的查找方法" class="headerlink" title="日志的查找方法:"></a>日志的查找方法:</h4><p>以mysql为例:</p><p>配置文件 my.conf    data/hostname.err文件</p><p>当前目录的log文件夹</p><p>/var/log</p><p>ps -ef 查看进程描述    如:–log-error=/usr/local/mysql/data/hostname.err</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop03之hdfs架构</title>
      <link href="2018/11/24/2020-11-25-quannnxu-hadoop3/"/>
      <url>2018/11/24/2020-11-25-quannnxu-hadoop3/</url>
      
        <content type="html"><![CDATA[<h4 id="1-HDFS主从架构"><a href="#1-HDFS主从架构" class="headerlink" title="1.HDFS主从架构"></a>1.HDFS主从架构</h4><h5 id="namenode：nn名称节点"><a href="#namenode：nn名称节点" class="headerlink" title="namenode：nn名称节点"></a>namenode：nn名称节点</h5><p>a.文件的名称</p><p>b.文件的目录结构</p><p>c.文件的属性    权限    副本数    创建时间</p><p>[hadoop@warehouse001 ~]$ hdfs dfs -ls /<br>20/11/25 21:17:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Found 3 items<br>drwx——   - hadoop supergroup          0 2018-11-24 23:06 /tmp<br>drwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /user<br>drwxr-xr-x   - hadoop supergroup          0 2018-11-24 23:06 /wordcount</p><p>d.一个文件被对应切割那些数据块(包含副本数的块) ==&gt;对应分布在哪些datenode</p><p>blockmap 块映射  nn是不会持久化内存这种映射关系</p><p>是通过集群的启动和运行时,dn定期汇报blockreport给nn,然后再内存中动态维护这种映射关系</p><p>作用:</p><p>管理文件系统的命名空间,启示就是维护文件系统树的文件和文件夹</p><p>镜像文件    fsimage</p><p>编辑日志文件    editlogs</p><p>[hadoop@warehouse001 current]$ pwd<br>/home/hadoop/tmp/dfs/name/current</p><p>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 20:06 edits_0000000000000000395-0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 21:06 edits_0000000000000000397-0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop 1048576 Nov 26 21:06 edits_inprogress_0000000000000000399<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 20:06 fsimage_0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 20:06 fsimage_0000000000000000396.md5<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 21:06 fsimage_0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 21:06 fsimage_0000000000000000398.md5</p><h5 id="secondary-namenode-sn-第二名称节点"><a href="#secondary-namenode-sn-第二名称节点" class="headerlink" title="secondary namenode:sn    第二名称节点"></a>secondary namenode:sn    第二名称节点</h5><p>a.fsimage editlog文件拿过来合并  备份 推送给nn</p><p>[hadoop@warehouse001 current]$ pwd<br>/home/hadoop/tmp/dfs/namesecondary/current</p><p>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 20:06 edits_0000000000000000395-0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 21:06 edits_0000000000000000397-0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 20:06 fsimage_0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 20:06 fsimage_0000000000000000396.md5<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 21:06 fsimage_0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 21:06 fsimage_0000000000000000398.md5<br>-rw-rw-r– 1 hadoop hadoop     204 Nov 26 21:06 VERSION</p><p>将nn的 fsimage_0000000000000000396</p><p>edits_0000000000000000397-0000000000000000398    ==&gt;检测点动作 checkpoint合并 fsimage_0000000000000000398    将398推送给nn</p><p>而新的读写记录则在 edits_inprogress_0000000000000000399编辑日志里</p><p>dfs.namenode.checkpoint.period  3600<br>dfs.namenode.checkpoint.txns    1000000</p><p>早期为了解决nn是单点的，单点故障，增加一个snn，1小时的checkpoint<br>虽然能够减轻单点故障的带来的数据丢失风险，但是生产上不允许使用snn</p><p>11:00 checkpoint<br>11:30  数据一直在写 突然nn硬盘挂了  无法恢复<br>拿snn节点的最新的fsimage，那么只能恢复11点的数据</p><p>在生产上是不允许snn，是使用HA 高可靠，是通过配置另外一个实时的备份nn节点，<br>随时等待老大active nn 挂掉，然后成为老大</p><h5 id="datanode：-数据节点-dn"><a href="#datanode：-数据节点-dn" class="headerlink" title="datanode： 数据节点 dn"></a>datanode： 数据节点 dn</h5><p>a.存储数据块 和 数据块的校验和</p><p>[hadoop@warehouse001 subdir0]$ ll<br>-rw-rw-r– 1 hadoop hadoop     58 Nov 24 23:07 blk_1073741843<br>-rw-rw-r– 1 hadoop hadoop     11 Nov 24 23:07 blk_1073741843_1019.meta<br>-rw-rw-r– 1 hadoop hadoop    349 Nov 24 23:07 blk_1073741844<br>-rw-rw-r– 1 hadoop hadoop     11 Nov 24 23:07 blk_1073741844_1020.meta<br>-rw-rw-r– 1 hadoop hadoop  33585 Nov 24 23:07 blk_1073741845<br>-rw-rw-r– 1 hadoop hadoop    271 Nov 24 23:07 blk_1073741845_1021.meta<br>-rw-rw-r– 1 hadoop hadoop 141109 Nov 24 23:07 blk_1073741846<br>-rw-rw-r– 1 hadoop hadoop   1111 Nov 24 23:07 blk_1073741846_1022.meta<br>[hadoop@warehouse001 subdir0]$ pwd<br>/home/hadoop/tmp/dfs/data/current/BP-1844033615-172.23.75.57-1606031076865/current/finalized/subdir0/subdir0</p><p>b.每隔一定的时间去发送blockreport<br>dfs.blockreport.intervalMsec          21600000        ==&gt;6h<br>dfs.datanode.directoryscan.interval   21600         ==&gt;6h </p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>补充:<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p><h4 id="2-HDFS写流程-面试"><a href="#2-HDFS写流程-面试" class="headerlink" title="2.HDFS写流程  面试"></a>2.HDFS写流程  面试</h4><p>对用户是无感知的</p><h5 id="2-1-HDFS-Client调用FileSystem-create-filePath-方法，去和NN进行【RPC】通信。"><a href="#2-1-HDFS-Client调用FileSystem-create-filePath-方法，去和NN进行【RPC】通信。" class="headerlink" title="2.1 HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信。"></a>2.1 HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信。</h5><p>NN会去check这个文件是否存在，是否有权限创建这个文件。<br>假如都可以，就创建一个新的文件，但是这时没有数据，是不关联任何block的。<br>NN根据文件的大小，根据块大小 副本数，计算要上传多少的块和对应哪些DN节点上。<br>最终这个信息返回给客户端【FSDataOutputStream】对象</p><h5 id="2-2Client-调用客户端【FSDataOutputStream】对象的write方法，"><a href="#2-2Client-调用客户端【FSDataOutputStream】对象的write方法，" class="headerlink" title="2.2Client 调用客户端【FSDataOutputStream】对象的write方法，"></a>2.2Client 调用客户端【FSDataOutputStream】对象的write方法，</h5><p>根据【副本放置策略】，将第一个块的第一个副本写到DN1，写完复制到DN2，写完再复制到DN3.<br>当第三个副本写完，就返回一个ack package确认包给DN2,DN2接收到ack 加上自己写完，<br>发送ack给DN1，DN1接收到ack加上自己写完，就发送ack给客户端【FSDataOutputStream】对象，<br>告诉它第一个块三副本写完了。<br>以此类推。</p><h5 id="2-3-当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，"><a href="#2-3-当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，" class="headerlink" title="2.3.当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，"></a>2.3.当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，</h5><p>关闭输出流。再次调用FileSystem.complete方法 ，告诉nn文件写成功。</p><p>伪分布式 1台dn，副本数参数必须设置是1吗？<br>设置2 也可以写，显示丢失一个副本</p><p>生产上分布式  3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>可以的 </p><p>生产上分布式  &gt;3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>肯定写</p><p><img src="https://i.loli.net/2020/11/26/xPDC61rl8XesfKt.png"></p><h4 id="3-HDFS读流程-面试"><a href="#3-HDFS读流程-面试" class="headerlink" title="3.HDFS读流程  面试"></a>3.HDFS读流程  面试</h4><h5 id="3-1Client调用FileSystem的open-filePath-，"><a href="#3-1Client调用FileSystem的open-filePath-，" class="headerlink" title="3.1Client调用FileSystem的open(filePath)，"></a>3.1Client调用FileSystem的open(filePath)，</h5><p>与NN进行【rpc】通信，返回该文件的部分或者全部的block列表<br>也就是返回【FSDataIntputStream】对象</p><h5 id="3-2Client调度【FSDataIntputStream】对象的read方法，"><a href="#3-2Client调度【FSDataIntputStream】对象的read方法，" class="headerlink" title="3.2Client调度【FSDataIntputStream】对象的read方法，"></a>3.2Client调度【FSDataIntputStream】对象的read方法，</h5><p>与第一个块的最近的DN的进行读取，读取完成后，会check，假如ok就关闭与DN通信。<br>假如不ok，就会记录块+DN的信息，下次就不从这个节点读取。那么从第二个节点读取。</p><p>然后与第二个块的最近的DN的进行读取，以此类推。<br>假如当block的列表全部读取完成，文件还没结束，就调用FileSystem从NN获取下一批次的block列表。</p><h5 id="3-3-Client调用【FSDataIntputStream】对象的close方法，关闭输入流。"><a href="#3-3-Client调用【FSDataIntputStream】对象的close方法，关闭输入流。" class="headerlink" title="3.3.Client调用【FSDataIntputStream】对象的close方法，关闭输入流。"></a>3.3.Client调用【FSDataIntputStream】对象的close方法，关闭输入流。</h5><p><img src="https://i.loli.net/2020/11/26/xPDC61rl8XesfKt.png"></p><h4 id="4-副本放置策略-不光光面试需要，生产也需要"><a href="#4-副本放置策略-不光光面试需要，生产也需要" class="headerlink" title="4.副本放置策略  不光光面试需要，生产也需要"></a>4.副本放置策略  不光光面试需要，生产也需要</h4><p><a href="https://www.bilibili.com/video/BV1eE411p7un">https://www.bilibili.com/video/BV1eE411p7un</a></p><p>生产上读写操作  尽量选择DN节点操作<br>第一个副本：<br>放置在上传的DN节点上，就近原则，节省IO(网络IO)<br>假如非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点。</p><p>第二个副本:<br>放置在第一个副本的不同机架上的某个节点</p><p>第三个副本:<br>与第二个副本放置同一个机架的不同节点上。</p><p>如果副本数设置更多，随机放。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop02之yarn部署及简析</title>
      <link href="2018/11/15/2020-11-22-quannnxu-hadoop2/"/>
      <url>2018/11/15/2020-11-22-quannnxu-hadoop2/</url>
      
        <content type="html"><![CDATA[<h4 id="主从架构"><a href="#主从架构" class="headerlink" title="主从架构"></a>主从架构</h4><p>ResourceManager daemon and NodeManager daemon</p><h4 id="1-yarn部署"><a href="#1-yarn部署" class="headerlink" title="1.yarn部署"></a>1.yarn部署</h4><h5 id="1-1-配置文件"><a href="#1-1-配置文件" class="headerlink" title="1.1 配置文件"></a>1.1 配置文件</h5><p>[hadoop@warehouse001hadoop]$ cp mapred-site.xml.template mapred-site.xml<br>[hadoop@warehouse001hadoop]$ vi mapred-site.xml</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001hadoop]$ vi yarn-site.xml </p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>open: <a href="http://114.67.101.143:8088/cluster%E5%AE%98%E7%BD%91">http://114.67.101.143:8088/cluster官网</a></p><p>启动:</p><p>[hadoop@warehouse001 hadoop]$ sbin/start-yarn.sh </p><p>查看进程:</p><p>[hadoop@warehouse001 hadoop]$ jps<br>12496 Jps<br>30954 ResourceManager<br>29050 NameNode<br>29179 DataNode<br>29340 SecondaryNameNode<br>31182 NodeManager</p><h4 id="2-案例"><a href="#2-案例" class="headerlink" title="2.案例"></a>2.案例</h4><p>wordcount</p><pre class=" language-linux"><code class="language-linux">#配置环境变量[hadoop@warehouse001 hadoop]$ cd [hadoop@warehouse001 ~]$ which hadoop/usr/bin/which: no hadoop in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)[hadoop@warehouse001 ~]$ vi .bashrc # .bashrc# Source global definitionsif [ -f /etc/bashrc ]; then        . /etc/bashrcfi# Uncomment the following line if you don't like systemctl's auto-paging feature:# export SYSTEMD_PAGER=export HADOOP_HOME=/home/hadoop/app/hadoopexport PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH# User specific aliases and functions[hadoop@warehouse001 ~]$ source .bashrc [hadoop@warehouse001 ~]$ which hadoop~/app/hadoop/bin/hadoop#造数据[hadoop@warehouse001 ~]$ cd data/[hadoop@warehouse001 data]$ mkdir input[hadoop@warehouse001 data]$ cd input/[hadoop@warehouse001 input]$ vi 1.loga b cwww.quanxu.comxiaominglihua1112a bc#上传hdfs[hadoop@warehouse001 input]$ hdfs dfs -ls /20/11/24 22:42:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 1 itemsdrwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /user[hadoop@warehouse001 input]$ hdfs dfs -mkdir -p /wordcount/input20/11/24 22:43:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@warehouse001 input]$ hdfs dfs -ls /20/11/24 22:43:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /userdrwxr-xr-x   - hadoop supergroup          0 2018-11-24 22:43 /wordcountdrwxr-xr-x   - hadoop supergroup          0 2018-11-24 22:43 /wordcount/input[hadoop@warehouse001 input]$ hdfs dfs -put 1.log /wordcount/input20/11/24 22:45:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@warehouse001 hadoop]$ cd src/[hadoop@warehouse001 src]$ lltotal 200drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 build-rw-r--r--  1 hadoop hadoop 12096 Jun  3  2018 BUILDING.txtdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 dev-supportdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-assembliesdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-build-toolsdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-clientdrwxr-xr-x 10 hadoop hadoop  4096 Jun  3  2018 hadoop-common-projectdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-distdrwxr-xr-x  6 hadoop hadoop  4096 Jun  3  2018 hadoop-hdfs-projectdrwxr-xr-x 10 hadoop hadoop  4096 Jun  3  2018 hadoop-mapreduce1-projectdrwxr-xr-x  9 hadoop hadoop  4096 Jun  3  2018 hadoop-mapreduce-projectdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-maven-pluginsdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-miniclusterdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-projectdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-project-distdrwxr-xr-x 18 hadoop hadoop  4096 Jun  3  2018 hadoop-toolsdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-yarn-project-rw-r--r--  1 hadoop hadoop 85063 Jun  3  2018 LICENSE.txt-rw-r--r--  1 hadoop hadoop 14978 Jun  3  2018 NOTICE.txt-rw-r--r--  1 hadoop hadoop 19039 Jun  3  2018 pom.xml-rw-r--r--  1 hadoop hadoop  1366 Jun  3  2018 README.txt[hadoop@warehouse001 src]$ cd hadoop-mapreduce-projecthadoop@warehouse001 src]$ cd hadoop-mapreduce-project[hadoop@warehouse001 hadoop-mapreduce-project]$ lltotal 328drwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 bin-rw-r--r--  1 hadoop hadoop   2018 Jun  3  2018 CHANGES.MAPREDUCE-2841.txt-rw-r--r--  1 hadoop hadoop 293811 Jun  3  2018 CHANGES.txtdrwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 confdrwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 dev-supportdrwxr-xr-x 10 hadoop hadoop   4096 Jun  3  2018 hadoop-mapreduce-clientdrwxr-xr-x  4 hadoop hadoop   4096 Jun  3  2018 hadoop-mapreduce-examplesdrwxr-xr-x  3 hadoop hadoop   4096 Jun  3  2018 lib-rw-r--r--  1 hadoop hadoop  11091 Jun  3  2018 pom.xml[hadoop@warehouse001 hadoop-mapreduce-project]$ cd hadoop-mapreduce-examples[hadoop@warehouse001 hadoop-mapreduce-examples]$ lltotal 16drwxr-xr-x 2 hadoop hadoop 4096 Jun  3  2018 dev-support-rw-r--r-- 1 hadoop hadoop 5097 Jun  3  2018 pom.xmldrwxr-xr-x 4 hadoop hadoop 4096 Jun  3  2018 src[hadoop@warehouse001 hadoop-mapreduce-examples]$ cd src[hadoop@warehouse001 src]$ cd main[hadoop@warehouse001 main]$ cd java/[hadoop@warehouse001 java]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 org[hadoop@warehouse001 java]$ cd org/[hadoop@warehouse001 org]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 apache[hadoop@warehouse001 org]$ cd apache/[hadoop@warehouse001 apache]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 hadoop[hadoop@warehouse001 apache]$ cd hadoop/[hadoop@warehouse001 hadoop]$ lltotal 4drwxr-xr-x 5 hadoop hadoop 4096 Jun  3  2018 examples[hadoop@warehouse001 hadoop]$ cd examples/[hadoop@warehouse001 examples]$ lltotal 204-rw-r--r-- 1 hadoop hadoop  2897 Jun  3  2018 AggregateWordCount.java-rw-r--r-- 1 hadoop hadoop  3016 Jun  3  2018 AggregateWordHistogram.java-rw-r--r-- 1 hadoop hadoop 21254 Jun  3  2018 BaileyBorweinPlouffe.javadrwxr-xr-x 2 hadoop hadoop  4096 Jun  3  2018 dancing-rw-r--r-- 1 hadoop hadoop 13495 Jun  3  2018 DBCountPageView.java-rw-r--r-- 1 hadoop hadoop  4301 Jun  3  2018 ExampleDriver.java-rw-r--r-- 1 hadoop hadoop  3730 Jun  3  2018 Grep.java-rw-r--r-- 1 hadoop hadoop  7033 Jun  3  2018 Join.java-rw-r--r-- 1 hadoop hadoop  8111 Jun  3  2018 MultiFileWordCount.java-rw-r--r-- 1 hadoop hadoop   853 Jun  3  2018 package.htmldrwxr-xr-x 3 hadoop hadoop  4096 Jun  3  2018 pi-rw-r--r-- 1 hadoop hadoop 12628 Jun  3  2018 QuasiMonteCarlo.java-rw-r--r-- 1 hadoop hadoop 40575 Jun  3  2018 RandomTextWriter.java-rw-r--r-- 1 hadoop hadoop 10573 Jun  3  2018 RandomWriter.java-rw-r--r-- 1 hadoop hadoop  7809 Jun  3  2018 SecondarySort.java-rw-r--r-- 1 hadoop hadoop  8167 Jun  3  2018 Sort.javadrwxr-xr-x 3 hadoop hadoop  4096 Jun  3  2018 terasort-rw-r--r-- 1 hadoop hadoop  3297 Jun  3  2018 WordCount.java-rw-r--r-- 1 hadoop hadoop  6327 Jun  3  2018 WordMean.java-rw-r--r-- 1 hadoop hadoop  7084 Jun  3  2018 WordMedian.java-rw-r--r-- 1 hadoop hadoop  7253 Jun  3  2018 WordStandardDeviation.java#下载到本地[hadoop@warehouse001 examples]$ sz WordCount.javarz?a? zmodem ′???￡  °′ Ctrl+C ???￡Transferring WordCount.java...  100%       3 KB    3 KB/s 00:00:01       0 Errors[hadoop@warehouse001 hadoop]$ find ./ -name *example*.jar./share/hadoop/mapreduce1/hadoop-examples-2.6.0-mr1-cdh5.16.2.jar./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-test-sources.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-sources.jarhadoop jar \./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \wordcount /wordcount/input /wordcount/output1 [hadoop@warehouse001 hadoop]$ hadoop jar \> ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \> wordcount /wordcount/input /wordcount/output1 20/11/24 23:06:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable20/11/24 23:06:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803220/11/24 23:06:52 INFO input.FileInputFormat: Total input paths to process : 120/11/24 23:06:52 INFO mapreduce.JobSubmitter: number of splits:1【了解split切分的规则】20/11/24 23:06:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1606044808202_000120/11/24 23:06:53 INFO impl.YarnClientImpl: Submitted application application_1606044808202_0001【分配的作业id号，web界面会显示】20/11/24 23:06:53 INFO mapreduce.Job: The url to track the job: http://warehouse001:8088/proxy/application_1606044808202_0001/20/11/24 23:06:53 INFO mapreduce.Job: Running job: job_1606044808202_000120/11/24 23:06:58 INFO mapreduce.Job: Job job_1606044808202_0001 running in uber mode : false【了解uber mode】20/11/24 23:06:58 INFO mapreduce.Job:  map 0% reduce 0%20/11/24 23:07:02 INFO mapreduce.Job:  map 100% reduce 0%20/11/24 23:07:07 INFO mapreduce.Job:  map 100% reduce 100%20/11/24 23:07:08 INFO mapreduce.Job: Job job_1606044808202_0001 completed successfully20/11/24 23:07:08 INFO mapreduce.Job: Counters: 49        File System Counters                FILE: Number of bytes read=96                FILE: Number of bytes written=286259                FILE: Number of read operations=0                FILE: Number of large read operations=0                FILE: Number of write operations=0                HDFS: Number of bytes read=160                HDFS: Number of bytes written=58                HDFS: Number of read operations=6                HDFS: Number of large read operations=0                HDFS: Number of write operations=2        Job Counters                 Launched map tasks=1                Launched reduce tasks=1【了解map、reduce个数的规则】                Data-local map tasks=1                Total time spent by all maps in occupied slots (ms)=1783                Total time spent by all reduces in occupied slots (ms)=1958                Total time spent by all map tasks (ms)=1783                Total time spent by all reduce tasks (ms)=1958                Total vcore-milliseconds taken by all map tasks=1783                Total vcore-milliseconds taken by all reduce tasks=1958                Total megabyte-milliseconds taken by all map tasks=1825792                Total megabyte-milliseconds taken by all reduce tasks=2004992        Map-Reduce Framework                Map input records=8                Map output records=11                Map output bytes=92                Map output materialized bytes=96                Input split bytes=111                Combine input records=11                Combine output records=8                Reduce input groups=8                Reduce shuffle bytes=96                Reduce input records=8                Reduce output records=8                Spilled Records=16                Shuffled Maps =1                Failed Shuffles=0                Merged Map outputs=1                GC time elapsed (ms)=89                CPU time spent (ms)=860                Physical memory (bytes) snapshot=489730048                Virtual memory (bytes) snapshot=5554565120                Total committed heap usage (bytes)=466092032        Shuffle Errors                BAD_ID=0                CONNECTION=0                IO_ERROR=0                WRONG_LENGTH=0                WRONG_MAP=0                WRONG_REDUCE=0        File Input Format Counters                 Bytes Read=49        File Output Format Counters                 Bytes Written=58</code></pre><p><img src="https://i.loli.net/2020/11/24/7s82mHTuvPDLntV.png"></p><p>如报错可进入history查看日志</p><h4 id="3-关于挖矿"><a href="#3-关于挖矿" class="headerlink" title="3.关于挖矿"></a>3.关于挖矿</h4><p><a href="https://segmentfault.com/a/1190000015264170">https://segmentfault.com/a/1190000015264170</a></p><p>[hadoop@warehouse001 hadoop]$ top</p><p>top - 23:20:34 up 8 days,  5:42,  1 user,  load average: 0.00, 0.01, 0.05<br>Tasks:  84 total,   1 running,  83 sleeping,   0 stopped,   0 zombie<br>%Cpu(s):  0.2 us,  0.2 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st<br>KiB Mem :  7734044 total,  2442532 free,  2900996 used,  2390516 buff/cache<br>KiB Swap:        0 total,        0 free,        0 used.  4533612 avail Mem </p><p>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND<br> 5931 root      20   0   42156   5532   2704 S   0.3  0.1   0:50.91 aliyun-service<br>29340 hadoop    20   0 2798988 248088  19424 S   0.3  3.2   1:12.20 java          </p><p>cpu过高，杀掉进程后，过段时间挖矿进程会重启</p><p>所以需要调整8088端口</p><p>在yarn-site.xml 新增:</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>$<span class="token entity" title="&#123;">&amp;#123;</span>yarn.resourcemanager.hostname<span class="token entity" title="&#125;">&amp;#125;</span>:7776<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>#重启</p><p>[hadoop@warehouse001 hadoop]$ sbin/stop-yarn.sh </p><p>[hadoop@warehouse001 hadoop]$ sbin/start-yarn.sh</p><p><img src="https://i.loli.net/2020/11/24/uaGfhgHEne2SVLB.png"></p><h4 id="4-jps命令"><a href="#4-jps命令" class="headerlink" title="4.jps命令"></a>4.jps命令</h4><h5 id="4-1-位置"><a href="#4-1-位置" class="headerlink" title="4.1 位置"></a>4.1 位置</h5><p>[hadoop@ruozedata001 hadoop]$ which jps<br>/usr/java/jdk1.8.0_181/bin/jps<br>[hadoop@ruozedata001 hadoop]$ </p><h5 id="4-2-使用"><a href="#4-2-使用" class="headerlink" title="4.2 使用"></a>4.2 使用</h5><p>[hadoop@ruozedata001 hadoop]$ jps<br>21828 NameNode<br>21959 DataNode<br>27577 NodeManager<br>27466 ResourceManager<br>22124 SecondaryNameNode</p><h5 id="4-3-对应的标识文件存储在哪里"><a href="#4-3-对应的标识文件存储在哪里" class="headerlink" title="4.3 对应的标识文件存储在哪里"></a>4.3 对应的标识文件存储在哪里</h5><p>[hadoop@warehouse001 hsperfdata_hadoop]$ ll<br>total 160<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 13863<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 14136<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29050<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29179<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29340<br>[hadoop@warehouse001 hsperfdata_hadoop]$ pwd<br>/tmp/hsperfdata_hadoop</p><h5 id="4-4-哪个用户可以查看"><a href="#4-4-哪个用户可以查看" class="headerlink" title="4.4 哪个用户可以查看"></a>4.4 哪个用户可以查看</h5><p>[root@warehouse001 ~]# jps<br>14389 Jps<br>13863 ResourceManager<br>14136 NodeManager<br>29050 NameNode<br>29179 DataNode<br>29340 SecondaryNameNode</p><p>root和当前用户</p><h5 id="4-5-出现process-information-unavailable描述"><a href="#4-5-出现process-information-unavailable描述" class="headerlink" title="4.5 出现process information unavailable描述"></a>4.5 出现process information unavailable描述</h5><p>当看见 process information unavailable<br>不能代表进程是存在 或者不存在，要当心，尤其使用jps命令来做脚本状态检测的<br>一般使用经典的 ps -ef | grep xxx命令去查看进程是否存在，<br>这才是真正的状态检测</p><p>但是: 比如spark thriftserver +hive 会启动一个driver 进程 110，<br>默认端口号 10001。由于该程序的内存泄露或者某种bug，导致<br>进程ps是存在的，10001端口号下线了，就不能够对外提供服务。</p><p>总结: 未来做任何程序的状态检测，必须通过端口号来。</p><p>CDH root用户，jps命令查看会有很多的 process information unavailable<br>ps -ef| grep xxx 查看是正确的<br>那么想要看到正常的表述，需要切换对应的用户，<br>比如su - hdfs(有可能你切换不过去，需要/etc/passwd文件的修正)<br>再执行jps命令</p><h5 id="4-6-删除之后是否影响服务重启呢"><a href="#4-6-删除之后是否影响服务重启呢" class="headerlink" title="4.6 删除之后是否影响服务重启呢"></a>4.6 删除之后是否影响服务重启呢</h5><p>经过测试验证，不影响重启</p><h4 id="5-pid文件"><a href="#5-pid文件" class="headerlink" title="5.pid文件"></a>5.pid文件</h4><h5 id="5-1-pid文件的位置"><a href="#5-1-pid文件的位置" class="headerlink" title="5.1 pid文件的位置"></a>5.1 pid文件的位置</h5><p>/tmp </p><p>[hadoop@warehouse001 tmp]$ ll</p><p>-rw-rw-r– 1 hadoop hadoop        6 Nov 22 15:59 hadoop-hadoop-datanode.pid</p><p>[hadoop@warehouse001 sbin]$ pwd<br>/home/hadoop/app/hadoop/sbin</p><p>停止进程脚本:</p><p>hadoop-daemon.sh</p><pre class=" language-shell"><code class="language-shell">    if [ -f $pid ]; then      TARGET_PID=`cat $pid`      if kill -0 $TARGET_PID > /dev/null 2>&1; then        echo stopping $command        kill $TARGET_PID        sleep $HADOOP_STOP_TIMEOUT        if kill -0 $TARGET_PID > /dev/null 2>&1; then          echo "$command did not stop gracefully after $HADOOP_STOP_TIMEOUT seconds: killing with kill -9"          kill -9 $TARGET_PID        fi      else        echo no $command to stop      fi      rm -f $pid    else      echo no $command to stop    fi    ;;  (*)    echo $usage    exit 1    ;;</code></pre><h5 id="5-2-删除之后是否影响服务重启呢"><a href="#5-2-删除之后是否影响服务重启呢" class="headerlink" title="5.2 删除之后是否影响服务重启呢"></a>5.2 删除之后是否影响服务重启呢</h5><p>经过测试验证，影响重启</p><h5 id="5-3-如何修改位置"><a href="#5-3-如何修改位置" class="headerlink" title="5.3 如何修改位置"></a>5.3 如何修改位置</h5><p>[hadoop@ruozedata001 hadoop]$ vi hadoop-env.sh<br>export HADOOP_PID_DIR=/home/hadoop/tmp</p><p>[hadoop@warehouse001 tmp]$ jps<br>17667 NameNode<br>17798 DataNode<br>17963 SecondaryNameNode<br>18077 Jps<br>[hadoop@warehouse001 tmp]$ cat hadoop-hadoop-datanode.pid<br>17798</p><h4 id="6-块-block"><a href="#6-块-block" class="headerlink" title="6.块 block"></a>6.块 block</h4><p>dfs.blocksize   128M<br>大文件  小文件</p><p>一缸水 260ml<br>瓶子的规格 128ml   ==》   dfs.blocksize<br>260/128=2 ….4ml<br>1 128ml<br>2 128ml<br>3 4ml</p><p>一个大文件 260m<br>1 128m<br>2 128m<br>3 4m    –只占用4m不会占用128m</p><p>存储到伪分布式hdfs上是                     3个块                   实际存储260m x 1=260m<br>      集群hdfs上&gt;=3节点  多副本机制3<br>      一个块会被连他自己 复制是3份    3x3= 9个块          实际存储260m x 3=780m</p><p>10个小文件 每个小文件 10m，那么伪分布式：<br>10个块 </p><p>namenode：维护一个文件被切割哪个块，这些块被存放到哪些机器</p><p>10条元数据</p><p>10个小文件 合并 1个文件100m,     那么伪分布式：<br>1个块</p><p>1条元数据</p><p>10个小文件10m VS 1个大文件100m<br>结果： 1个大文件对 nn的存储压力较小</p><p>再举例:<br>假设1亿个小文件，每个小文件10kb, 集群3副本机制，3亿个block，3亿个元数据<br>假如1亿个小文件，合并为1kw个 100m文件 , 集群3副本机制，3kw block，3kw元数据 </p><p>nn维护 3亿个元数据 还是 3kw元数据 的压力，谁轻松？<br>3kw轻松</p><p>元数据是存储在nn进程的内存里 ，那内存是一定的 8g</p><p>所以生产上:<br>尽量规避小文件在hdfs上的存储<br>a.数据传输到hdfs之前，提前合并<br>b.数据已经到hdfs，就定时的  业务低谷期，去合并冷文件<br>          写个脚本 每一天合并<br>          11-1 合并09-30数据<br>          11-2 合并10-1数据</p><p>​          一天卡一天去处理 </p><p>关于split切分的规则</p><p><a href="https://blog.csdn.net/jinywum/article/details/81458359">https://blog.csdn.net/jinywum/article/details/81458359</a></p><p>map、reduce任务个数的规则</p><p><a href="https://blog.csdn.net/zhanglh046/article/details/78567105">https://blog.csdn.net/zhanglh046/article/details/78567105</a></p><p>什么是uber模式</p><p>Uber模式简单地可以理解成JVM重用，该模式是2.x开始引入的；以Uber模式运行MR作业，所有的Map Tasks和Reduce Tasks将会在ApplicationMaster所在的容器（container）中运行，也就是说整个MR作业运行的过程只会启动AM container，因为不需要启动mapper 和 reducer containers，所以AM不需要和远程containers通信，整个过程简单了。</p><p>不是所有的MR作业都可以启用Uber模式，如果我们的MR作业输入的数据量非常小，启动Map container或Reduce container的时间都比处理数据要长，那么这个作业就可以考虑启用Uber模式运行，一般情况下，对小作业启用Uber模式运行会得到2x-3x的性能提升。</p><p>启用uber模式的要求非常严格，代码如下：</p><p>isUber = uberEnabled &amp;&amp; smallNumMapTasks &amp;&amp; smallNumReduceTasks &amp;&amp; smallInput &amp;&amp; smallMemory &amp;&amp; smallCpu &amp;&amp; notChainJob &amp;&amp; isValidUberMaxReduces;</p><ul><li><strong>uberEnabled</strong>：其实就是 mapreduce.job.ubertask.enable 参数的值，默认情况下为 false ；也就是说默认情况不启用Uber模式；</li><li><strong>smallNumMapTasks</strong>：启用Uber模式的作业Map的个数必须小于等于 mapreduce.job.ubertask.maxmaps 参数的值，该值默认为9；也计算说，在默认情况下，如果你想启用Uber模式，作业的Map个数必须小于10；</li><li><strong>smallNumReduceTasks</strong>：同理，Uber模式的作业Reduce的个数必须小于等于mapreduce.job.ubertask.maxreduces，该值默认为1；也计算说，在默认情况下，如果你想启用Uber模式，作业的Reduce个数必须小于等于1；</li><li><strong>smallInput</strong>：不是任何作业都适合启用Uber模式的，输入数据的大小必须小于等于 mapreduce.job.ubertask.maxbytes 参数的值，默认情况是HDFS一个文件块大小；</li><li><strong>smallMemory</strong>：因为作业是在AM所在的container中运行，所以要求我们设置的Map内存（mapreduce.map.memory.mb）和Reduce内存（mapreduce.reduce.memory.mb）必须小于等于 AM所在容器内存大小设置（yarn.app.mapreduce.am.resource.mb）；</li><li><strong>smallCpu</strong>：同理，Map配置的vcores（mapreduce.map.cpu.vcores）个数和 Reduce配置的vcores（mapreduce.reduce.cpu.vcores）个数也必须小于等于AM所在容器vcores个数的设置（yarn.app.mapreduce.am.resource.cpu-vcores）；</li><li><strong>notChainJob</strong>：此外，处理数据的Map class（mapreduce.job.map.class）和Reduce class（mapreduce.job.reduce.class）必须不是 ChainMapper 或 ChainReducer 才行；</li><li><strong>isValidUberMaxReduces</strong>：目前仅当Reduce的个数小于等于1的作业才能启用Uber模式。</li></ul><p>同时满足上面八个条件才能在作业运行的时候启动Uber模式。下面是一个启用Uber模式运行的作业运行成功的日志：</p><p>File System Counters FILE: Number of bytes read=215 FILE: Number of bytes written=505 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=1200 HDFS: Number of bytes written=274907 HDFS: Number of read operations=57 HDFS: Number of large read operations=0 HDFS: Number of write operations=11 Job Counters Launched map tasks=2 Launched reduce tasks=1 Other local map tasks=2 Total time spent by all maps in occupied slots (ms)=3664 Total time spent by all reduces in occupied slots (ms)=2492 TOTAL_LAUNCHED_UBERTASKS=3 NUM_UBER_SUBMAPS=2 NUM_UBER_SUBREDUCES=1 Map-Reduce Framework Map input records=2 Map output records=8 Map output bytes=82 Map output materialized bytes=85 Input split bytes=202 Combine input records=8 Combine output records=6 Reduce input groups=5 Reduce shuffle bytes=0 Reduce input records=6 Reduce output records=5 Spilled Records=12 Shuffled Maps =0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=65 CPU time spent (ms)=1610 Physical memory (bytes) snapshot=1229729792 Virtual memory (bytes) snapshot=5839392768 Total committed heap usage (bytes)=3087532032 File Input Format Counters Bytes Read=50 File Output Format Counters Bytes Written=41</p><p>细心的同学应该会发现里面多了 TOTAL_LAUNCHED_UBERTASKS、NUM_UBER_SUBMAPS 以及 NUM_UBER_SUBREDUCES 信息，以前需要启用Map Task 或 Reduce Task运行的工作直接在AM中运行，所有出现了NUM_UBER_SUBMAPS和原来Map Task个数一样；同理，NUM_UBER_SUBREDUCES 和Reduce Task个数一样。</p><p>作业:</p><p>1.作业输出的内容</p><p>2.拓展 hdfs小文件如何合并</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop01之安装部署</title>
      <link href="2018/11/10/2020-11-21-quannnxu-hadoop1/"/>
      <url>2018/11/10/2020-11-21-quannnxu-hadoop1/</url>
      
        <content type="html"><![CDATA[<h4 id="1-谈谈大数据"><a href="#1-谈谈大数据" class="headerlink" title="1.谈谈大数据"></a>1.谈谈大数据</h4><p>存储                      hdfs  hive  hbase  kudu<br>计算                      SQL 数据处理 、scala/java、spark、flink<br>资源和计算作业的调度分配  yarn</p><h4 id="2-hadoop官网"><a href="#2-hadoop官网" class="headerlink" title="2.hadoop官网"></a>2.hadoop官网</h4><p><a href="https://hadoop.apache.org/">https://hadoop.apache.org</a></p><p>hive.apache.org</p><h4 id="3-hadoop版本选择"><a href="#3-hadoop版本选择" class="headerlink" title="3.hadoop版本选择"></a>3.hadoop版本选择</h4><p>hadoop2.x    hadoop3.x</p><h4 id="4-hadoop认知"><a href="#4-hadoop认知" class="headerlink" title="4.hadoop认知"></a>4.hadoop认知</h4><p>广义: 以apache hadoop软件为主的生态圈 包含hive sqoop hbase kafka spark flink<br>狭义: apache hadoop软件<br>        hdfs          存储<br>        mapreduce     计算<br>        yarn          资源作业的分配调度</p><p>大数据平台： 存储是第一位；存储和计算是相辅相成的</p><h4 id="5-部署前准备"><a href="#5-部署前准备" class="headerlink" title="5.部署前准备"></a>5.部署前准备</h4><p>创建用户<br>useradd hadoop<br>su - hadoop<br>mkdir sourcecode software app log lib data tmp shell</p><p>cd software/</p><p>rz上传tar包</p><p>[hadoop@warehouse001 software]$ ll<br>total 424180<br>-rw-r–r– 1 root root 434354462 Nov 22 09:42 hadoop-2.6.0-cdh5.16.2.tar.gz</p><h4 id="6-jdk部署"><a href="#6-jdk部署" class="headerlink" title="6.jdk部署"></a>6.jdk部署</h4><p>[hadoop@warehouse001 software]$ which java<br>/usr/java/jdk1.8.0_181/bin/java</p><h4 id="7-hdfs安装"><a href="#7-hdfs安装" class="headerlink" title="7.hdfs安装"></a>7.hdfs安装</h4><h5 id="7-1解压"><a href="#7-1解压" class="headerlink" title="7.1解压"></a>7.1解压</h5><p>[hadoop@warehouse001 ~]$ cd software/<br>[hadoop@warehouse001 software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</p><h5 id="7-2软连接"><a href="#7-2软连接" class="headerlink" title="7.2软连接"></a>7.2软连接</h5><p>[hadoop@warehouse001 ~]$ cd app/<br>[hadoop@warehouse001 app]$ ll<br>total 4<br>drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2018 hadoop-2.6.0-cdh5.16.2<br>[hadoop@warehouse001 app]$ ln -s hadoop-2.6.0-cdh5.16.2  hadoop<br>[hadoop@warehouse001 app]$ ll<br>total 4<br>lrwxrwxrwx  1 hadoop hadoop   22 Nov 22 09:44 hadoop -&gt; hadoop-2.6.0-cdh5.16.2<br>drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2018 hadoop-2.6.0-cdh5.16.2<br>【补充】软连接：<br>    a.版本切换，脚本应用是配置的hadoop，是无感知的 </p><p>b.小盘换大盘<br>    /       系统盘  100G  /ruozedata 80G<br>    /data01 数据盘  2T</p><p>​    ll /             看一下ruozedata文件夹权限<br>​    ll /ruozedata    看一下ruozedata文件夹内容的权限<br>​    mv      /ruozedata /data01/ruozedata<br>​    ln -s   /data01/ruozedata  /ruozedata</p><p>注意的是：<br>权限 rwx  用户用户组</p><h5 id="7-3-解读目录只关注-bin-sbin-etc"><a href="#7-3-解读目录只关注-bin-sbin-etc" class="headerlink" title="7.3 解读目录只关注 bin  sbin  etc"></a>7.3 解读目录只关注 bin  sbin  etc</h5><p>[hadoop@warehouse001 app]$ cd hadoop<br>[hadoop@warehouse001 hadoop]$ ll<br>total 152<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 bin        可执行命令<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 bin-mapreduce1<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 cloudera<br>drwxr-xr-x  6 hadoop hadoop  4096 Jun  3  2018 etc        配置文件<br>drwxr-xr-x  5 hadoop hadoop  4096 Jun  3  2018 examples<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 examples-mapreduce1<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 include<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 lib<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 libexec<br>-rw-r–r–  1 hadoop hadoop 85063 Jun  3  2018 LICENSE.txt<br>-rw-r–r–  1 hadoop hadoop 14978 Jun  3  2018 NOTICE.txt<br>-rw-r–r–  1 hadoop hadoop  1366 Jun  3  2018 README.txt<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 sbin        启动停止脚本<br>drwxr-xr-x  4 hadoop hadoop  4096 Jun  3  2018 share<br>drwxr-xr-x 18 hadoop hadoop  4096 Jun  3  2018 src</p><h5 id="7-4部署模式"><a href="#7-4部署模式" class="headerlink" title="7.4部署模式"></a>7.4部署模式</h5><p>Local (Standalone) Mode  本地      1台机器  1个单独的java进程 用于debug<br>Pseudo-Distributed Mode  伪分布式  1台机器  多个java进程<br>Fully-Distributed Mode   集群      多台机器 多个java进程 </p><h5 id="7-5修改hadoop-env-sh文件，显性java家目录"><a href="#7-5修改hadoop-env-sh文件，显性java家目录" class="headerlink" title="7.5修改hadoop-env.sh文件，显性java家目录"></a>7.5修改hadoop-env.sh文件，显性java家目录</h5><p>export JAVA_HOME=/usr/java/jdk1.8.0_181</p><h5 id="7-6配置hadoop用户的ssh信任关系"><a href="#7-6配置hadoop用户的ssh信任关系" class="headerlink" title="7.6配置hadoop用户的ssh信任关系"></a>7.6配置hadoop用户的ssh信任关系</h5><p>删除已存在的.ssh，建议生产上mv移走重命名</p><p>[hadoop@warehouse001 ~]$ ssh-keygen<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):<br>Created directory ‘/home/hadoop/.ssh’.<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/hadoop/.ssh/id_rsa.<br>Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>SHA256:dBcBU5BHmP5ZqYc1cy8Bsd1bcHJbgQJ18aESVXW2bUs hadoop@warehouse001<br>The key’s randomart image is:<br>+—[RSA 2048]—-+<br>|         .*@B**+X|<br>|          +o==oBB|<br>|        …o+o.E=|<br>|       . …. O.=|<br>|        S  . * B.|<br>|            = o .|<br>|             . . |<br>|                 |<br>|                 |<br>+—-[SHA256]—–+<br>[hadoop@warehouse001 ~]$<br>[hadoop@warehouse001 ~]$ cd .ssh<br>[hadoop@warehouse001 .ssh]$ ll<br>total 8<br>-rw——- 1 hadoop hadoop 1679 Nov 22 10:29 id_rsa<br>-rw-r–r– 1 hadoop hadoop  401 Nov 22 10:29 id_rsa.pub<br>[hadoop@warehouse001 .ssh]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[hadoop@warehouse001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys</p><p>验证ssh,第一次必须输入yes建立关系,无需输入密码即可打印日期</p><p>[hadoop@warehouse001 .ssh]$ ssh hadoop@warehouse001 date<br>The authenticity of host ‘warehouse001 (172.23.75.57)’ can’t be established.<br>ECDSA key fingerprint is SHA256:XliTXyWZCOu2gk6FPXdhy4QOtYyTzmmKx6UdMuQ1LpY.<br>ECDSA key fingerprint is MD5:3e:e7:27:d3:95:28:f2:dd:b5:71:df:93:c9:83:b6:f3.<br>Are you sure you want to continue connecting (yes/no)? yes<br>Warning: Permanently added ‘warehouse001,172.23.75.57’ (ECDSA) to the list of known hosts.<br>Sun Nov 22 10:31:25 CST 2018</p><p>第二次直接打印</p><p>[hadoop@warehouse001 .ssh]$ ssh hadoop@warehouse001 date<br>Sun Nov 22 10:33:23 CST 2018<br>[hadoop@warehouse001 .ssh]$ </p><h5 id="7-7-配置namenode进程以warehouse001启动"><a href="#7-7-配置namenode进程以warehouse001启动" class="headerlink" title="7.7 配置namenode进程以warehouse001启动"></a>7.7 配置namenode进程以warehouse001启动</h5><p>修改core-site.xml文件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://warehouse001:9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h5 id="7-8-配置secondary-namenode进程以warehouse001启动"><a href="#7-8-配置secondary-namenode进程以warehouse001启动" class="headerlink" title="7.8 配置secondary namenode进程以warehouse001启动"></a>7.8 配置secondary namenode进程以warehouse001启动</h5><p>修改hdfs-site.xml文件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>warehouse001:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.https-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>warehouse001:50091<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h5 id="7-9配置datanode进程以warehouse001启动"><a href="#7-9配置datanode进程以warehouse001启动" class="headerlink" title="7.9配置datanode进程以warehouse001启动"></a>7.9配置datanode进程以warehouse001启动</h5><p>[hadoop@warehouse001 hadoop]$ vi slaves                </p><p>warehouse001</p><h5 id="7-10格式化"><a href="#7-10格式化" class="headerlink" title="7.10格式化"></a>7.10格式化</h5><p>[hadoop@warehouse001 hadoop]$ sbin/start-dfs.sh<br>20/11/22 11:17:04 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0<br>20/11/22 11:17:04 INFO util.ExitUtil: Exiting with status 0<br>20/11/22 11:17:04 INFO namenode.NameNode: SHUTDOWN_MSG:<br>/<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>****</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong><br>SHUTDOWN_MSG: Shutting down NameNode at warehouse001/172.23.75.57<br>**<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>**</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>/</p><h5 id="7-11启动"><a href="#7-11启动" class="headerlink" title="7.11启动"></a>7.11启动</h5><p>[hadoop@warehouse001 sbin]$ sh start-dfs.sh<br>which: no start-dfs.sh in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)<br>20/11/22 11:29:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Starting namenodes on [warehouse001]<br>warehouse001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-namenode-warehouse001.out<br>localhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-datanode-warehouse001.out<br>Starting secondary namenodes [warehouse001]<br>warehouse001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-secondarynamenode-warehouse001.out<br>20/11/22 11:29:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</p><p>[hadoop@warehouse001 hadoop]$ jps<br>13377 Jps<br>13090 NameNode<br>12098 SecondaryNameNode<br>11944 DataNode</p><p>hdfs存储来说:<br>namenode             名称节点       老大<br>secondary namenode   第二名称节点   老二  每隔1小时 把老大备份一下</p><p>datanode             数据节点       小弟</p><p>最终三个进程以warehouse001 机器名称启动 </p><p>[hadoop@warehouse001 sbin]$ sh start-dfs.sh<br>which: no start-dfs.sh in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)<br>20/11/22 11:54:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Starting namenodes on [warehouse001]<br>warehouse001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-namenode-warehouse001.out<br>warehouse001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-datanode-warehouse001.out<br>Starting secondary namenodes [warehouse001]<br>warehouse001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-secondarynamenode-warehouse001.out<br>20/11/22 11:54:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</p><p>打开web，云主机需要开启【安全组】</p><p><a href="http://warehouse001:50070/dfshealth.html#tab-overview">http://warehouse001:50070/dfshealth.html#tab-overview</a></p><p>Make the HDFS directories required to execute MapReduce jobs:</p><pre><code>  $ bin/hdfs dfs -mkdir /user  $ bin/hdfs dfs -mkdir /user/&lt;username&gt;</code></pre><p>8.案例<br>[hadoop@warehouse001 hadoop]$bin/hadoop jar <br>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar <br>grep input output ‘dfs[a-z.]+’</p><p>执行成功后</p><p>[hadoop@warehouse001 hadoop]$ bin/hdfs dfs -get output output</p><p>[hadoop@warehouse001 hadoop]$ cd output/<br>[hadoop@warehouse001 output]$ ll<br>total 4<br>-rw-r–r– 1 hadoop hadoop 301 Nov 22 14:44 part-r-00000<br>-rw-r–r– 1 hadoop hadoop   0 Nov 22 14:44 _SUCCESS<br>[hadoop@warehouse001 output]$ cat part-r-00000<br>6       dfs.audit.logger<br>4       dfs.class<br>3       dfs.logger<br>3       dfs.server.namenode.<br>2       dfs.audit.log.maxfilesize<br>2       dfs.audit.log.maxbackupindex<br>2       dfs.period<br>1       dfsmetrics.log<br>1       dfsadmin<br>1       dfs.servers<br>1       dfs.replication<br>1       dfs.namenode.secondary.https<br>1       dfs.namenode.secondary.http<br>1       dfs.log<br>1       dfs.file<br>1       dfs.namenode.http</p><p>但是namenode、datanode、checkpoint(secondarynamenode)官方默认配置如下:<br>dfs.namenode.name.dir –&gt; file://${hadoop.tmp.dir}/dfs/name<br>dfs.datanode.data.dir –&gt; file://${hadoop.tmp.dir}/dfs/data<br>dfs.namenode.checkpoint.dir –&gt; file://${hadoop.tmp.dir}/dfs/namesecondary</p><p>所以配置hadoop.tmp.dir临时目录改为/home/hadoop/tmp，<br>那么namenode、datanode、checkpoint(secondarynamenode)<br>存储也对应变更。</p><p>1.sql行转列、列转行</p><p> 我们首先先通过一个学生成绩表(下面简化了些)来形象了解下行转列</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span>  <span class="token keyword">TABLE</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token punctuation">(</span>UserName <span class="token keyword">VARCHAR</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     Subject <span class="token keyword">VARCHAR</span><span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Score <span class="token keyword">int</span><span class="token punctuation">)</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">;</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'数学'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'生物'</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'数学'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">;</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'生物'</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">;</span><span class="token keyword">select</span>  <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token keyword">select</span> UserName<span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'语文'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'语文'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'数学'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'数学'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'英语'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'英语'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'生物'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'生物'</span><span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token keyword">group</span> <span class="token keyword">by</span> UserName</code></pre><p><img src="https://i.loli.net/2020/11/22/b3HoLOiN8YnzwFq.png"></p><p>列转行:</p><p><img src="https://i.loli.net/2020/11/23/UCgtDy2PNVxOz1e.png"></p><p>实现:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'语文'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>语文 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student    <span class="token keyword">union</span> <span class="token keyword">all</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'数学'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>数学 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student    <span class="token keyword">union</span> <span class="token keyword">all</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'物理'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>物理 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student<span class="token punctuation">)</span> <span class="token number">a</span> </code></pre><p><img src="https://i.loli.net/2020/11/23/Oy9qUF2gAQt3ez4.png"></p><p>理解：把语文列装换为分数列，本身列头没有了，使用字符来填充列头，代表本身列具备的含义。</p><p>单个示例:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>  <span class="token string">'语文'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span> 语文 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student</code></pre><p><img src="https://i.loli.net/2020/11/23/qAjzQyilEUvGY2h.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql的join及练习</title>
      <link href="2018/11/06/2020-11-18-quannnxu-mysql2/"/>
      <url>2018/11/06/2020-11-18-quannnxu-mysql2/</url>
      
        <content type="html"><![CDATA[<h5 id="left-join"><a href="#left-join" class="headerlink" title="left join"></a>left join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/TB9nkIH7EWUMRGh.png"></p><h5 id="right-join"><a href="#right-join" class="headerlink" title="right join"></a>right join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/TB9nkIH7EWUMRGh.png"></p><h5 id="inner-join"><a href="#inner-join" class="headerlink" title="inner join"></a>inner join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">inner</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/vGtbnOWNh81pYDM.png"></p><h5 id="full-join"><a href="#full-join" class="headerlink" title="full join"></a>full join</h5><p>a表+b表全部</p><p>mysql中没有这个语法,spark,hive支持</p><p>总结:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span><span class="token comment" spellcheck="true">--字段</span><span class="token keyword">from</span><span class="token comment" spellcheck="true">--表</span><span class="token keyword">where</span> xxx<span class="token keyword">group</span> <span class="token keyword">by</span> xxxx <span class="token keyword">having</span> xxxx<span class="token keyword">order</span> <span class="token keyword">by</span> xxxx<span class="token keyword">limit</span> <span class="token number">1</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">--注意使用顺序</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--创建视图</span><span class="token keyword">create</span> <span class="token keyword">view</span> mydatabase<span class="token punctuation">.</span>tv  <span class="token keyword">as</span> <span class="token keyword">select</span>  <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token keyword">as</span> vid<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">as</span> vname<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">as</span> bid<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name <span class="token keyword">as</span> bname<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token comment" spellcheck="true">--这个视图是不存数据的,查询时去执行视图sql</span><span class="token comment" spellcheck="true">--物化视图:是存数据的</span></code></pre><p>作业:</p><p>1.mysql存储引擎myisam和innodb的区别</p><p>2.between A and B</p><p>3.物化视图是什么</p><p>4.sql题</p><hr><p>数据仓库</p><p>离线数仓:</p><p>​        数据源同步    oracle/mysql–&gt;sqoop/datax/kettle–&gt;hive</p><p>​        etl加工            hivesql、spark sql、spark code</p><p>实时数仓:</p><p>​        数据源同步    oracle–&gt;ogg        –&gt;kafaka</p><p>​                                                                                        –&gt;spark streaming/flink–&gt;hbase/kudu/elasticserch</p><p>​                                mysql–&gt;maxwell–&gt;kafaka</p><p>​        ETL加工                                                                  –&gt;spark streaming/flink–&gt;hbase(java)–&gt;oracle/mysql    ETL加工</p><p>数据平台</p><p>数据中台</p><p>数据湖</p><p>单点—————–&gt;分布式存储</p><p>​        数据迁移</p><p>数仓的根本:数据质量,保证数据不丢,哪怕丢了能够补齐,重跑计算</p><p>检测:</p><p>​        比对源端和目标端数据量    100%</p><p>​        抽查源端和目标端数据内容    5%    概率性</p><p>​        明细数据汇总和科目表(汇总表)对比</p><p>业务数据是明细</p><p>1    100</p><p>2    100</p><p>3    50</p><p>科目汇总</p><p>250</p><hr><p>​                源表                                                                                目标表</p><p>​        1    xiaoxing                                                                     1    xiaoxing<br>​        5    xiaoming                                                                    5    xiaoming<br>​        6    lisi                                                                                6    lisi                                                                                        –成功</p><pre class=" language-sql"><code class="language-sql">    <span class="token number">5</span>    xiaoming                            <span class="token boolean">null</span>    <span class="token boolean">null</span>                                                                            <span class="token number">6</span>    lisi                                <span class="token boolean">null</span>    <span class="token boolean">null</span>                                <span class="token comment" spellcheck="true">--源端有 目标没有                            </span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">left</span> <span class="token keyword">join</span> ruozedata<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">is</span> <span class="token boolean">null</span>            <span class="token comment" spellcheck="true">--需要补齐数据</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token boolean">null</span>    <span class="token boolean">null</span>                            <span class="token number">5</span>    xiaoming                                                            <span class="token boolean">null</span>    <span class="token boolean">null</span>                            <span class="token number">6</span>    lisi    <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> ruozedata<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">is</span> <span class="token boolean">null</span>            <span class="token comment" spellcheck="true">--需要剔除</span></code></pre><p>练习</p><p>use mydatabasedb;</p><p>–部门表<br>dept部门表(deptno部门编号/dname部门名称/loc地点)<br>create table dept (<br>    deptno numeric(2),<br>    dname varchar(14),<br>    loc varchar(13)<br>);</p><p>insert into dept values (10, ‘ACCOUNTING’, ‘NEW YORK’);<br>insert into dept values (20, ‘RESEARCH’, ‘DALLAS’);<br>insert into dept values (30, ‘SALES’, ‘CHICAGO’);<br>insert into dept values (40, ‘OPERATIONS’, ‘BOSTON’);</p><p>–工资等级表<br>salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)<br>create table salgrade (<br>    grade numeric,<br>    losal numeric,<br>    hisal numeric<br>);</p><p>insert into salgrade values (1, 700, 1200);<br>insert into salgrade values (2, 1201, 1400);<br>insert into salgrade values (3, 1401, 2000);<br>insert into salgrade values (4, 2001, 3000);<br>insert into salgrade values (5, 3001, 9999);</p><p>–员工表<br>emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)<br>工资 ＝ 薪金 ＋ 佣金</p><p>create table emp (<br>    empno numeric(4) not null,<br>    ename varchar(10),<br>    job varchar(9),<br>    mgr numeric(4),<br>    hiredate datetime,<br>    sal numeric(7, 2),<br>    comm numeric(7, 2),<br>    deptno numeric(2)<br>);</p><p>insert into emp values (7369, ‘SMITH’, ‘CLERK’, 7902, ‘1980-12-17’, 800, null, 20);<br>insert into emp values (7499, ‘ALLEN’, ‘SALESMAN’, 7698, ‘1981-02-20’, 1600, 300, 30);<br>insert into emp values (7521, ‘WARD’, ‘SALESMAN’, 7698, ‘1981-02-22’, 1250, 500, 30);<br>insert into emp values (7566, ‘JONES’, ‘MANAGER’, 7839, ‘1981-04-02’, 2975, null, 20);<br>insert into emp values (7654, ‘MARTIN’, ‘SALESMAN’, 7698, ‘1981-09-28’, 1250, 1400, 30);<br>insert into emp values (7698, ‘BLAKE’, ‘MANAGER’, 7839, ‘1981-05-01’, 2850, null, 30);<br>insert into emp values (7782, ‘CLARK’, ‘MANAGER’, 7839, ‘1981-06-09’, 2450, null, 10);<br>insert into emp values (7788, ‘SCOTT’, ‘ANALYST’, 7566, ‘1982-12-09’, 3000, null, 20);<br>insert into emp values (7839, ‘KING’, ‘PRESIDENT’, null, ‘1981-11-17’, 5000, null, 10);<br>insert into emp values (7844, ‘TURNER’, ‘SALESMAN’, 7698, ‘1981-09-08’, 1500, 0, 30);<br>insert into emp values (7876, ‘ADAMS’, ‘CLERK’, 7788, ‘1983-01-12’, 1100, null, 20);<br>insert into emp values (7900, ‘JAMES’, ‘CLERK’, 7698, ‘1981-12-03’, 950, null, 30);<br>insert into emp values (7902, ‘FORD’, ‘ANALYST’, 7566, ‘1981-12-03’, 3000, null, 20);<br>insert into emp values (7934, ‘MILLER’, ‘CLERK’, 7782, ‘1982-01-23’, 1300, null, 10);</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--1. 查询出部门编号为30的所有员工的编号和姓名</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token keyword">where</span> deptno<span class="token operator">=</span><span class="token string">'30'</span><span class="token comment" spellcheck="true">--2.找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token number">10</span> <span class="token operator">and</span> job<span class="token operator">=</span><span class="token string">'MANAGER'</span><span class="token keyword">union</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token number">20</span> <span class="token operator">and</span> job<span class="token operator">=</span><span class="token string">'SALESMAN'</span><span class="token comment" spellcheck="true">--3.查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</span><span class="token keyword">select</span>  <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> salary<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">order</span> <span class="token keyword">by</span> salary <span class="token keyword">desc</span><span class="token punctuation">,</span>hiredate <span class="token comment" spellcheck="true">--4.列出薪金大于1500的各种工作及从事此工作的员工人数。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>job<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> sal<span class="token operator">></span><span class="token number">1500</span><span class="token keyword">group</span> <span class="token keyword">by</span> job<span class="token comment" spellcheck="true">--5.列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno<span class="token keyword">where</span> <span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token operator">=</span><span class="token string">'SALES'</span><span class="token comment" spellcheck="true">--6.查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L  __</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'S%'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'%S'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'%S%'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'_L%'</span><span class="token comment" spellcheck="true">--7.查询每种工作的最高工资、最低工资、人数</span><span class="token keyword">select</span> <span class="token function">max</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> maxsalary<span class="token punctuation">,</span><span class="token function">min</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> minsalary<span class="token punctuation">,</span>job<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> jobcount<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">group</span> <span class="token keyword">by</span> job<span class="token comment" spellcheck="true">--8.列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token keyword">case</span> <span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">700</span> <span class="token operator">and</span> <span class="token number">1200</span> <span class="token keyword">then</span> <span class="token number">1</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">1201</span> <span class="token operator">and</span> <span class="token number">1400</span> <span class="token keyword">then</span> <span class="token number">2</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">1404</span> <span class="token operator">and</span> <span class="token number">2000</span> <span class="token keyword">then</span> <span class="token number">3</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">2001</span> <span class="token operator">and</span> <span class="token number">3000</span> <span class="token keyword">then</span> <span class="token number">3</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">3001</span> <span class="token operator">and</span> <span class="token number">9999</span> <span class="token keyword">then</span> <span class="token number">4</span> <span class="token keyword">end</span> <span class="token keyword">as</span> grade<span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>empno <span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname<span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>mgr<span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> salary<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">></span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">avg</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token punctuation">)</span> avgsal<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token comment" spellcheck="true">--9.列出薪金  高于  在部门30工作的  所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token operator">></span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">max</span><span class="token punctuation">(</span>sal<span class="token punctuation">)</span><span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp t<span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token string">'30'</span><span class="token punctuation">)</span></code></pre><p>tips:mysql分组topN</p><p>#哪些部门的哪些职业的薪水和,最高1位的职业是什么?<br>#1.每个部门的每个职业的薪水和</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">view</span> sal<span class="token keyword">as</span> <span class="token keyword">select</span>deptno<span class="token punctuation">,</span>job<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>sal<span class="token operator">+</span>ifnull<span class="token punctuation">(</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> salary<span class="token keyword">from</span> emp <span class="token keyword">group</span> <span class="token keyword">by</span> deptno<span class="token punctuation">,</span>job<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> sal<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token keyword">from</span> sal <span class="token number">a</span> <span class="token keyword">where</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> sal <span class="token number">b</span><span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>deptno<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>deptno <span class="token operator">and</span> <span class="token number">a</span><span class="token punctuation">.</span>salary<span class="token operator">&lt;</span><span class="token number">b</span><span class="token punctuation">.</span>salary        <span class="token comment" spellcheck="true">--自己关联自己,找到比自己大个数为0的,也就是最大的</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">0</span><span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">.</span>deptno<span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql语法</title>
      <link href="2018/11/05/2020-11-16-quanxu-shu-cang-zhi-mysql-yu-fa/"/>
      <url>2018/11/05/2020-11-16-quanxu-shu-cang-zhi-mysql-yu-fa/</url>
      
        <content type="html"><![CDATA[<h4 id="1-字段类型"><a href="#1-字段类型" class="headerlink" title="1.字段类型"></a>1.字段类型</h4><h5 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型:"></a>数值类型:</h5><p>​        int    整数</p><p>​        long    长整型</p><p>​        float    单精度</p><p>​        double    双精度</p><p>​        decimal    钱</p><h5 id="字符串"><a href="#字符串" class="headerlink" title="字符串:"></a>字符串:</h5><p>​        char 字节    定长0-255长度    aaaaxxxxxx    自动补齐</p><p>​        varchar    字符串    变长0-65535字节    aaaa</p><h5 id="日期"><a href="#日期" class="headerlink" title="日期"></a>日期</h5><p>​        date    日期    YYYY-MM-DD</p><p>​        time    时间    HH:MM:SS</p><p>​        datetime    年月日时分秒</p><p>​        timestamp        年月日时分秒</p><h5 id="datetime和timestamp的区别"><a href="#datetime和timestamp的区别" class="headerlink" title="datetime和timestamp的区别:"></a>datetime和timestamp的区别:</h5><h6 id="datetime类型使用8个字节来表示日期和时间。"><a href="#datetime类型使用8个字节来表示日期和时间。" class="headerlink" title="datetime类型使用8个字节来表示日期和时间。"></a>datetime类型使用8个字节来表示日期和时间。</h6><p>支持的常见插入格式为：</p><ol><li>（推荐甚至强制要求必须）‘YYYY-MM-DD HH:MM:SS’或‘YYYYMMDDHHMMSS’格式的字符串表示。这种方式可以表达的范围是‘1000-01-01 00:00:00’~~‘9999-12-31 23:59:59’。</li><li>MySQL中还支持一些不严格的语法格式，任何的标点都可以用来做间隔符。情况与date类型相同，而且时间部分也可以使用任意的分隔符隔开，这与Time类型不同，Time类型只能用‘:’隔开呢。</li><li>使用now()来输入当前系统日期和时间。</li></ol><h6 id="timestamp类型使用4个字节来表示日期和时间。"><a href="#timestamp类型使用4个字节来表示日期和时间。" class="headerlink" title="timestamp类型使用4个字节来表示日期和时间。"></a>timestamp类型使用4个字节来表示日期和时间。</h6><p>支持的常见插入格式为：</p><p>二者主要区别在于取值范围。</p><ol><li><p>timestamp存储需要四个字节，它的取值范围为“1970-01-01 00:00:01” UTC ~ “2038-01-19 03:14:07” （和时区有关）</p></li><li><p>而datetime取值范围为“1000-01-01 00:00:00” ~ “9999-12-31 23:59:59”（和时区无关，怎么存入怎么返回，对程序员友好）</p><p>3、timestamp类型还有一个很大的特殊点，就是时间是根据时区来显示的。<br>例如，在东八区插入的timestamp类型为2009-09-30 14:21:25，在东七区显示时，时间部门就变成了13:21:25，在东九区显示时，时间部门就变成了15:21:25。<br>4、需要显示日期与时间，timestamp类型需要根据不同地区的时区来转换时间，但是，timestamp类型的范围太小，其最大时间为2038-01-19 11:14:07。<br>如果插入时间的比这个大，将会数据库插入0000-00-00 00:00:00。所以需要的时间范围比较大，还是选择dateTime类型比较安全。</p></li></ol><h4 id="2-sql类型"><a href="#2-sql类型" class="headerlink" title="2.sql类型"></a>2.sql类型</h4><p>ddl    数据定义语言    create drop</p><p>dml    数据操作语言    select    insert    update    delete    (增删改查)</p><p>dcl    数据控制语言    grant</p><h4 id="3-建表规范"><a href="#3-建表规范" class="headerlink" title="3.建表规范"></a>3.建表规范</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">auto_increment</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--第一列必须为自增长字段 主键，且无业务意义  架构设计默认规则</span>name <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--业务字段</span>age <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment" spellcheck="true">--业务字段</span>create_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建者</span>create_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建时间</span>update_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新者</span>update_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新时间</span><span class="token keyword">primary</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>id<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">--一张表只有一个主键 id，业务字段需要唯一的话，就使用唯一约束来保证</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>规范:<br>1.表名称  字段名称 不要写中文  尽量不要汉语拼音<br>2.统一风格:<br>已经存在表结构设计，风格比如是create_user;不同表的同一字段名称尽量统一,以防后面开发字段混淆.</p><h4 id="4-增删改查"><a href="#4-增删改查" class="headerlink" title="4.增删改查"></a>4.增删改查</h4><p>插入数据</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">(</span>name<span class="token punctuation">,</span>age<span class="token punctuation">)</span> <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'quanxu'</span><span class="token punctuation">,</span><span class="token number">22</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>查询数据:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">;</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token keyword">update</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">set</span> age<span class="token operator">=</span><span class="token string">'25'</span> <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'quanxu'</span><span class="token punctuation">;</span></code></pre><p>删除数据:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">delete</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> id<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span></code></pre><p>创建唯一约束:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">ADD</span> <span class="token keyword">CONSTRAINT</span> employee_un <span class="token keyword">UNIQUE</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>SQL 错误 [1062] [23000]: Duplicate entry ‘quanxu’ for key ‘employee_un’</p><p>违反唯一约束,这时就要去检查是否重复</p><h4 id="5-后面四个字段一定要加上"><a href="#5-后面四个字段一定要加上" class="headerlink" title="5.后面四个字段一定要加上"></a>5.后面四个字段一定要加上</h4><pre class=" language-sql"><code class="language-sql">create_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建者</span>create_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建时间</span>update_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新者</span>update_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新时间</span></code></pre><p>离线数仓抽数的基础</p><h4 id="6-字段注释一定要加上"><a href="#6-字段注释一定要加上" class="headerlink" title="6.字段注释一定要加上"></a>6.字段注释一定要加上</h4><p>comment ‘xxxx’</p><pre class=" language-sql"><code class="language-sql">name <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span> <span class="token keyword">comment</span> <span class="token string">'名字'</span><span class="token punctuation">,</span></code></pre><h4 id="7-其他语法"><a href="#7-其他语法" class="headerlink" title="7.其他语法"></a>7.其他语法</h4><ul><li><p>1.where(筛选)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'zhangsan'</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'zhangsan'</span> <span class="token operator">and</span> age<span class="token operator">=</span><span class="token number">22</span><span class="token punctuation">;</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age<span class="token operator">=</span><span class="token number">12</span> <span class="token operator">or</span> age<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age<span class="token operator">></span><span class="token number">12</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age <span class="token operator">in</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre></li><li><p>2.order by (排序)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--默认正序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">asc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--正序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">desc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--倒序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">desc</span><span class="token punctuation">,</span>name <span class="token keyword">desc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--多个排序</span></code></pre></li><li><p>3.like(模糊匹配)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'l%'</span>    <span class="token comment" spellcheck="true">--以l开头</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'%o'</span>    <span class="token comment" spellcheck="true">--以o结尾</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'%i%'</span>    <span class="token comment" spellcheck="true">--含有i</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> '<span class="token comment" spellcheck="true">--p%'    --第三个位为p,_表示占位符</span></code></pre></li><li><p>4.union(合并)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">union</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span>    <span class="token comment" spellcheck="true">--合并后去重</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span>    <span class="token comment" spellcheck="true">--合并不去重</span></code></pre></li><li><p>5.分组语法</p><pre class=" language-sql"><code class="language-sql">sum    <span class="token comment" spellcheck="true">--求和</span>avg    <span class="token comment" spellcheck="true">--平均数</span>max    <span class="token comment" spellcheck="true">--最大值</span>min    <span class="token comment" spellcheck="true">--最小值</span>count    <span class="token comment" spellcheck="true">--计数</span><span class="token keyword">select</span> <span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">avg</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">min</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">;</span><span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token punctuation">;</span><span class="token keyword">select</span>name<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span> sum_age<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token keyword">having</span> sum_age <span class="token operator">></span><span class="token number">30</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--having相当于过滤</span><span class="token comment" spellcheck="true">--    等同于==></span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span>name<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span> sum_age<span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--子查询</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token keyword">where</span> sum_age <span class="token operator">></span><span class="token number">30</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">--能使用having尽量使用having,不能使用时再使用子查询</span></code></pre></li></ul><p>tips:</p><h5 id="spark中exists和in有什么区别"><a href="#spark中exists和in有什么区别" class="headerlink" title="spark中exists和in有什么区别?"></a>spark中exists和in有什么区别?</h5><h5 id="一、exists和in的使用方式"><a href="#一、exists和in的使用方式" class="headerlink" title="一、exists和in的使用方式"></a>一、exists和in的使用方式</h5><p>​       1、exists是对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；</p><p>　　2、in是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。</p><p>　　3、如果用not in ，则是内外表都全表扫描，无索引，效率低，可考虑使用not exists，也可使用A left join B on A.id=B.id where B.id is null 进行优化。</p><h5 id="二、EXISTS与IN-的用法异同"><a href="#二、EXISTS与IN-的用法异同" class="headerlink" title="二、EXISTS与IN 的用法异同"></a>二、EXISTS与IN 的用法异同</h5><p>1、IN只能返回一个字段值，但EXITS允许返回多个字段。<br>2、exists是对外表做loop循环，每次loop循环再对内表做查询；IN是将外表和内表做hash连接，相当于多个or条件叠加。exists需要查询数据库，in是内存里遍历比较。</p><h5 id="2、EXISTS与IN-的效率比较"><a href="#2、EXISTS与IN-的效率比较" class="headerlink" title="2、EXISTS与IN 的效率比较"></a>2、EXISTS与IN 的效率比较</h5><p>对于表A，表B：</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--用法1</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token number">cc</span> <span class="token operator">in</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">cc</span> <span class="token keyword">from</span> <span class="token number">b</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">--用法2</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">cc</span><span class="token operator">=</span><span class="token number">a</span><span class="token punctuation">.</span><span class="token number">cc</span><span class="token punctuation">)</span></code></pre><p>1、若表A/B大小相当，那运行效率差异不大；<br>2、若表A（外表）更大，则用法1即in()效率更高；<br>3、若表B（内表）更大，则用法2即exists()效率更高。</p>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql部署</title>
      <link href="2018/11/03/2020-11-14-quanxu-shu-cang-zhi-mysql-bu-shu/"/>
      <url>2018/11/03/2020-11-14-quanxu-shu-cang-zhi-mysql-bu-shu/</url>
      
        <content type="html"><![CDATA[<h3 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h3><p><a href="https://www.bilibili.com/video/BV12b411N7Lv">https://www.bilibili.com/video/BV12b411N7Lv</a><br><a href="https://www.bilibili.com/video/BV1Tt411p7de">https://www.bilibili.com/video/BV1Tt411p7de</a></p><p><a href="https://github.com/Hackeruncle/MySQL">https://github.com/Hackeruncle/MySQL</a><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.7.11%20Install.txt">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.7.11%20Install.txt</a></p><h4 id="1-jdk安装"><a href="#1-jdk安装" class="headerlink" title="1.jdk安装"></a>1.jdk安装</h4><p>mkdir -p /usr/java</p><p>tar  -zxvf jdk-8u181-linux-x64.tar.gz -C  /usr/java</p><p>[root@warehouse001 java]# ll<br>total 350520<br>drwxr-xr-x 7   10  143      4096 Jul  7  2018 jdk1.8.0_181【注意赋所属组的权限】</p><p>chmod 775 jdk1.8.0_181</p><p>chown -R root:root jdk1.8.0_181</p><p>vi /etc/profile</p><p>追加:</p><p>export JAVA_HOME=/usr/java/jdk1.8.0_181<br>export PATH=$JAVA_HOME/bin:$PATH</p><h4 id="2-mysql的安装"><a href="#2-mysql的安装" class="headerlink" title="2.mysql的安装"></a>2.mysql的安装</h4><p>要注意文档中的命令是哪个用户,哪个目录执行</p><p>tar -zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz -C /usr/local/</p><p>mv mysql-5.7.11-linux-glibc2.5-x86_64 mysql</p><h5 id="创建my-cnf-见文件"><a href="#创建my-cnf-见文件" class="headerlink" title="创建my.cnf(见文件)"></a>创建my.cnf(见文件)</h5><p>vi /etc/my.cnf</p><pre class=" language-shell"><code class="language-shell">[client]port            = 3306socket          = /usr/local/mysql/data/mysql.sockdefault-character-set=utf8mb4[mysqld]port            = 3306socket          = /usr/local/mysql/data/mysql.sockskip-slave-startskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU's*2 for thread_concurrency#thread_concurrency = 32 #isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id  = 1739basedir     = /usr/local/mysqldatadir     = /usr/local/mysql/datapid-file     = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = ROWlog_bin_trust_function_creators=1log-error  = /usr/local/mysql/data/hostname.errlog-bin = /usr/local/mysql/arch/mysql-binexpire_logs_days = 7innodb_write_io_threads=16relay-log  = /usr/local/mysql/relay_log/relay-logrelay-log-index = /usr/local/mysql/relay_log/relay-log.indexrelay_log_info_file= /usr/local/mysql/relay_log/relay-log.infolog_slave_updates=1gtid_mode=OFFenforce_gtid_consistency=OFF# slaveslave-parallel-type=LOGICAL_CLOCKslave-parallel-workers=4master_info_repository=TABLErelay_log_info_repository=TABLErelay_log_recovery=ON#other logs#general_log =1#general_log_file  = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slavesync_binlog = 500#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:1G;ibdata2:1G:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 4innodb_log_file_size = 1Ginnodb_log_buffer_size = 200M#根据生产需要，调整pool size innodb_buffer_pool_size = 2G#innodb_additional_mem_pool_size = 50M #deprecated in 5.6tmpdir = /usr/local/mysql/tmpinnodb_lock_wait_timeout = 1000#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 2innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 128M[mysql]no-auto-rehashdefault-character-set=utf8mb4[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M</code></pre><p>#根据生产需要，调整pool size </p><p>innodb_buffer_pool_size = 2G</p><p>#innodb_additional_mem_pool_size = 50M #deprecated in 5.6</p><p>tmpdir = /usr/local/mysql/tmp</p><h4 id="3-创建用户组及用户"><a href="#3-创建用户组及用户" class="headerlink" title="3.创建用户组及用户"></a>3.创建用户组及用户</h4><p>groupadd -g 101 dba</p><p>useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</p><p>id mysqladmin</p><p>–uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)</p><p>4.copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量</p><p>cp /etc/skel/.* /usr/local/mysql  ###important</p><h4 id="5-配置环境变量"><a href="#5-配置环境变量" class="headerlink" title="5.配置环境变量"></a>5.配置环境变量</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# vi mysql/.bash_profile# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then        . ~/.bashrcfi# User specific environment and startup programsexport MYSQL_BASE=/usr/local/mysqlexport PATH=$&#123;MYSQL_BASE&#125;/bin:$PATHunset USERNAME#stty erase ^Hset umask to 022umask 022PS1=`uname -n`":"'$USER'":"'$PWD'":>"; export PS1</code></pre><h4 id="6-赋权限和用户组，切换用户mysqladmin，安装"><a href="#6-赋权限和用户组，切换用户mysqladmin，安装" class="headerlink" title="6.赋权限和用户组，切换用户mysqladmin，安装"></a>6.赋权限和用户组，切换用户mysqladmin，安装</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# chown  mysqladmin:dba /etc/my.cnf [root@hadoop39 local]# chmod  640 /etc/my.cnf  [root@hadoop39 local]# chown -R mysqladmin:dba /usr/local/mysql[root@hadoop39 local]# chmod -R 755 /usr/local/mysql </code></pre><h4 id="7-配置服务及开机自启动"><a href="#7-配置服务及开机自启动" class="headerlink" title="7.配置服务及开机自启动"></a>7.配置服务及开机自启动</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# cd /usr/local/mysql#将服务文件拷贝到init.d下，并重命名为mysql[root@hadoop39 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限[root@hadoop39 mysql]# chmod +x /etc/rc.d/init.d/mysql#删除服务[root@hadoop39 mysql]# chkconfig --del mysql#添加服务[root@hadoop39 mysql]# chkconfig --add mysql[root@hadoop39 mysql]# chkconfig --level 345 mysql on</code></pre><p>这步并不能真正实现开机自启动</p><p>这步需要测试</p><pre class=" language-shell"><code class="language-shell">[root@sht-sgmhadoopnn-01 mysql]# vi /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don't# want to do the full Sys V style init stuff.touch /var/lock/subsys/localsu - mysqladmin -c "/etc/init.d/mysql start --federated"</code></pre><h4 id="8-安装libaio及安装mysql的初始db"><a href="#8-安装libaio及安装mysql的初始db" class="headerlink" title="8.安装libaio及安装mysql的初始db"></a>8.安装libaio及安装mysql的初始db</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 mysql]# yum -y install libaio[root@hadoop39 mysql]# sudo su - mysqladminhadoop39.ruoze:mysqladmin:/usr/local/mysql:> bin/mysqld \--defaults-file=/etc/my.cnf \--user=mysqladmin \--basedir=/usr/local/mysql/ \--datadir=/usr/local/mysql/data/ \--initialize</code></pre><p>在初始化时如果加上 –initial-insecure，则会创建空密码的 root@localhost 账号，否则会创建带密码的 root@localhost 账号，密码直接写在 log-error 日志文件中<br>（在5.6版本中是放在 ~/.mysql_secret 文件里，更加隐蔽，不熟悉的话可能会无所适从）</p><h4 id="9-查看临时密码"><a href="#9-查看临时密码" class="headerlink" title="9.查看临时密码"></a>9.查看临时密码</h4><p>warehouse001:mysqladmin:/usr/local/mysql/data:&gt;cat hostname.err |grep password</p><p>2020-11-15T09:59:50.686247Z 1 [Note] A temporary password is generated for root@localhost: oifbVz!YZ2.D</p><h4 id="10-启动"><a href="#10-启动" class="headerlink" title="10.启动"></a>10.启动</h4><p>/usr/local/mysql/bin/mysqld_safe –defaults-file=/etc/my.cnf &amp;</p><h4 id="11-登录及修改用户密码"><a href="#11-登录及修改用户密码" class="headerlink" title="11.登录及修改用户密码"></a>11.登录及修改用户密码</h4><pre class=" language-shell"><code class="language-shell">hadoop39.ruoze:mysqladmin:/usr/local/mysql/data:>mysql -uroot -p'kFCqrXeh2y(0'mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.11-logCopyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql> alter user root@localhost identified by 'ruozedata';Query OK, 0 rows affected (0.05 sec)mysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'ruozedata' ;Query OK, 0 rows affected, 1 warning (0.02 sec)mysql> flush privileges;Query OK, 0 rows affected (0.00 sec)mysql> exit;Bye</code></pre><h4 id="12-重启"><a href="#12-重启" class="headerlink" title="12.重启"></a>12.重启</h4><pre class=" language-shell"><code class="language-shell">hadoop39.ruoze:mysqladmin:/usr/local/mysql:> service mysql restarthadoop39.ruoze:mysqladmin:/usr/local/mysql/data:>mysql -uroot -pruozedatamysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.11-log MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql> </code></pre><h3 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3.常用命令"></a>3.常用命令</h3><p>create database ruozedata;<br>grant all privileges on ruozedata.* to quanxu@’%’ identified by ‘ruozedata’;<br>flush privileges;</p><p>【注意】：对于mysql的用户操作，比如权限相关的 ，最后一步必须执行刷新权限。<br>          %代表了 任意的客户端的IP地址 都被允许使用quanxu用户来远程访问</p><p>grant all privileges on ruozedata.* to quanxu@’16.2.3.2’ identified by ‘ruozedata’;</p><p>我已经赋予%权限，在访问的时候为什么还抛错权限  访问错误<br>grant all privileges on ruozedata.* to quanxu@’客户端机器的IP’ identified by ‘ruozedata’;</p><p>grant all privileges on ruozedata.* to quanxu@’192.168.0.%’ identified by ‘ruozedata’;</p><p>139.224.129这个网段的所有ip都允许有权限去访问</p><p>mysql&gt; show databases;<br>mysql&gt; use mysql;<br>mysql&gt; show tables;</p><p>查看表结构<br>mysql&gt; desc db;<br>mysql&gt; show create table DB;<br>mysql&gt; select User,Host,db,Select_priv,Delete_priv from db;</p><p>杀会话<br>mysql&gt; show processlist;<br>mysql&gt; kill 7;</p><p>报错:DBeaver连接时connect time out</p><p>原因:云主机安全组没有加3306端口,阿里云需要配置</p><p><img src="C:\Users\10090\AppData\Roaming\Typora\typora-user-images\image-20201116175652491.png" alt="image-20201116175652491"></p>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql练习题1</title>
      <link href="2018/11/01/2020-11-27-quannnxu-zao-ke/"/>
      <url>2018/11/01/2020-11-27-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.简述mysql部署流程</p><p>2.mysql的创建DB 、创建用户设置密码权限的、刷权限，三句话会默写吗？博客写了吗？<br>create database ruozedata;<br>grant all privileges on ruozedata.* to mysql@’%’ identified by ‘ruozedata’;<br>flush privileges;</p><p>3.建表规范，谈谈你的理解<br>第一列必须为自增长字段 主键，且无业务意义<br>后面加上:<br>create_user varchar(255),    –创建者<br>create_time timestamp not null default current_timestamp,    –创建时间<br>update_user varchar(255),    –更新者<br>update_time timestamp not null default current_timestamp,    –更新时间<br>数仓基础用的</p><p>4.新增语句 怎么写？<br>insert into ruozedata.employee(name,age) values (‘zhangsan’,22);</p><p>5.修改语句 怎么写？<br>update ruozedata.employee set age=’25’ where name =’zhangsan’;</p><p>6.删除语句 怎么写？<br>delete from ruozedata.employee where id=1;</p><p>7.查询语句 怎么写？<br>select * from a</p><p>8.比如一张表5KW数据，一不小心 没有带where条件，是不是很致命？<br>是的</p><p>9.查询名称第三个字母是 s的<br>select * from a where name like ‘__s%’</p><p>10.建表自增长ID为主键，是不是比 uuid随机数这种好点？<br>是的,不会重复</p><p>11.where order by  group by limit  这个顺序对吗？（昨晚群里截图的 ，其他人）<br>错,先group by 再order by</p><p>12.谈谈你对子表/子查询的理解？<br>子表是查询条件作为表<br>子查询是查询条件作为字段</p><p>13.聚合函数有哪些 常见的？<br>sum,count,avg,min,max</p><p>14.group by … having 语法的，你的心得？<br>能使用having尽量使用having,不能使用时再使用子查询</p><p>15.join语法的哪三种？谈谈对第一种的理解？<br>left join,right join,inner join,full join<br>左表为主表,显示全部左表信息右表关联不到的为null</p><ol start="16"><li>a b表，其中aid null 1kw， bid null 10w数据。那么select xxxx from a left join b on a.aid=b.bid；请问致命吗？<br>致命,会出现10w*1kw的数据,笛卡尔积</li></ol><p>17.写sql还是后面spark代码（join的on条件字段还是group by字段），调优先学会抽样检查，知道自己的数据分布</p><p>18.a b表，其中aid “010” 1kw， bid “010” 10w数据。那么select xxxx from a left join b on a.aid=b.bid；请问致命吗?<br>和16一样</p><p>19.union all 和 union的区别<br>union all不去重,union去重</p><p>20.默认排序是 降序？<br>升序</p><p>21.写sql之前，你们觉得应该要做什么？</p>]]></content>
      
      
      <categories>
          
          <category> mysql练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.26早课</title>
      <link href="2018/10/30/2020-11-26-quannnxu-zao-ke/"/>
      <url>2018/10/30/2020-11-26-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.Linux的hosts文件，我们应该注意什么</p><p>2.Windows系统的hosts会找吗？</p><p>3.一般shell，代码这些我们与机器通信，是hostname吗？</p><p>4.后台执行脚本或命令，前后加什么?</p><p>5.执行xxx.sh脚本需要什么权限，哪两种方式执行</p><p>6.做软连接语法是什么? 主要应用场景哪两个? 同时需要注意什么?</p><p>7.crontab 编辑和查看参数是什么?  五个 * ，分别代表什么</p><p>8.作业  rundeck视频要看看做做，不要忘记</p><p>9.Linux现在遇见两个经典错误，链接拒绝，权限受限，会排查解决了吗？</p><p>10.mysql部署简述流程</p><p>11.对用户执行完权限相关操作，最后一句命令是什么?</p><p>12.%代表什么</p><p>13.创建用户和设置密码，那句sql会背吗？</p><p>14.vi简述编辑流程</p><p>15.Linux命令里带有大写 R的命令，哪两个?</p><p>16.rwxr–r–  数字多少</p><p>17.一个log文件很大，1G，找ERROR怎么办</p><p>18.个人环境变量文件在哪，怎样生效</p><p>19.su - 做什么事</p><p>20.三种方式切换用户的家目录</p><p>21.一个文件差不多1W行，我要去vi编辑一个参数xxx，但是我不知道在多少行，怎么快速找到?</p><p>1.Linux的hosts文件，我们应该注意什么<br>配置ip与机器名</p><p>2.Windows系统的hosts会找吗？<br>C:\Windows\System32\drivers\etc\hosts</p><p>3.一般shell，代码这些我们与机器通信，是hostname吗？<br>是的,hosts配置后hostname代表ip</p><p>4.后台执行脚本或命令，前后加什么?<br>nohub    …    &amp;</p><p>5.执行xxx.sh脚本需要什么权限，哪两种方式执行<br>./xxx.sh<br>sh xxx.sh</p><p>6.做软连接语法是什么? 主要应用场景哪两个? 同时需要注意什么?<br>ln -s<br>a.版本切换，脚本应用是配置的hadoop，是无感知的<br>b.小盘换大盘<br>注意的是：<br>权限 rwx  用户用户组</p><p>7.crontab 编辑和查看参数是什么?  五个 * ，分别代表什么<br>crontab -e </p><hr><p>分    时 日 月 周</p><p>8.作业  rundeck视频要看看做做，不要忘记<br><a href="https://www.bilibili.com/video/BV1Tb411c7nW?from=search&amp;seid=1376855633580937330">https://www.bilibili.com/video/BV1Tb411c7nW?from=search&amp;seid=1376855633580937330</a></p><p>9.Linux现在遇见两个经典错误，链接拒绝，权限受限，会排查解决了吗？<br>连接错误先ping ip,再telnet ip port,分析错误原因检查配置文件是否配置ip,防火墙是否关闭,安全组是否开启</p><p>10.mysql部署简述流程</p><p>11.对用户执行完权限相关操作，最后一句命令是什么?<br>flush privileges;</p><p>12.%代表什么<br>通配符,表示一串字符串</p><p>13.创建用户和设置密码，那句sql会背吗？<br>create user ‘UserName’@’%’ by ‘passwd’;<br>grant all privileges on <em>.</em> to ‘UserName’@’%’identified by ‘passwd’;<br>flush privileges;</p><p>14.vi简述编辑流程<br>i键进入编辑模式,插入内容,esc退出编辑模式,shift:进入尾行模式,wq保存退出</p><p>15.Linux命令里带有大写 R的命令，哪两个?<br>chmod    chown</p><p>16.rwxr–r–  数字多少<br>744</p><p>17.一个log文件很大，1G，找ERROR怎么办<br>cat xx.log | grep -C ERROR</p><p>18.个人环境变量文件在哪，怎样生效<br>~/.bash_profile<br>~/.bashrc<br>source生效</p><p>19.su - 做什么事<br>切换用户</p><p>20.三种方式切换用户的家目录<br>cd<br>cd ~<br>cd /home/username</p><p>21.一个文件差不多1W行，我要去vi编辑一个参数xxx，但是我不知道在多少行，怎么快速找到?<br>用/+关键字可以快速查找关键字。<br>被查到的关键字以高亮方式显示。如果根据关键字查到的有多个，可以通过按键“N”，快速定位到下一个高亮关键字。</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.25早课</title>
      <link href="2018/10/29/2020-11-25-quannnxu-zao-ke/"/>
      <url>2018/10/29/2020-11-25-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.高危命令有哪些</p><p>2.带R的命令哪些    </p><p>3.带r的命令哪些</p><p>4.块128m,三副本机制，一个文件180m，需要多少块，多少存储空间？</p><p>5.boss让我打开A服务器的 只知道名称好像含有xxx服务的，web界面，请问怎么做？</p><p>6.在杀死进程时，我们应该是否要确认 这个进程就是我们可以杀的？那么全局杀死所有 名称xxx的 进程，命令是什么？</p><p>7.which的执行结果输出，是来自哪个系统环境变量呢？</p><p>8.tar命令的压缩 、解压的命令参数 ？</p><p>9.谈谈你们对命令帮助 ，应该怎么看呢？会看吗？</p><p>10.查看系统负载、内存、磁盘 命令</p><p>1.高危命令有哪些<br>rm -rf;kill -9;&gt;</p><p>2.带R的命令哪些<br>chmod    chown    </p><p>3.带r的命令哪些<br>rm;cp等</p><p>4.块128m,三副本机制，一个文件180m，需要多少块，多少存储空间？<br>6个块;540m空间</p><p>5.boss让我打开A服务器的 只知道名称好像含有xxx服务的，web界面，请问怎么做？<br>先ps -ef | grep xxxx查看进程号<br>netstat -nlp 进程号    查看端口号<br>再ip+端口号进入web</p><p>6.在杀死进程时，我们应该是否要确认 这个进程就是我们可以杀的？那么全局杀死所有 名称xxx的 进程，命令是什么？<br>需要确认<br>kill -9 $(pgrep -f xxx)</p><p>7.which的执行结果输出，是来自哪个系统环境变量呢？<br>环境变量PATH中保存了命令的目录，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。</p><p>8.tar命令的压缩 、解压的命令参数 ？<br>tar -zxvf 解压<br>tar -zcvf 压缩</p><p>9.谈谈你们对命令帮助 ，应该怎么看呢？会看吗？<br>–help    查看参数和解释 【】为可选参数</p><p>10.查看系统负载、内存、磁盘 命令<br>top</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.24早课</title>
      <link href="2018/10/28/2020-11-24-quannnxu-zao-ke/"/>
      <url>2018/10/28/2020-11-24-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.个人环境变量文件，默认在哪里？推荐使用哪个呢？怎样生效？生效完成后，习惯做一件事是执行什么命令？</p><p>2.历史命令，执行第55行；清空历史命令</p><p>3.假如su/ssh 无法登录机器，这个用户是不是有可能在/etc/passwd文件里，做了禁止？</p><p>4.一个普通用户，想要瞬间临时获取root的最大权限，请问在哪个配置文件修改什么内容？博客写了吗？  在执行命令时，sudo命令加在前面？</p><p>5.vi编辑文件，想要从外部文件内容粘贴复制到这个文件，必须提前做一件什么事？否则数据丢失</p><p>6.window 和 Linux 去测试端口号的命令，会先部署吗？会使用吗？</p><p>7.netstat 一个服务时，显示  127.0.0.1:8899  ，外部window 或者其他服务器进行 访问，能通吗？</p><p>8.netstat 一个服务时，显示  hostname:8899  ，外部window 或者其他服务器进行 访问，假如不通，我们应该要调整什么？</p><p>9.rwx分别代表数字多少？  rwx–x-w- ,请问权限数字多少？</p><p>10.简述vi编辑文件的流程</p><p>1.个人环境变量文件，默认在哪里？推荐使用哪个呢？怎样生效？生效完成后，习惯做一件事是执行什么命令？<br>该用户的家目录<br>.bash_profile        .bashrc【推荐】<br>source生效<br>which xxx    (查看是否成功)</p><p>2.历史命令，执行第55行；清空历史命令<br>!55<br>history -c</p><p>3.假如su/ssh 无法登录机器，这个用户是不是有可能在/etc/passwd文件里，做了禁止？<br>是<br>mysqladmin:x:514:101::/usr/local/mysql:/bin/bash<br>apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin<br>ruoze:x:1004:1005::/home/ruoze:/usr/bin/false 没提示</p><p>4.一个普通用户，想要瞬间临时获取root的最大权限，请问在哪个配置文件修改什么内容？在执行命令时，sudo命令加在前面？<br>vi /etc/sudoers<br>hadoop   ALL=(root)      NOPASSWD:ALL<br>sudo xxx</p><p>5.vi编辑文件，想要从外部文件内容粘贴复制到这个文件，必须提前做一件什么事？否则数据丢失<br>先进入编辑模式—&gt;i键</p><p>6.window 和 Linux 去测试端口号的命令，会先部署吗？会使用吗？<br>先ping ip<br>再telnet ip port</p><p>7.netstat 一个服务时，显示  127.0.0.1:8899  ，外部window 或者其他服务器进行 访问，能通吗？<br>不能    127.0.0.1只可本机访问</p><p>8.netstat 一个服务时，显示  hostname:8899  ，外部window 或者其他服务器进行 访问，假如不通，我们应该要调整什么？<br>检查防火墙、安全组、/etc/httpd是否配置ip</p><p>9.rwx分别代表数字多少？  rwx–x-w- ,请问权限数字多少？<br>421    712</p><p>10.简述vi编辑文件的流程<br>i键进入编辑模式,插入,esc退出编辑模式,shift:进入尾行模式,wq保存退出</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.23早课</title>
      <link href="2018/10/26/2020-11-23-quannnxu-zao-ke/"/>
      <url>2018/10/26/2020-11-23-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.查看当前目录</p><p>2.隐藏文件、文件夹以什么开始，怎样查看</p><p>3.ls -l 等价于什么？</p><p>4.级联创建文件夹</p><p>5.创建文件哪几种方式</p><p>6.cp和mv谁快？ 能不能在cp 、mv过程，顺便修改名称呢？</p><p>7.查看文件的大小哪两种命令？</p><p>8.查看文件夹的大小的命令？</p><p>9.ll 命令执行完成后，文件、文件夹展示一长串信息，有哪些？</p><p>10.绝对路径和相对路径，区别是什么？</p><p>11.root的家目录在哪？普通用户的默认家目录在哪？</p><p>12.切换到用户的家目录，哪三种方式？</p><p>13.切换到 上一层目录 与 上一次目录，命令分别是什么？</p><p>14.清除屏幕信息</p><p>15.查看文件内容 哪几个命令？</p><p>16.实时查看文件内容，-f -F区别是什么？</p><p>17.谈谈你对  如何定位ERROR的 理解？</p><ol start="18"><li><blockquote><blockquote><blockquote><p>区别是什么？</p></blockquote></blockquote></blockquote></li></ol><p>19.设置变量  key= value  ,这样写对吗？如何引用呢？</p><p>20.全局环境变量文件在哪？ 个人环境变量文件(推荐哪个)在哪？如何生效？</p><p>1.查看当前目录<br>pwd</p><p>2.隐藏文件、文件夹以什么开始，怎样查看<br>以. 开头的；ls -a或ll -a</p><p>3.ls -l 等价于什么？<br>ll</p><p>4.级联创建文件夹<br>mkdir -p</p><p>5.创建文件哪几种方式<br>touch<br>vi</p><p>6.cp和mv谁快？ 能不能在cp 、mv过程，顺便修改名称呢？<br>mv 快<br>可以修改</p><p>7.查看文件的大小哪两种命令？<br>du -sh<br>ll -h</p><p>8.查看文件夹的大小的命令？<br>du -sh</p><p>9.ll 命令执行完成后，文件、文件夹展示一长串信息，有哪些？<br>权限，所属用户和用户组，创建/修改时间,文件大小<br>drwxrwxr-x 3 hadoop hadoop 4096 Nov 22 09:44 app</p><p>10.绝对路径和相对路径，区别是什么？<br>绝对路径以/开头<br>绝对路径可以精确引用，而相对路径则是模糊应用概念，只是在目标目录下找到引用</p><p>11.root的家目录在哪？普通用户的默认家目录在哪?<br>/root<br>/home/xxx</p><p>12.切换到用户的家目录，哪三种方式？<br>cd<br>cd /home/xx<br>cd ~</p><p>13.切换到 上一层目录 与 上一次目录，命令分别是什么？<br>cd  ..<br>cd -</p><p>14.清除屏幕信息<br>clear</p><p>15.查看文件内容 哪几个命令？<br>cat<br>less<br>more</p><p>16.实时查看文件内容，-f -F区别是什么？<br>-F 会一直try</p><p>17.谈谈你对  如何定位ERROR的 理解？<br>cat xx.log|grep -C 5 ERROR&gt;20201122.log</p><ol start="18"><li><blockquote><blockquote><blockquote><p>区别是什么？<br>覆盖</p></blockquote></blockquote><blockquote><p>追加</p></blockquote></blockquote></li></ol><p>19.设置变量  key= value  ,这样写对吗？如何引用呢？<br>不对 等号前后不能有空格<br>${key}</p><p>20.全局环境变量文件在哪？ 个人环境变量文件(推荐哪个)在哪？如何生效？<br>/etc/profile<br>~/.bashrc<br>source  ～/.bashrc</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.19早课</title>
      <link href="2018/10/25/2020-11-19-quannnxu-zao-ke/"/>
      <url>2018/10/25/2020-11-19-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.which  whereis命令区别</p><p>2.su  sudo命令区别</p><p>3.全局搜索</p><p>4.压缩解压  哪两个命令</p><p>5.vi三种模式是什么。如何切换</p><p>6.谈谈哪些命令是操作文件夹的</p><p>7.谈谈哪些命令是操作文件的</p><p>8.哪两个命令是 大R</p><p>9.谈谈左连接理解</p><p>10.mysql的 full join 是怎样实现的，在数据质量的 数据量校验 发挥作用，是否理解？</p><p>1.which  whereis命令区别<br>which 查看可执行文件的位置。  whereis 查看文件的位置。</p><p>2.su  sudo命令区别<br>u切换用户，sudo临时获取root权限</p><p>3.全局搜索<br>find / -name ‘<em>xxx</em>‘</p><p>4.压缩解压  哪两个命令<br>tar -zcvf 压缩<br>tar -zxvf 解压</p><p>5.vi三种模式是什么。如何切换<br>编辑模式    –&gt; i键<br>命令行模式    –&gt; esc<br>尾行模式    –&gt; shift:</p><p>6.谈谈哪些命令是操作文件夹的<br>mkdir/find/mv等</p><p>7.谈谈哪些命令是操作文件的<br>vi/cp/mv/cat/less/more/tail/echo/touch等</p><p>8.哪两个命令是 大R<br>chmod    chown</p><p>9.谈谈左连接理解<br>left join （左连接）：返回包括左表中的所有记录和右表中连接字段相等的记录。</p><p>10.mysql的 full join 是怎样实现的，在数据质量的 数据量校验 发挥作用，是否理解？<br>select * from A a<br>left join B b on a.id=b.id<br>union<br>select * from A a<br>right join B b on a.id=b.id</p><p>full join后的数据量和目标表、源表的数据量一致</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.18早课</title>
      <link href="2018/10/23/2020-11-18-quannnxu-zao-ke/"/>
      <url>2018/10/23/2020-11-18-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.which 命令作用是什么<br>which 　　查看可执行文件的位置。</p><ol start="2"><li>whereis  xxx命令<br>whereis 　查看文件的位置</li></ol><p>3.简述vi清空文件的步骤<br>先进入编辑模式,gg跳转首行,dG清空当前及以下行</p><p>4.谈谈  权限 相关的<br>r-4–&gt;读权限,w-2–&gt;写权限,x-1–&gt;执行权限<br>chmod 修改权限<br>chown 修改所属组</p><p>5.谈谈你对命令帮助的<br>–help    查看参数和可选参数,以及参数用法</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.17早课</title>
      <link href="2018/10/22/2020-11-17-quannnxu-zao-ke/"/>
      <url>2018/10/22/2020-11-17-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.查看硬盘命令，一般分为哪两种盘</p><p>2.查看内存命令，剩余内存怎么看</p><p>3.使用top命令，可以查看哪些数据</p><p>4.which  命令，命令出现的位置和命令使用时，其实在哪配置的</p><p>5.下载命令</p><p>6.上传命令</p><p>7.tar压缩解压 </p><p>8.定时调度查看，编辑语法</p><p>9.set -u是万能的吗？假如K=  空，也是致命的?</p><p>10.rw-r-x-wx   权限数字多少</p><p>最好先自己思考一下,再看后面的答案.</p><p>1.查看硬盘命令，一般分为哪两种盘<br>df -h<br>分为系统盘和数据盘</p><p>2.查看内存命令，剩余内存怎么看<br>free -m<br>剩余空间:total-used/toatl<br>预留内存最好在15%左右</p><p>3.使用top命令，可以查看哪些数据<br>查看cpu使用情况<br>load average: 0.00, 0.01, 0.05        1m 5m 15m的平均负载<br>经验值:10 生产上尽量控制在10,否则服务器就认为卡<br>PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND<br> 1466 root      10 -10  131500  13332  10160 S   0.3  0.2   3:18.22 AliYunDun<br>    1 root      20   0   43384   3776   2576 S   0.0  0.0   0:01.34 systemd     </p><p>4.which  命令，命令出现的位置和命令使用时，其实在哪配置的<br>配置在环境变量里</p><p>5.下载命令<br>rz</p><p>6.上传命令<br>sz</p><p>7.tar压缩解压<br>tar -zcvf xxx.tar.gz ./*    压缩<br>tar -zxvf xxx.tar.gz -C /xxx/    解压(指定目录)</p><p>8.定时调度查看，编辑语法<br>crontab -e</p><ul><li><ul><li><ul><li><ul><li><ul><li>分 时 日 月 周</li></ul></li></ul></li></ul></li></ul></li></ul><p>9.set -u是万能的吗？假如K=  空，也是致命的?<br>不是万能的,变量不可赋空</p><p>10.rw-r-x-wx   权限数字多少<br>653</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.16早课</title>
      <link href="2018/10/21/2020-11-16-quannnxu-zao-ke/"/>
      <url>2018/10/21/2020-11-16-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.简述mysql安装流程<br>创建my.cnf<br>创建用户组及用户<br>配置环境变量<br>赋权限和用户组，切换用户mysqladmin，安装<br>配置服务及开机自启动<br>安装libaio及安装mysql的初始db<br>查看临时密码<br>启动<br>登录及修改用户密码<br>重启</p><p>2.MySQL默认配置文件在哪<br>/etc/my.cnf</p><p>3.建表规范谈谈看<br>create table ruozedata.rz(<br>id int(11)  not null auto_increment, –第一列必须是自增长id    </p><p>name varchar(255),<br>age  int(3),            –comment ‘xxx’  业务字段加上注释</p><p>create_user varchar(255),<br>create_time timestamp not null default current_timestamp,</p><p>update_user varchar(255),<br>update_time timestamp not null default current_timestamp on update current_timestamp,<br>primary key(id)  主键= 唯一约束 +not null非空约束<br>);</p><p>4.模糊查询第三个字符为P<br>select * from table where 字段 like ‘__p%’(_占位符)</p><p>5.合并去重  语法<br>select * from A<br>union<br>select * from B</p><p>6.mysql默认端口号多少<br>3306</p><p>7.测试服务通不通  怎么办<br>先ping ip<br>再telnet ip port</p><p>8.常用错误有哪些<br>Command not found<br>permission defined<br>connect refused</p><p>9.命令找不到  怎么办<br>检查安装包安装了没有<br>环境变量是否配置</p><p>10.高危命令有哪些<br> rm -rf; &gt;; kill -9</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.13早课</title>
      <link href="2018/10/19/2020-11-13-quannnxu-zao-ke/"/>
      <url>2018/10/19/2020-11-13-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.全局环境变量在哪，怎么生效<br>/etc/profile    source或者. 生效</p><p>2.个人环境变量文件 默认在哪？ 一般使用哪个？ 个人环境变量文件一定在默认位置吗？（一般存放在当前用户的家目录）<br><del>/.bash_profile 和</del>/.bashrc位置<br>一般使用/.bashrc</p><p>3.声明环境变量  必须类似这样  export  JAVA_HOME = /usr/java/jdk1.8，是否有问题？<br>exprot JAVA_HOME=/usr/java/jdk1.8<br>export PATH=$JAVA_HOME/bin:$PATH【推荐】<br>=前后不能有空格</p><p>4.在环境变量文件 PATH=$JAVA_HOME/bin:$PATH    这样写法一般在最后，是声明JAVA的bin文件夹的可执行文件 java，这样敲命令 java，就能够找到</p><p>5.发现用户无法登录或者 无法执行命令 ，有可能什么文件导致？<br>/etc/passwd<br>文件中用户的最后一段为 /bin/false<br>mysqladmin:x:514:101::/usr/local/mysql:/bin/bash</p><p>6.同事给你交接服务器，就简单交接，请问拿到服务器，一般操作什么？<br>查看一下常规操作有哪些，搜索一下相应的软件包在什么地方<br>find / -name ‘<em>xxx</em>‘ 查看部署内容和位置<br>history</p><p>7.高危命令哪些<br>rm -fr</p><blockquote></blockquote><p>kill -9 (今天)</p><p>在我看来 生产上操作 修改 尤其不是自己掌管的服务配置文件，是不是高危？要不要提前备份呢？<br>是高危,需要备份</p><p>8.在生产上常见错误是什么 ，当前讲了两个<br>commond not found和permission defined</p><p>9.sudo  su 分别做什么<br>sudo su - jepson  这样执行是否OK？<br>sudo普通用户临时切换root的权限，su切换用户<br>可以执行</p><p>10.命令帮助是否会看，看什么？<br>–help查看[可选项option]和必填项以及后面的英文解释,【】里可选参数</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.12早课</title>
      <link href="2018/10/18/2020-11-12-quannnxu-zao-ke/"/>
      <url>2018/10/18/2020-11-12-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.经典错误1是什么  大概什么导致<br>command not found<br>要不就是没有安装，yum安装或者tar解压安装<br>要不就是安装好了，没有配置环境变量  到PATH里配置</p><p>2.经典错误2是什么  大概什么导致<br>permission denied<br>首先分析出 你去访问是什么用户===》 文件或者文件夹是什么用户对应什么权限、什么用户组对应什么权限、其他用户对应什么权限，一一比对<br>是否可以满足访问的需求：如  读、写、执行<br>如果不满足，则：<br>2.1 修改对应权限 chmod<br>2.2 修改所属用户或者用户组  chown<br>2.3 切换对应的 所属用户来操作（一般所属的用户的权限是 rwx）</p><ol start="3"><li>rwx分别代表什么<br>读 写 执行 421</li></ol><p>4.r–r—-x  数字多少<br>441</p><p>5.切换用户命令是什么，带-  有什么作用<br>su    切换该用户并执行该用户的环境变量</p><p>6.临时最大权限的命令是什么，该怎么配置？<br>sudo   编辑/etc/sudoers  添加ruozedata(用户名)  ALL=(ALL)       ALL</p><p>7.vi 简述编辑流程的快捷键<br>i    –&gt;编辑模式<br>编辑内容<br>esc    –&gt;退出编辑模式<br>shift+:    –&gt;尾行模式<br>wq    –&gt;保存退出</p><p>9.vi清空文件内容，最关键的两个命令是什么<br>gg dG        gg(跳转到首行首字)    Dg(删除当前行及以下所有行)</p><p>10.高危命令哪几个？<br>rm -rf ,&gt;</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础03</title>
      <link href="2018/10/17/2020-11-13-quannnxu-linux3/"/>
      <url>2018/10/17/2020-11-13-quannnxu-linux3/</url>
      
        <content type="html"><![CDATA[<h2 id="1-查看磁盘空间"><a href="#1-查看磁盘空间" class="headerlink" title="1. 查看磁盘空间"></a>1. 查看磁盘空间</h2><p>[root@warehouse001 ~]# df -h<br>Filesystem      Size  Used Avail Use% Mounted on<br>/dev/vda1        40G   11G   27G  29% /<br>devtmpfs        3.7G     0  3.7G   0% /dev<br>tmpfs           3.7G     0  3.7G   0% /dev/shm<br>tmpfs           3.7G  496K  3.7G   1% /run<br>tmpfs           3.7G     0  3.7G   0% /sys/fs/cgroup<br>tmpfs           756M     0  756M   0% /run/user/0<br>[root@warehouse001 ~]# </p><p>/dev/vda1        40G   11G   27G  29% /    系统盘</p><p>/dev/vdb1        2T    0            2T    0%    /data01        数据盘</p><h3 id="2-查看内存"><a href="#2-查看内存" class="headerlink" title="2. 查看内存"></a>2. 查看内存</h3><p>free -m<br>[root@warehouse001 ~]# free -m<br>              total        used        free      shared  buff/cache   available<br>Mem:    7552         800         162           0        6590        6442<br>Swap:        0           0           0</p><p>剩余空间:total-used/toatl</p><p>预留内存最好在15%左右</p><p>swap    因为内存不够,使用部分磁盘空间来充当内存使用,虽然可以解决内存紧缺的问题,但是效率不高.尤其大数据,swap哪怕设置了大小,也尽量设置惰性使用.</p><h3 id="3-机器负载"><a href="#3-机器负载" class="headerlink" title="3.机器负载"></a>3.机器负载</h3><p>load average: 0.00, 0.01, 0.05<br>                1m    5m        15m    </p><pre><code>经验值:10 生产上尽量控制在10,否则服务器就认为卡</code></pre><p>a.计算程序hivesql,spark,flink密集计算是不是要调优<br>b.是不是被挖矿了<br>6240 root      10 -10  131208  13072  10192 S   1.0  0.2   9:18.02 AliYunDun<br>                                                                                 cpu<br>查看负载和cpu<br>c.硬件问题    内存条损坏    最后一招,万能重启,检测是不是硬件问题</p><h3 id="4-安装"><a href="#4-安装" class="headerlink" title="4.安装"></a>4.安装</h3><p>yum install</p><p>yum remove</p><p>yum search xxx 搜索应用</p><h3 id="5-查看进程"><a href="#5-查看进程" class="headerlink" title="5.查看进程"></a>5.查看进程</h3><p>ps -ef | grep xxx(进程名称)<br>[root@warehouse001 ~]# ps -ef | grep httpd<br>root     11615 11102  0 16:02 pts/1    00:00:00 grep –color=auto httpd<br>            pid      父id</p><h3 id="6-端口号"><a href="#6-端口号" class="headerlink" title="6.端口号"></a>6.端口号</h3><p>netstat -nlp | grep 0<br>tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      3845/sshd<br>总结:<br>a.有进程pid,不一定有端口号<br>b.服务的通信交流,其实就是要ip+端口号(交互)<br>如:那个机器上运行xxx服务,打开web页面<br>ip<br>ps -ef | grep xxx 找到pid</p><p>netstat -nlp | grep pid</p><p><a href="https://139.224.129.11:80/">https://139.224.129.11:80</a></p><p>windows:提前安装telnet客户端,控制面板卸载程序里面打开启用或关闭windows功能勾选telnet选项</p><p>[root@warehouse001 ~]# telnet 139.224.129.11 22<br>Trying 139.224.129.11…<br>Connected to 139.224.129.11.<br>Escape character is ‘^]’.<br>SSH-2.0-OpenSSH_7.4</p><p>先ping ip再telnet ip+port</p><h3 id="7-卸载"><a href="#7-卸载" class="headerlink" title="7.卸载"></a>7.卸载</h3><p>yum remove<br>–nodeps                         do not verify package dependencies<br>不检查包的依赖性</p><p>connection refused    连接拒绝<br>connection time out</p><p>杀死进程<br>kill -9 pid 【高危命令】<br>误杀造成生产事故<br>kill -9 $(pgrep -f 匹配字符) 全局杀</p><p>[root@warehouse001 ~]# netstat -nlp | grep ssh<br>tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 :::22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 139.224.129.11:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>22端口号服务器可以对外</p><p>tcp        0      0 localhost:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 127.0.0.1:22              0.0.0.0:*               LISTEN      3845/sshd </p><p>localhost,127.0.0.1代表本机,外部不可访问</p><p>在/etc/httpd里面配置</p><h3 id="8-下载wget"><a href="#8-下载wget" class="headerlink" title="8.下载wget"></a>8.下载wget</h3><h3 id="9-压缩和解压"><a href="#9-压缩和解压" class="headerlink" title="9.压缩和解压"></a>9.压缩和解压</h3><p>zip -r 报名.zip 压缩目录<br>unzip 包名</p><p>tar -czvf xxx.tar.gz xxx/*<br>tar -xzvf xxx.tar.gz</p><h3 id="10-command-not-found"><a href="#10-command-not-found" class="headerlink" title="10.command not found"></a>10.command not found</h3><p>没有安装<br>没有配置环境变量</p><p>[root@warehouse001 ~]# which java1<br>/usr/bin/which: no java1 in (/usr/java/jdk1.8.0_45/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)<br>/etc/profile<br>exprot JAVA_HOME=/XXX<br>export PATH=$JAVA_HOME/bin:$PATH【推荐】</p><h3 id="11-定时"><a href="#11-定时" class="headerlink" title="11.定时"></a>11.定时</h3><p>crontab -e    编辑一个定时器文件</p><p>`* * * * * sleep 10s;date &gt;&gt; /root/ruoze.log</p><p>分 时 日 月 周(最低粒度是分)<br>每隔10s打印一次</p><pre class=" language-shell"><code class="language-shell">#/bin/bashset -u for ((i=1;i<=6;i++));do    date    sleep 10sdone</code></pre><p>`* * * * *  z10s.sh &gt;&gt; /root/ruoze.log</p><h3 id="12-后台执行脚本"><a href="#12-后台执行脚本" class="headerlink" title="12.后台执行脚本"></a>12.后台执行脚本</h3><p>nohub … &amp; 后台执行<br>nohub /root/ruoze.sh &gt;&gt; ruoze.log 2&gt;&amp;1 &amp;</p><p>rm -rf </p><p>#!/bin/bash<br>set -u</p><p>row=0<br>#arry=(t1 t2)<br>arry=(t1)</p><p>while(( ${row} &lt;2))<br>do<br>    tablename=$(arry[${row}])<br>    echo ${tablename}</p><pre><code>row=$(($&#123;row&#125;+1))</code></pre><p>done</p><p>拿到别人的脚本千万不要太自信,千万不要直接在生产操作<br>假如修改先去开发环境测试验证,不要在生产环境随意执行<br>set -u 只会在参数不存在时报错,参数为空时不会报错</p><p>生产一定要有敬畏之心 宁愿少做也不要做错 做之前必须开发环境走一下 哪怕生产有临时变动操作,请及时知会领导,让领导复核</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.11早课</title>
      <link href="2018/10/16/2020-11-11-quannnxu-zao-ke/"/>
      <url>2018/10/16/2020-11-11-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.vi三种模式<br>命令行模式、编辑模式、最后行模式</p><p>2.高危命令有哪两个？<br>rm -rf，&gt;(覆盖)</p><p>3.实时查看 大小F 区别<br>-f   假如文件被移除 然后重命名 就无法再监控到文件<br>-F   假如文件被移除 然后重命名 会不断的retry尝试 去监控文件，直到监控到位</p><p>4.管道符 是什么 ，过滤是什么<br>管道符 |     一个命令的标准输出传送到另一个命令的标准输入中<br>过滤 grep</p><p>5.别名语法是什么<br>alias</p><p>6.个人的  用谁   .bash_profile    .bashrc<br>用.bashrc</p><p>7.tab按一次 ，2次分别什么意思<br>tab键一次，只有1个 命令自动补全<br>按二次，会把当前匹配到的 所有 打印出来，再挑选</p><p>8.history -c 是干什么<br>清空历史命令(堡垒机无效哦)</p><p>9.chmod -R 777 /  这个命令生产上是否允许执行？<br>不允许,会把根目录下所有文件所有用户都可以读写执了</p><p>10.除了ftp sftp上传下载工具，还有什么命令？<br>rz(上传) sz(下载)<br>默认系统不自带，安装命令是<br>sudo yum install lrzsz -y</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.10早课</title>
      <link href="2018/10/15/2020-11-10-quannnxu-zao-ke/"/>
      <url>2018/10/15/2020-11-10-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.隐藏文件文件夹怎么标识，如何查看<br>隐藏文件以.开头        ls -a或者ll -a</p><p>2.级联创建文件夹参数是什么<br>-p</p><p>3.移动复制什么区别，是否可以改名<br>复制最终产生两份文件    速度慢<br>移动最终只有一份文件    速度快<br>都可以改名</p><p>4.rwx分别代表数字多少<br>r-&gt;4 w-&gt;2 x-&gt;1</p><p>5.rwxr–r-x  数字多少<br>rwx -&gt;7     r– -&gt;4     r-x -&gt;5<br>所以就是745</p><p>6.如上 三组，分别代表什么<br>所属用户,所属用户组其他成员,其他用户组成员</p><p>7.查看命令帮助会不?  要看什么<br>命令 –help，看usage用法还有option选项<br>【】为可选参数</p><p>8.发现一个用户登录不上或者无法执行命令  是什么文件问题<br>/etc/passwd</p><p>9.切换用户 带  -，标识什么<br>切换该用户并执行该用户的环境变量</p><p>10.临时获取root权限 ，会配置吗？ 执行要加什么？<br>使用root用户在/etc/sudoers 给用户加上ALL，如:ruozedata  ALL=(ALL)       ALL<br>执行加sudo</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础02</title>
      <link href="2018/10/15/2020-11-08-quannnxu-linux2/"/>
      <url>2018/10/15/2020-11-08-quannnxu-linux2/</url>
      
        <content type="html"><![CDATA[<h2 id="1-rm-删除"><a href="#1-rm-删除" class="headerlink" title="1.rm 删除"></a>1.rm 删除</h2><pre class=" language-shell"><code class="language-shell">rm -rf  #强制删除-f #直接不提示 直接删除</code></pre><p>场景:</p><p>LOG_PATH=””</p><p>业务逻辑判断 赋值 /xxxx/yyyy</p><p>​    漏了一种</p><p>rm -rf ${LOG_PATH}$/*  ==&gt; rm -rf /*</p><p>如何避免:</p><p>每次删除之前,都判断${LOG_PATH}$目录是否存在</p><p>set -u参数    在脚本的第一行#!bin/bash,在第二行写(赋值为空报错)</p><h2 id="2-添加用户"><a href="#2-添加用户" class="headerlink" title="2.添加用户"></a>2.添加用户</h2><p>[root@hadoop01 ~]# useradd quanxu<br>[root@hadoop01 ~]# id quanxu<br>uid=502(quanxu) gid=503(quanxu) 组=503(quanxu)</p><pre class=" language-shell"><code class="language-shell">cat /etc/passwd    #查看密码cat /etc/group    #查看用户组</code></pre><p>切换用户</p><pre class=" language-shell"><code class="language-shell">su - quanxu</code></pre><p>删除用户</p><pre class=" language-shell"><code class="language-shell">userdel quanxu</code></pre><p>删除用户,那么该用户所属的用户组加入没有其他用户则一起删除;否则有其他用户则不能删除</p><p>[root@hadoop01 quanxu]# rm -f /home/quanxu/.bash*</p><p>[root@hadoop01 quanxu]# su - quanxu<br>-bash-4.1$ </p><p>当前用户的所属家目录的个人环境变量不存在</p><p>一个用户默认创建的家目录是在/home/用户名,但是后期可以修改</p><pre class=" language-shell"><code class="language-shell">usermod -d /tmp #单独使用时，只是把保存在/etc/passwd这个配置文件当中的源目录名改成指定的新目录名，并不会把源目录下的内容移动到新目录下，如果要把源目录下的内容移动新目录下，则要和-m选项一起使用，才会把源目录下的内容移动到新目录下。</code></pre><pre class=" language-shell"><code class="language-shell">usermod -a -G dw qx  #添加用户qx  到组dw 里</code></pre><pre class=" language-shell"><code class="language-shell">usermod -g dw xx     #修改用户所属的群组</code></pre><pre class=" language-shell"><code class="language-shell">groupsmems -g xx -l #查看组的所有成员</code></pre><p>用户切换不了查看/etc/passwd文件</p><p>sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin #不可登录</p><p>quanxu:x:502:503::/home/quanxu:/bin/bash/false #禁止切换</p><pre class=" language-shell"><code class="language-shell">sudo #普通用户临时使用root的最大权限</code></pre><h2 id="4-权限"><a href="#4-权限" class="headerlink" title="4.权限"></a>4.权限</h2><pre class=" language-shell"><code class="language-shell">drwxr-xr-x.  22 root root  4096 7月  10 07:58 vard    rwx    r-x    r-x #第一个字符d代表文件夹 -文件#接下来三组的三个字母 分别是代表 读r 4,写w 2,执行x 1,-没有权限 0rwx:7        r-x:5        r-x :5#所属用户    所属组的成员    其他组的成员777 rwxrwxrwx755 rwxr-xr-x-rw-rw-r--. 1 quanxu quanxu   15 7月  11 02:29 quan.logchmod 640 quan.log    #所属成员可读可写,所属组其他成员可读,其他组成员不可读写执</code></pre><h2 id="5-大小"><a href="#5-大小" class="headerlink" title="5.大小"></a>5.大小</h2><p>文件大小:ll -h</p><p>文件或文件夹大小:du -sh</p><h2 id="6-搜索-find"><a href="#6-搜索-find" class="headerlink" title="6.搜索    find"></a>6.搜索    find</h2><p>find / -name ‘<em>hadoop</em>‘</p><p>history    查看历史操作</p><p>ps -ef    查看进程</p><h2 id="7-vi编辑"><a href="#7-vi编辑" class="headerlink" title="7.vi编辑"></a>7.vi编辑</h2><p>良好习惯:</p><p>vi 编辑生产配置文件</p><p>先cp conf conf20201108</p><p>粘贴的坑:</p><p>必须先进入编辑模式,否则第一行内容丢失,不完整</p><p>在命令行模式,常用的快捷方式:</p><pre class=" language-shell"><code class="language-shell">dd    删除当前行dG    删除当前行及以下所有行gg    跳转到第一行的第一个字母G    跳转到最后一行的第一个字母</code></pre><p>shift+$ 行尾</p><p>如何通过vi命令进行清空文件内容:</p><p>dd dG</p><p>i</p><p>复制粘贴</p><p>esc</p><p>shfit+:wq</p><pre class=" language-shell"><code class="language-shell">#尾行模式下set nu    #显示行号set nonu    #恢复正常</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础01</title>
      <link href="2018/10/13/2020-11-04-quannnxu-linux/"/>
      <url>2018/10/13/2020-11-04-quannnxu-linux/</url>
      
        <content type="html"><![CDATA[<p>一.Linux基础命令</p><p>pwd 查看当前路径</p><p>[root@hadoop01 ~]# pwd</p><pre class=" language-shell"><code class="language-shell">/root/ #代表根ls #查看ls #显示的文件夹 文件的名称ls -l #权限 用户用户组 时间 => ll</code></pre><pre class=" language-shell"><code class="language-shell">ls -l -a #显示隐藏文件#以.开头的文件或文件夹就是隐藏文件ls -l -h #仅仅查看文件的大小ls -l -r -t #按时间排序 => ls -lrtmkdir #创建文件夹mkdir 1 2 3 #并联创建文件夹mkdir -p 4/5/6 #串联创建文件夹cd #切换目录cd ../ #退回上层目录cd ../../  #退回上两层目录#回到当前用户家目录cd 回车cd ~cd - #回退到上一次目录</code></pre><pre class=" language-shell"><code class="language-shell">目录绝对目录  全路径相对目录  相对路径</code></pre><h2 id="1-clear-清理屏幕"><a href="#1-clear-清理屏幕" class="headerlink" title="1.clear 清理屏幕"></a>1.clear 清理屏幕</h2><h2 id="2-mv-始终是一份-快"><a href="#2-mv-始终是一份-快" class="headerlink" title="2.mv 始终是一份 快"></a>2.mv 始终是一份 快</h2><p>cp 两份 慢</p><h2 id="3-创建文件"><a href="#3-创建文件" class="headerlink" title="3.创建文件"></a>3.创建文件</h2><pre class=" language-shell"><code class="language-shell">#创建一个空文件 touch 1.logvi 2.log#i键        进入编辑模式#esc        退出编辑模式#shift+:     从命令行模式进入尾行模式#\> 覆盖        [高危命令]#\>> 追加</code></pre><h2 id="4-查看文件命令"><a href="#4-查看文件命令" class="headerlink" title="4.查看文件命令"></a>4.查看文件命令</h2><p>cat 文件内容一下全部显示</p><p>more 文件内容一页页往下 翻,按空格键,ctrl+b回退</p><p>less 一行一行滚动,上下左右键</p><h2 id="5-tail实时查看文件内容"><a href="#5-tail实时查看文件内容" class="headerlink" title="5.tail实时查看文件内容"></a>5.tail实时查看文件内容</h2><p>-f 假如文件移除 然后重命名 就无法再监控到</p><p>-F 文件删除 然后重命名 会不断的retry</p><p>tail -100f 1.log 查看后一百行</p><h2 id="6-log4j-规则-大小100M-保留10份"><a href="#6-log4j-规则-大小100M-保留10份" class="headerlink" title="6.log4j    规则:大小100M,保留10份"></a>6.log4j    规则:大小100M,保留10份</h2><p>erp.log<br>               mv erp.log erp.log1<br>               touch erp.log</p><pre><code>   erp.log  空的---》100m   erp.log1 100m        mv erp.log erp.log2            touch erp.log   erp.log  空的   erp.log1 100m   erp.log2 100m   log---&gt;flume--&gt;kafka</code></pre><p>文件内容特别多 如何快速定位到ERROR、关键词信息<br>cat CloudAgent.log | grep ERROR</p><p>cat CloudAgent.log | grep -A 10 ERROR 后10行<br>cat CloudAgent.log | grep -B 10 ERROR 前10行<br>cat CloudAgent.log | grep -C 10 ERROR 前后各10行 20行 【推荐】<br>| 管道符<br>grep过滤</p><p>如:</p><pre class=" language-shell"><code class="language-shell">cat CloudAgent.log | grep -C 20 ERROR > 20201107error.logmore 20201107error.log</code></pre><p>通过编辑去搜索<br>    vi xxx.log<br>    shift+:<br>    /ERROR 回车<br>    n键寻找 </p><p>将日志文件 下载到window电脑，进行搜索 定位 分析  【推荐】<br>坑: 假如CloudAgent.log 原文件很大，那么从生产下载到公司网络 是不是要走外网带宽10M的，<br>想问 会不会影响 公司服务？<br>建议： 假如下载大文件，业务高峰或者工作日白天 尽量不要做，非要做，那就【限速】（FTP设置）</p><h2 id="5-上传下载"><a href="#5-上传下载" class="headerlink" title="5.上传下载"></a>5.上传下载</h2><p>yum install -y lrzsz<br>sz xxx.log 下载 Linux–》window<br>rz 直接回车 相反的</p><h2 id="6-别名-alias"><a href="#6-别名-alias" class="headerlink" title="6.别名 alias"></a>6.别名 alias</h2><p>ls -l ==&gt;ll </p><h2 id="7-环境变量"><a href="#7-环境变量" class="headerlink" title="7.环境变量"></a>7.环境变量</h2><p>全局:/etc/profile</p><p>个人:~/.bash_profile</p><p>​        ~/.bashrc 【推荐】</p><p>加好后 source一下,在新的窗口生效</p><p>场景：<br>      ssh 远程执行B机器 启动服务命令 抛错， java command not found<br>          直接登录B机器 命令是找到的  which java找到</p><p>配置环境变量文件在 .bash_profile 是不正确的，<br>     应该配置在 .bashrc文件里。</p><h2 id="8-创建用户和设置密码"><a href="#8-创建用户和设置密码" class="headerlink" title="8.创建用户和设置密码"></a>8.创建用户和设置密码</h2><p>useradd jj    </p><p>passwd jj</p><h2 id="9-tab"><a href="#9-tab" class="headerlink" title="9.tab"></a>9.tab</h2><p>按一次 匹配只有一串的 就补齐这串<br>按2次 打印出匹配的所有字符</p><h2 id="10-历史命令-history"><a href="#10-历史命令-history" class="headerlink" title="10.历史命令 history"></a>10.历史命令 history</h2><p>alias ll=’rm -rf /*’</p><p>history -c  清空</p><p>堡垒机仍然会被记录</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java基础01</title>
      <link href="2018/08/13/java-day01/"/>
      <url>2018/08/13/java-day01/</url>
      
        <content type="html"><![CDATA[<p>JDK包含JRE和java工具</p><h4 id="1-hello-world解析"><a href="#1-hello-world解析" class="headerlink" title="1.hello world解析"></a>1.hello world解析</h4><h4 id="2-语言基础"><a href="#2-语言基础" class="headerlink" title="2.语言基础"></a>2.语言基础</h4><h5 id="1-变量"><a href="#1-变量" class="headerlink" title="1.变量"></a>1.变量</h5><p>​    int a=1;        在内存中申请一个空间来储存变量</p><p>​        A.类型    B.名称    C.这个区域的值可以不断变化(a=2)        </p><p>什么时候定义变量?</p><p>整数类型:  byte,short,int,long</p><p>浮点:float,double</p><p>字符:char</p><p>布尔</p><p>class,interface,[]</p><pre class=" language-javvva"><code class="language-javvva">package com.company;public class Main &#123;   /*   1.程序入口,个类有了一个main,他就可以独立运行   2.这个main方法是被谁调用的?JVM   3.javac编译之后生成一个.class的文件.java Main.class    */   public static void main(String[] args) &#123;   // write your code//        byte b1,b2,b3;//        //byte取值范围?类型提升从byte提升到int.//        b1=5;//        b2=3;//        b3=b1+b2;     编译报错,原因b3超出byte的字节长度,-128~127       byte b1,b2;       int b3;       b1=5;       b2=3;       b3=b1+b2;//        byte b;//        b=5+111000;     超出长度       //int i=3;       System.out.println(b3);   &#125;&#125;</code></pre><h5 id="2-常量"><a href="#2-常量" class="headerlink" title="2.常量"></a>2.常量</h5><p>变量的特殊形式,它的值不变,关键字final,名称定义成大写</p><p><img src="https://i.loli.net/2020/11/29/gkPscImORNYtLCQ.png"></p><p>1.整数,小数</p><p>2.布尔</p><p>3.字符(串)</p><p>4.null</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
