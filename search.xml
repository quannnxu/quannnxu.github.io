<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hadoop05之MapReduce/yarn架构</title>
      <link href="2018/11/30/2020-11-29-quannnxu-hadoop5/"/>
      <url>2018/11/30/2020-11-29-quannnxu-hadoop5/</url>
      
        <content type="html"><![CDATA[<p>tip1:</p><p>A表:主表</p><p>id    name</p><p>1        a        1</p><p>2        b        1</p><p>3        c        1</p><p>4        c        1</p><p>null    x        1w</p><p>B表:明细表</p><p>id    name</p><p>1        a        1w</p><p>2        b        2w</p><p>3        c        3w</p><p>null    x        4w</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token keyword">from</span> <span class="token number">a</span><span class="token keyword">left</span> <span class="token keyword">join</span> <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id</code></pre><p>当null值join时会产生4wx1w数据</p><p>先要过滤null,否则会出现笛卡尔积</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span>id<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span> <span class="token number">c</span><span class="token keyword">from</span> <span class="token number">a</span><span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">)</span> <span class="token number">d</span> <span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token number">c</span> <span class="token keyword">desc</span> <span class="token keyword">limit</span> <span class="token number">100</span></code></pre><p>如果结果都是1则不需要担心,否则就要考虑这些值是否需要,如果需要的话只能慢慢跑,不需要的话就过滤掉.</p><p>tip2:</p><p>关于被挖矿后如何解决:</p><p><a href="https://blog.csdn.net/kevin_darkelf/article/details/46042739">https://blog.csdn.net/kevin_darkelf/article/details/46042739</a></p><p>分析脚本</p><p>一般给脚本权限 赋予 000</p><p>分析脚本，一般把目录删除掉，会又被下载，一般都是通过 wget命令</p><p>将wget命令 卸载掉</p><p>或者将wget重命名</p><p>最后一招，云主机 客户 工单，重置系统如何做</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop04之hdfs命令</title>
      <link href="2018/11/29/2020-11-28-quannnxu-hadoop4/"/>
      <url>2018/11/29/2020-11-28-quannnxu-hadoop4/</url>
      
        <content type="html"><![CDATA[<p>1.SNN(检查点动作)</p><p><img src="https://i.loli.net/2020/11/29/yqU1YtOkIhZVxSn.png"></p><p>517是正在编辑的日志                                                                                                                                                                                                                                                                                                             </p><p>1.snn执行checkpoint动作的时候,nn会使用当前的edit文件5515-516,nn会暂时将读写操作记录到一个新的edit文件中5117</p><p>2.snn将nn的fsimage 514和edits文件515-516远程下载到本地</p><p>3.snn将fsimage 514加载到内存中,将edits文件515-516年内容之内存中从头到尾的执行一次,创建一个新的fsimage文件516</p><p>4.snn将新的fsimage 516推送给nn</p><p>5.nn接受到fsimage 516.ckpt滚动为edit 157</p><p>是一份最新的</p><p>SNN主要目的是为了备份,但是生产不用,用的是HA,仅学习过程了解</p><p>HA:</p><p>NN(active)        NN(standby)热备</p><h4 id="2-hdfs命令"><a href="#2-hdfs命令" class="headerlink" title="2.hdfs命令"></a>2.hdfs命令</h4><h5 id="1-hadoop命令"><a href="#1-hadoop命令" class="headerlink" title="1.hadoop命令"></a>1.hadoop命令</h5><p>[hadoop@warehouse001 bin]$ ./hadoop<br>Usage: hadoop [–config confdir] COMMAND<br>       where COMMAND is one of:<br>  fs                   run a generic filesystem user client<br>  version              print the version<br>  jar <jar>            run a jar file<br>  checknative [-a|-h]  check native hadoop and compression libraries availability<br>  distcp <srcurl> <desturl> copy file or directories recursively<br>  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive<br>  classpath            prints the class path needed to get the<br>  credential           interact with credential providers<br>                       Hadoop jar and the required libraries<br>  daemonlog            get/set the log level for each daemon<br>  s3guard              manage data on S3<br>  trace                view and modify Hadoop tracing settings<br> or<br>  CLASSNAME            run the class named CLASSNAME</p><p>Most commands print help when invoked w/o parameters.<br>[hadoop@warehouse001 bin]$ pwd<br>/home/hadoop/app/hadoop/bin</p><h5 id="2-fs命令"><a href="#2-fs命令" class="headerlink" title="2.fs命令"></a>2.fs命令</h5><p>[hadoop@warehouse001 bin]$ hadoop fs<br>Usage: hadoop fs [generic options]<br>        [-appendToFile <localsrc> … <dst>]<br>        [-cat [-ignoreCrc] <src> …]<br>        [-checksum <src> …]<br>        [-chgrp [-R] GROUP PATH…]<br>        [-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>        [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>        [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-count [-q] [-h] [-v] [-x] <path> …]<br>        [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>        [-createSnapshot <snapshotDir> [<snapshotName>]]<br>        [-deleteSnapshot <snapshotDir> <snapshotName>]<br>        [-df [-h] [<path> …]]<br>        [-du [-s] [-h] [-x] <path> …]<br>        [-expunge]<br>        [-find <path> … <expression> …]<br>        [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-getfacl [-R] <path>]<br>        [-getfattr [-R] {-n name | -d} [-e en] <path>]<br>        [-getmerge [-nl] <src> <localdst>]<br>        [-help [cmd …]]<br>        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> …]]<br>        [-mkdir [-p] <path> …]<br>        [-moveFromLocal <localsrc> … <dst>]<br>        [-moveToLocal <src> <localdst>]<br>        [-mv <src> … <dst>]<br>        [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-renameSnapshot <snapshotDir> <oldName> <newName>]<br>        [-rm [-f] [-r|-R] [-skipTrash] <src> …]<br>        [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[–set <acl_spec> <path>]]<br>        [-setfattr {-n name [-v value] | -x name} <path>]<br>        [-setrep [-R] [-w] <rep> <path> …]<br>        [-stat [format] <path> …]<br>        [-tail [-f] <file>]<br>        [-test -[defsz] <path>]<br>        [-text [-ignoreCrc] <src> …]<br>        [-touchz <path> …]<br>        [-usage [cmd …]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><p>检查支持压缩格式</p><p>[hadoop@warehouse001 bin]$ hadoop checknative<br>2018/11/29 18:31:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Native library checking:<br>hadoop:  false<br>zlib:    false<br>snappy:  false<br>lz4:     false<br>bzip2:   false<br>openssl: false<br>2018/11/29 18:31:58 INFO util.ExitUtil: Exiting with status 1</p><p>编译： <a href="https://blog.csdn.net/u010452388/article/details/99691421">https://blog.csdn.net/u010452388/article/details/99691421</a></p><p>尝试编译,编译后有更好的兼容性,二次开发</p><p>classpath</p><p>当前生效的目录,java运行的环境</p><p>[hadoop@warehouse001 bin]$ hadoop classpath<br>/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/</em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/lib/<em>:/home/hadoop/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/</em>:/home/hadoop/app/hadoop/contrib/capacity-scheduler/*.jar</p><p><a href="http://cn.voidcc.com/question/p-tenieuea-bex.html">http://cn.voidcc.com/question/p-tenieuea-bex.html</a></p><h5 id="3-hdfs"><a href="#3-hdfs" class="headerlink" title="3.hdfs"></a>3.hdfs</h5><p>hadoop fs=hdfs dfs</p><p>[hadoop@warehouse001 bin]$ cat hadoop</p><p># the core commands<br>    if [ “$COMMAND” = “fs” ] ; then<br>      CLASS=org.apache.hadoop.fs.FsShell</p><p>[hadoop@warehouse001 bin]$ cat hdfs</p><p>elif [ “$COMMAND” = “dfs” ] ; then<br>  CLASS=org.apache.hadoop.fs.FsShell</p><p>底层同样调用org.apache.hadoop.fs.FsShell</p><h5 id="3-1-dfs命令-使用较多的参数"><a href="#3-1-dfs命令-使用较多的参数" class="headerlink" title="3.1 dfs命令:(*使用较多的参数)"></a>3.1 dfs命令:(*使用较多的参数)</h5><p>[hadoop@warehouse001 bin]$ hdfs dfs<br>Usage: hadoop fs [generic options]<br>        [-appendToFile <localsrc> … <dst>]<br>      *  [-cat [-ignoreCrc] <src> …]<br>        [-checksum <src> …]<br>      *  [-chgrp [-R] GROUP PATH…]<br>      *[-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>      *  [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>      * [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]    ==&gt;put<br>      *[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]    ==&gt;get<br>        [-count [-q] [-h] [-v] [-x] <path> …]<br>      *  [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>        [-createSnapshot <snapshotDir> [<snapshotName>]]<br>        [-deleteSnapshot <snapshotDir> <snapshotName>]<br>        [-df [-h] [<path> …]]<br>      *  [-du [-s] [-h] [-x] <path> …]<br>        [-expunge]<br>      *  [-find <path> … <expression> …]<br>      *  [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>        [-getfacl [-R] <path>]<br>        [-getfattr [-R] {-n name | -d} [-e en] <path>]<br>        [-getmerge [-nl] <src> <localdst>]<br>      *  [-help [cmd …]]<br>      *  [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> …]]<br>      *  [-mkdir [-p] <path> …]<br>        [-moveFromLocal <localsrc> … <dst>]<br>        [-moveToLocal <src> <localdst>]<br>      * [-mv <src> … <dst>]    【生产不建议使用移动，原因是移动过程中假如有问题，会导致数据不全。建议使用cp到目标端，验证通过再删除源端】<br>      *  [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>        [-renameSnapshot <snapshotDir> <oldName> <newName>]<br>      * [-rm [-f] [-r|-R] [-skipTrash] <src> …]    【不建议使用skipTrash参数，会跳过回收站直接删除】<br>      * [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[–set <acl_spec> <path>]]<br>        [-setfattr {-n name [-v value] | -x name} <path>]<br>        [-setrep [-R] [-w] <rep> <path> …]<br>        [-stat [format] <path> …]<br>        [-tail [-f] <file>]<br>        [-test -[defsz] <path>]<br>        [-text [-ignoreCrc] <src> …]<br>        [-touchz <path> …]<br>        [-usage [cmd …]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><h5 id="3-2-dfsadmin"><a href="#3-2-dfsadmin" class="headerlink" title="3.2 dfsadmin"></a>3.2 dfsadmin</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin<br>Usage: hdfs dfsadmin<br>Note: Administrative commands can only be run as the HDFS superuser.<br>        [-report [-live] [-dead] [-decommissioning]]    【可以检查节点状况】<br>        [-safemode &lt;enter | leave | get | wait&gt;]    【安全模式】<br>        [-saveNamespace]<br>        [-rollEdits]<br>        [-restoreFailedStorage true|false|check]<br>        [-refreshNodes]<br>        [-setQuota <quota> <dirname>…<dirname>]<br>        [-clrQuota <dirname>…<dirname>]<br>        [-setSpaceQuota <quota> <dirname>…<dirname>]<br>        [-clrSpaceQuota <dirname>…<dirname>]<br>        [-finalizeUpgrade]<br>        [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]<br>        [-refreshServiceAcl]<br>        [-refreshUserToGroupsMappings]<br>        [-refreshSuperUserGroupsConfiguration]<br>        [-refreshCallQueue]<br>        [-refresh <a href="host:ipc_port">host:ipc_port</a> <key> [arg1..argn]<br>        [-reconfig &lt;datanode|…&gt; <a href="host:ipc_port">host:ipc_port</a> &lt;start|status|properties&gt;]<br>        [-printTopology]<br>        [-refreshNamenodes datanode_host:ipc_port]<br>        [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]<br>        [-setBalancerBandwidth <bandwidth in bytes per second>]<br>        [-fetchImage <local directory>]<br>        [-allowSnapshot <snapshotDir>]<br>        [-disallowSnapshot <snapshotDir>]<br>        [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]    【shutdown节点】<br>        [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]<br>        [-metasave filename]<br>        [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]<br>        [-listOpenFiles [-blockingDecommission] [-path <path>]]<br>        [-help [cmd]]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><hr><h5 id="3-3-dfsadmin"><a href="#3-3-dfsadmin" class="headerlink" title="3.3 dfsadmin"></a>3.3 dfsadmin</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -report<br>2018/11/29 19:03:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Configured Capacity: 42139451392 (39.25 GB)<br>Present Capacity: 26043494400 (24.25 GB)<br>DFS Remaining: 26043150336 (24.25 GB)<br>DFS Used: 344064 (336 KB)<br>DFS Used%: 0.00%<br>Under replicated blocks: 0<br>Blocks with corrupt replicas: 0<br>Missing blocks: 0<br>Missing blocks (with replication factor 1): 0</p><hr><p>Live datanodes (1):</p><p>Name: 172.23.75.57:50010 (warehouse001)<br>Hostname: warehouse001<br>Decommission Status : Normal<br>Configured Capacity: 42139451392 (39.25 GB)<br>DFS Used: 344064 (336 KB)<br>Non DFS Used: 13931802624 (12.98 GB)<br>DFS Remaining: 26043150336 (24.25 GB)<br>DFS Used%: 0.00%<br>DFS Remaining%: 61.80%<br>Configured Cache Capacity: 0 (0 B)<br>Cache Used: 0 (0 B)<br>Cache Remaining: 0 (0 B)<br>Cache Used%: 100.00%<br>Cache Remaining%: 0.00%<br>Xceivers: 1<br>Last contact: Sun Nov 29 19:03:57 CST 2018</p><hr><p>[hadoop@warehouse001 bin]$ hdfs haadmin<br>Usage: DFSHAAdmin [-ns <nameserviceId>]<br>    [-transitionToActive <serviceId> [–forceactive]]<br>    [-transitionToStandby <serviceId>]<br>    [-failover [–forcefence] [–forceactive] <serviceId> <serviceId>]<br>    [-getServiceState <serviceId>]<br>    [-checkHealth <serviceId>]<br>    [-help <command>]</p><p>Generic options supported are<br>-conf <configuration file>     specify an application configuration file<br>-D &lt;property=value&gt;            use value for given property<br>-fs &lt;local|namenode:port&gt;      specify a namenode<br>-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster<br>-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.<br>-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.</p><p>The general command line syntax is<br>bin/hadoop command [genericOptions] [commandOptions]</p><hr><h5 id="3-4-fsck"><a href="#3-4-fsck" class="headerlink" title="3.4 fsck"></a>3.4 fsck</h5><p>[hadoop@warehouse001 bin]$ hdfs fsck /    【健康检查】<br>2018/11/29 19:07:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Connecting to namenode via <a href="http://warehouse001:50070/fsck?ugi=hadoop&amp;path=/">http://warehouse001:50070/fsck?ugi=hadoop&amp;path=%2F</a><br>FSCK started by hadoop (auth:SIMPLE) from /172.23.75.57 for path / at Sun Nov 29 19:07:50 CST 2020<br>…………….Status: HEALTHY<br> Total size:    202382 B<br> Total dirs:    16<br> Total files:   16<br> Total symlinks:                0<br> Total blocks (validated):      14 (avg. block size 14455 B)<br> Minimally replicated blocks:   14 (100.0 %)<br> Over-replicated blocks:        0 (0.0 %)<br> Under-replicated blocks:       0 (0.0 %)<br> Mis-replicated blocks:         0 (0.0 %)<br> Default replication factor:    1<br> Average block replication:     1.0<br> Corrupt blocks:                0<br> Missing replicas:              0 (0.0 %)<br> Number of data-nodes:          1<br> Number of racks:               1<br>FSCK ended at Sun Nov 29 19:07:50 CST 2020 in 3 milliseconds</p><p>The filesystem under path ‘/‘ is HEALTHY</p><h4 id="4-安全模式"><a href="#4-安全模式" class="headerlink" title="4.安全模式"></a>4.安全模式</h4><h5 id="4-1-检查安全模式状态"><a href="#4-1-检查安全模式状态" class="headerlink" title="4.1 检查安全模式状态:"></a>4.1 检查安全模式状态:</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -safemode get<br>2018/11/29 19:12:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Safe mode is OFF</p><h5 id="4-2-进入安全模式"><a href="#4-2-进入安全模式" class="headerlink" title="4.2 进入安全模式"></a>4.2 进入安全模式</h5><p>[hadoop@warehouse001 bin]$ hdfs dfsadmin -safemode enter<br>2018/11/29 19:13:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Safe mode is ON</p><h5 id="4-3-安全模式下是否可以读写"><a href="#4-3-安全模式下是否可以读写" class="headerlink" title="4.3 安全模式下是否可以读写"></a>4.3 安全模式下是否可以读写</h5><p>[hadoop@warehouse001 bin]$ echo 123 &gt; 3.log<br>[hadoop@warehouse001 bin]$ cat 3.log<br>123<br>[hadoop@warehouse001 bin]$ hdfs dfs -put 3.log /<br>2018/11/29 19:14:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>put: Cannot create file/3.log.<em>COPYING</em>. Name node is in safe mode.</p><hr><p>[hadoop@warehouse001 bin]$ hdfs dfs -ls /<br>2018/11/29 19:15:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Found 3 items<br>drwx——   - hadoop supergroup          0 2020-11-24 23:06 /tmp<br>drwxr-xr-x   - hadoop supergroup          0 2020-11-22 16:02 /user<br>drwxr-xr-x   - hadoop supergroup          0 2020-11-24 23:06 /wordcount</p><p>结论:安全模式下可读,但不可写</p><p>安全模式,未来必然hdfs查看日志出现安全模式的英文单词,必然说明你的hdfs集群的有问题的,相当于处于一个保护模式.</p><p>一般需要你尝试手动执行命令离开安全模式(有可能会失败,根据问题去解决问题)</p><h5 id="4-4-维护操作-主动进入安全模式-维护操作"><a href="#4-4-维护操作-主动进入安全模式-维护操作" class="headerlink" title="4.4 维护操作(主动进入安全模式,维护操作)"></a>4.4 维护操作(主动进入安全模式,维护操作)</h5><p>保证这个时间段hdfs不会有新的数据写入,注意通知相关部门.</p><h4 id="5-回收站"><a href="#5-回收站" class="headerlink" title="5.回收站"></a>5.回收站</h4><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml</a></p><h5 id="5-1-启用回收站"><a href="#5-1-启用回收站" class="headerlink" title="5.1 启用回收站"></a>5.1 启用回收站</h5><p>core-default.xml加入如下配置:(单位是分钟)</p><pre class=" language-xml"><code class="language-xml">   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10080<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001 sbin]$ hdfs dfs -rm /wordcount/input/1.log<br>2018/11/29 19:35:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>2018/11/29 19:35:57 INFO fs.TrashPolicyDefault: Moved: ‘hdfs://warehouse001:9000/wordcount/input/1.log’ to trash at: hdfs://warehouse001:9000/user/hadoop/.Trash/Current/wordcount/input/1.log</p><h5 id="5-2-回收站位置"><a href="#5-2-回收站位置" class="headerlink" title="5.2 回收站位置"></a>5.2 回收站位置</h5><p>hdfs://warehouse001:9000/user/hadoop/.Trash/Current/wordcount/input/1.log</p><p>生产必须要开启回收站,且回收站默认时间尽量长点.</p><p>涉及删除不要使用-skipTrash,以防万一.</p><p>恢复文件直接mv就可以了.</p><h4 id="6-各个节点平衡"><a href="#6-各个节点平衡" class="headerlink" title="6.各个节点平衡"></a>6.各个节点平衡</h4><p>开启节点平衡</p><p>[hadoop@warehouse001 sbin]$ ./start-balancer.sh<br>starting balancer, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.out<br>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</p><hr><p>[hadoop@warehouse001 sbin]$ cat  /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.out<br>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved<br>The cluster is balanced. Exiting…<br>Nov 29, 2020 7:41:28 PM           0                  0 B                 0 B               -1 B<br>Nov 29, 2020 7:41:28 PM  Balancing took 1.47 seconds<br>f) unlimited<br>pending signals                 (-i) 30128<br>max locked memory       (kbytes, -l) 64<br>max memory size         (kbytes, -m) unlimited<br>open files                      (-n) 65535<br>pipe size            (512 bytes, -p) 8<br>POSIX message queues     (bytes, -q) 819200<br>real-time priority              (-r) 0<br>stack size              (kbytes, -s) 8192<br>cpu time               (seconds, -t) unlimited<br>max user processes              (-u) 4096<br>virtual memory          (kbytes, -v) unlimited<br>file locks                      (-x) unlimited</p><hr><p>[hadoop@warehouse001 sbin]$ cat  /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-balancer-warehouse001.log<br>2020-11-29 19:41:27,417 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs://warehouse001:9000]<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, run during upgrade = false]<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []<br>2020-11-29 19:41:27,421 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []<br>2020-11-29 19:41:27,504 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)<br>2020-11-29 19:41:28,311 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)<br>2020-11-29 19:41:28,315 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)<br>2018-11-29 19:41:28,331 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.23.75.57:50010<br>2020-11-29 19:41:28,332 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 over-utilized: []<br>2020-11-29 19:41:28,332 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 underutilized: []</p><hr><p>threshold = 10.0【阈值】    </p><p>各个节点的使用率-平均磁盘使用率&lt;10%</p><p>生产上可以写个定时脚本,在业务低谷时定时执行.</p><p>dfs.datanode.balance.bandwidthPerSec             10m    【平衡带宽调节参数】</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>如何查看使用率:web,report等等</p><h4 id="7-单个节点多块磁盘平衡"><a href="#7-单个节点多块磁盘平衡" class="headerlink" title="7.单个节点多块磁盘平衡"></a>7.单个节点多块磁盘平衡</h4><p>磁盘挂载目录:</p><table><thead><tr><th>dfs.datanode.data.dir</th><th>file://${hadoop.tmp.dir}/dfs/data</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/data01/dfs/dn,/data02/dfs/dn,/data03/dfs/dn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>#具体看磁盘挂载路径</code></pre><p>hdfs-default.xml配置节点信息</p><p>Apache hadoop2.x 不支持    dfs.disk.balancer.enabled 搜索不到<br><a href="https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>Apache hadoop3.x 支持           dfs.disk.balancer.enabled 搜索到 是true<br><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>CDH    hadoop2.x 支持           dfs.disk.balancer.enabled 搜索到 是false<br><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>如何去执行呢?<br>文档：<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><pre class=" language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.disk.balancer.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001 hadoop]$ hdfs diskbalancer -plan warehouse001<br>20/11/28 22:37:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>20/11/28 22:37:02 INFO planner.GreedyPlanner: Starting plan for Node : warehouse001:50020<br>20/11/28 22:37:02 INFO planner.GreedyPlanner: Compute Plan for Node : warehouse001:50020 took 1 ms<br>20/11/28 22:37:03 INFO command.Command: No plan generated. DiskBalancing not needed for node: warehouse001 threshold used: 10.0</p><p>hdfs diskbalancer -execute warehouse001.plan.json 执行<br>hdfs diskbalancer -query warehouse001 </p><h4 id="遇到问题的解决方式"><a href="#遇到问题的解决方式" class="headerlink" title="遇到问题的解决方式:"></a>遇到问题的解决方式:</h4><p>先自己分析,必须找到log,找到错误</p><p>百度谷歌搜素</p><p>问老师,同事,群友</p><p>apache issue</p><p>源代码导入IDEA进行debug</p><h4 id="日志的查找方法"><a href="#日志的查找方法" class="headerlink" title="日志的查找方法:"></a>日志的查找方法:</h4><p>以mysql为例:</p><p>配置文件 my.conf    data/hostname.err文件</p><p>当前目录的log文件夹</p><p>/var/log</p><p>ps -ef 查看进程描述    如:–log-error=/usr/local/mysql/data/hostname.err</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop03之hdfs架构</title>
      <link href="2018/11/24/2020-11-25-quannnxu-hadoop3/"/>
      <url>2018/11/24/2020-11-25-quannnxu-hadoop3/</url>
      
        <content type="html"><![CDATA[<h4 id="1-HDFS主从架构"><a href="#1-HDFS主从架构" class="headerlink" title="1.HDFS主从架构"></a>1.HDFS主从架构</h4><h5 id="namenode：nn名称节点"><a href="#namenode：nn名称节点" class="headerlink" title="namenode：nn名称节点"></a>namenode：nn名称节点</h5><p>a.文件的名称</p><p>b.文件的目录结构</p><p>c.文件的属性    权限    副本数    创建时间</p><p>[hadoop@warehouse001 ~]$ hdfs dfs -ls /<br>20/11/25 21:17:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Found 3 items<br>drwx——   - hadoop supergroup          0 2018-11-24 23:06 /tmp<br>drwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /user<br>drwxr-xr-x   - hadoop supergroup          0 2018-11-24 23:06 /wordcount</p><p>d.一个文件被对应切割那些数据块(包含副本数的块) ==&gt;对应分布在哪些datenode</p><p>blockmap 块映射  nn是不会持久化内存这种映射关系</p><p>是通过集群的启动和运行时,dn定期汇报blockreport给nn,然后再内存中动态维护这种映射关系</p><p>作用:</p><p>管理文件系统的命名空间,启示就是维护文件系统树的文件和文件夹</p><p>镜像文件    fsimage</p><p>编辑日志文件    editlogs</p><p>[hadoop@warehouse001 current]$ pwd<br>/home/hadoop/tmp/dfs/name/current</p><p>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 20:06 edits_0000000000000000395-0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 21:06 edits_0000000000000000397-0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop 1048576 Nov 26 21:06 edits_inprogress_0000000000000000399<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 20:06 fsimage_0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 20:06 fsimage_0000000000000000396.md5<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 21:06 fsimage_0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 21:06 fsimage_0000000000000000398.md5</p><h5 id="secondary-namenode-sn-第二名称节点"><a href="#secondary-namenode-sn-第二名称节点" class="headerlink" title="secondary namenode:sn    第二名称节点"></a>secondary namenode:sn    第二名称节点</h5><p>a.fsimage editlog文件拿过来合并  备份 推送给nn</p><p>[hadoop@warehouse001 current]$ pwd<br>/home/hadoop/tmp/dfs/namesecondary/current</p><p>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 20:06 edits_0000000000000000395-0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      42 Nov 26 21:06 edits_0000000000000000397-0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 20:06 fsimage_0000000000000000396<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 20:06 fsimage_0000000000000000396.md5<br>-rw-rw-r– 1 hadoop hadoop    2599 Nov 26 21:06 fsimage_0000000000000000398<br>-rw-rw-r– 1 hadoop hadoop      62 Nov 26 21:06 fsimage_0000000000000000398.md5<br>-rw-rw-r– 1 hadoop hadoop     204 Nov 26 21:06 VERSION</p><p>将nn的 fsimage_0000000000000000396</p><p>edits_0000000000000000397-0000000000000000398    ==&gt;检测点动作 checkpoint合并 fsimage_0000000000000000398    将398推送给nn</p><p>而新的读写记录则在 edits_inprogress_0000000000000000399编辑日志里</p><p>dfs.namenode.checkpoint.period  3600<br>dfs.namenode.checkpoint.txns    1000000</p><p>早期为了解决nn是单点的，单点故障，增加一个snn，1小时的checkpoint<br>虽然能够减轻单点故障的带来的数据丢失风险，但是生产上不允许使用snn</p><p>11:00 checkpoint<br>11:30  数据一直在写 突然nn硬盘挂了  无法恢复<br>拿snn节点的最新的fsimage，那么只能恢复11点的数据</p><p>在生产上是不允许snn，是使用HA 高可靠，是通过配置另外一个实时的备份nn节点，<br>随时等待老大active nn 挂掉，然后成为老大</p><h5 id="datanode：-数据节点-dn"><a href="#datanode：-数据节点-dn" class="headerlink" title="datanode： 数据节点 dn"></a>datanode： 数据节点 dn</h5><p>a.存储数据块 和 数据块的校验和</p><p>[hadoop@warehouse001 subdir0]$ ll<br>-rw-rw-r– 1 hadoop hadoop     58 Nov 24 23:07 blk_1073741843<br>-rw-rw-r– 1 hadoop hadoop     11 Nov 24 23:07 blk_1073741843_1019.meta<br>-rw-rw-r– 1 hadoop hadoop    349 Nov 24 23:07 blk_1073741844<br>-rw-rw-r– 1 hadoop hadoop     11 Nov 24 23:07 blk_1073741844_1020.meta<br>-rw-rw-r– 1 hadoop hadoop  33585 Nov 24 23:07 blk_1073741845<br>-rw-rw-r– 1 hadoop hadoop    271 Nov 24 23:07 blk_1073741845_1021.meta<br>-rw-rw-r– 1 hadoop hadoop 141109 Nov 24 23:07 blk_1073741846<br>-rw-rw-r– 1 hadoop hadoop   1111 Nov 24 23:07 blk_1073741846_1022.meta<br>[hadoop@warehouse001 subdir0]$ pwd<br>/home/hadoop/tmp/dfs/data/current/BP-1844033615-172.23.75.57-1606031076865/current/finalized/subdir0/subdir0</p><p>b.每隔一定的时间去发送blockreport<br>dfs.blockreport.intervalMsec          21600000        ==&gt;6h<br>dfs.datanode.directoryscan.interval   21600         ==&gt;6h </p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>补充:<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p><h4 id="2-HDFS写流程-面试"><a href="#2-HDFS写流程-面试" class="headerlink" title="2.HDFS写流程  面试"></a>2.HDFS写流程  面试</h4><p>对用户是无感知的</p><h5 id="2-1-HDFS-Client调用FileSystem-create-filePath-方法，去和NN进行【RPC】通信。"><a href="#2-1-HDFS-Client调用FileSystem-create-filePath-方法，去和NN进行【RPC】通信。" class="headerlink" title="2.1 HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信。"></a>2.1 HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信。</h5><p>NN会去check这个文件是否存在，是否有权限创建这个文件。<br>假如都可以，就创建一个新的文件，但是这时没有数据，是不关联任何block的。<br>NN根据文件的大小，根据块大小 副本数，计算要上传多少的块和对应哪些DN节点上。<br>最终这个信息返回给客户端【FSDataOutputStream】对象</p><h5 id="2-2Client-调用客户端【FSDataOutputStream】对象的write方法，"><a href="#2-2Client-调用客户端【FSDataOutputStream】对象的write方法，" class="headerlink" title="2.2Client 调用客户端【FSDataOutputStream】对象的write方法，"></a>2.2Client 调用客户端【FSDataOutputStream】对象的write方法，</h5><p>根据【副本放置策略】，将第一个块的第一个副本写到DN1，写完复制到DN2，写完再复制到DN3.<br>当第三个副本写完，就返回一个ack package确认包给DN2,DN2接收到ack 加上自己写完，<br>发送ack给DN1，DN1接收到ack加上自己写完，就发送ack给客户端【FSDataOutputStream】对象，<br>告诉它第一个块三副本写完了。<br>以此类推。</p><h5 id="2-3-当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，"><a href="#2-3-当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，" class="headerlink" title="2.3.当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，"></a>2.3.当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，</h5><p>关闭输出流。再次调用FileSystem.complete方法 ，告诉nn文件写成功。</p><p>伪分布式 1台dn，副本数参数必须设置是1吗？<br>设置2 也可以写，显示丢失一个副本</p><p>生产上分布式  3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>可以的 </p><p>生产上分布式  &gt;3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>肯定写</p><p><img src="https://i.loli.net/2020/11/26/xPDC61rl8XesfKt.png"></p><h4 id="3-HDFS读流程-面试"><a href="#3-HDFS读流程-面试" class="headerlink" title="3.HDFS读流程  面试"></a>3.HDFS读流程  面试</h4><h5 id="3-1Client调用FileSystem的open-filePath-，"><a href="#3-1Client调用FileSystem的open-filePath-，" class="headerlink" title="3.1Client调用FileSystem的open(filePath)，"></a>3.1Client调用FileSystem的open(filePath)，</h5><p>与NN进行【rpc】通信，返回该文件的部分或者全部的block列表<br>也就是返回【FSDataIntputStream】对象</p><h5 id="3-2Client调度【FSDataIntputStream】对象的read方法，"><a href="#3-2Client调度【FSDataIntputStream】对象的read方法，" class="headerlink" title="3.2Client调度【FSDataIntputStream】对象的read方法，"></a>3.2Client调度【FSDataIntputStream】对象的read方法，</h5><p>与第一个块的最近的DN的进行读取，读取完成后，会check，假如ok就关闭与DN通信。<br>假如不ok，就会记录块+DN的信息，下次就不从这个节点读取。那么从第二个节点读取。</p><p>然后与第二个块的最近的DN的进行读取，以此类推。<br>假如当block的列表全部读取完成，文件还没结束，就调用FileSystem从NN获取下一批次的block列表。</p><h5 id="3-3-Client调用【FSDataIntputStream】对象的close方法，关闭输入流。"><a href="#3-3-Client调用【FSDataIntputStream】对象的close方法，关闭输入流。" class="headerlink" title="3.3.Client调用【FSDataIntputStream】对象的close方法，关闭输入流。"></a>3.3.Client调用【FSDataIntputStream】对象的close方法，关闭输入流。</h5><p><img src="https://i.loli.net/2020/11/26/xPDC61rl8XesfKt.png"></p><h4 id="4-副本放置策略-不光光面试需要，生产也需要"><a href="#4-副本放置策略-不光光面试需要，生产也需要" class="headerlink" title="4.副本放置策略  不光光面试需要，生产也需要"></a>4.副本放置策略  不光光面试需要，生产也需要</h4><p><a href="https://www.bilibili.com/video/BV1eE411p7un">https://www.bilibili.com/video/BV1eE411p7un</a></p><p>生产上读写操作  尽量选择DN节点操作<br>第一个副本：<br>放置在上传的DN节点上，就近原则，节省IO(网络IO)<br>假如非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点。</p><p>第二个副本:<br>放置在第一个副本的不同机架上的某个节点</p><p>第三个副本:<br>与第二个副本放置同一个机架的不同节点上。</p><p>如果副本数设置更多，随机放。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop02之yarn部署及简析</title>
      <link href="2018/11/15/2020-11-22-quannnxu-hadoop2/"/>
      <url>2018/11/15/2020-11-22-quannnxu-hadoop2/</url>
      
        <content type="html"><![CDATA[<h4 id="主从架构"><a href="#主从架构" class="headerlink" title="主从架构"></a>主从架构</h4><p>ResourceManager daemon and NodeManager daemon</p><h4 id="1-yarn部署"><a href="#1-yarn部署" class="headerlink" title="1.yarn部署"></a>1.yarn部署</h4><h5 id="1-1-配置文件"><a href="#1-1-配置文件" class="headerlink" title="1.1 配置文件"></a>1.1 配置文件</h5><p>[hadoop@warehouse001hadoop]$ cp mapred-site.xml.template mapred-site.xml<br>[hadoop@warehouse001hadoop]$ vi mapred-site.xml</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>[hadoop@warehouse001hadoop]$ vi yarn-site.xml </p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><p>open: <a href="http://114.67.101.143:8088/cluster%E5%AE%98%E7%BD%91">http://114.67.101.143:8088/cluster官网</a></p><p>启动:</p><p>[hadoop@warehouse001 hadoop]$ sbin/start-yarn.sh </p><p>查看进程:</p><p>[hadoop@warehouse001 hadoop]$ jps<br>12496 Jps<br>30954 ResourceManager<br>29050 NameNode<br>29179 DataNode<br>29340 SecondaryNameNode<br>31182 NodeManager</p><h4 id="2-案例"><a href="#2-案例" class="headerlink" title="2.案例"></a>2.案例</h4><p>wordcount</p><pre class=" language-linux"><code class="language-linux">#配置环境变量[hadoop@warehouse001 hadoop]$ cd [hadoop@warehouse001 ~]$ which hadoop/usr/bin/which: no hadoop in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)[hadoop@warehouse001 ~]$ vi .bashrc # .bashrc# Source global definitionsif [ -f /etc/bashrc ]; then        . /etc/bashrcfi# Uncomment the following line if you don't like systemctl's auto-paging feature:# export SYSTEMD_PAGER=export HADOOP_HOME=/home/hadoop/app/hadoopexport PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH# User specific aliases and functions[hadoop@warehouse001 ~]$ source .bashrc [hadoop@warehouse001 ~]$ which hadoop~/app/hadoop/bin/hadoop#造数据[hadoop@warehouse001 ~]$ cd data/[hadoop@warehouse001 data]$ mkdir input[hadoop@warehouse001 data]$ cd input/[hadoop@warehouse001 input]$ vi 1.loga b cwww.quanxu.comxiaominglihua1112a bc#上传hdfs[hadoop@warehouse001 input]$ hdfs dfs -ls /20/11/24 22:42:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 1 itemsdrwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /user[hadoop@warehouse001 input]$ hdfs dfs -mkdir -p /wordcount/input20/11/24 22:43:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@warehouse001 input]$ hdfs dfs -ls /20/11/24 22:43:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 itemsdrwxr-xr-x   - hadoop supergroup          0 2018-11-22 16:02 /userdrwxr-xr-x   - hadoop supergroup          0 2018-11-24 22:43 /wordcountdrwxr-xr-x   - hadoop supergroup          0 2018-11-24 22:43 /wordcount/input[hadoop@warehouse001 input]$ hdfs dfs -put 1.log /wordcount/input20/11/24 22:45:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@warehouse001 hadoop]$ cd src/[hadoop@warehouse001 src]$ lltotal 200drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 build-rw-r--r--  1 hadoop hadoop 12096 Jun  3  2018 BUILDING.txtdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 dev-supportdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-assembliesdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-build-toolsdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-clientdrwxr-xr-x 10 hadoop hadoop  4096 Jun  3  2018 hadoop-common-projectdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-distdrwxr-xr-x  6 hadoop hadoop  4096 Jun  3  2018 hadoop-hdfs-projectdrwxr-xr-x 10 hadoop hadoop  4096 Jun  3  2018 hadoop-mapreduce1-projectdrwxr-xr-x  9 hadoop hadoop  4096 Jun  3  2018 hadoop-mapreduce-projectdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-maven-pluginsdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-miniclusterdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-projectdrwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 hadoop-project-distdrwxr-xr-x 18 hadoop hadoop  4096 Jun  3  2018 hadoop-toolsdrwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 hadoop-yarn-project-rw-r--r--  1 hadoop hadoop 85063 Jun  3  2018 LICENSE.txt-rw-r--r--  1 hadoop hadoop 14978 Jun  3  2018 NOTICE.txt-rw-r--r--  1 hadoop hadoop 19039 Jun  3  2018 pom.xml-rw-r--r--  1 hadoop hadoop  1366 Jun  3  2018 README.txt[hadoop@warehouse001 src]$ cd hadoop-mapreduce-projecthadoop@warehouse001 src]$ cd hadoop-mapreduce-project[hadoop@warehouse001 hadoop-mapreduce-project]$ lltotal 328drwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 bin-rw-r--r--  1 hadoop hadoop   2018 Jun  3  2018 CHANGES.MAPREDUCE-2841.txt-rw-r--r--  1 hadoop hadoop 293811 Jun  3  2018 CHANGES.txtdrwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 confdrwxr-xr-x  2 hadoop hadoop   4096 Jun  3  2018 dev-supportdrwxr-xr-x 10 hadoop hadoop   4096 Jun  3  2018 hadoop-mapreduce-clientdrwxr-xr-x  4 hadoop hadoop   4096 Jun  3  2018 hadoop-mapreduce-examplesdrwxr-xr-x  3 hadoop hadoop   4096 Jun  3  2018 lib-rw-r--r--  1 hadoop hadoop  11091 Jun  3  2018 pom.xml[hadoop@warehouse001 hadoop-mapreduce-project]$ cd hadoop-mapreduce-examples[hadoop@warehouse001 hadoop-mapreduce-examples]$ lltotal 16drwxr-xr-x 2 hadoop hadoop 4096 Jun  3  2018 dev-support-rw-r--r-- 1 hadoop hadoop 5097 Jun  3  2018 pom.xmldrwxr-xr-x 4 hadoop hadoop 4096 Jun  3  2018 src[hadoop@warehouse001 hadoop-mapreduce-examples]$ cd src[hadoop@warehouse001 src]$ cd main[hadoop@warehouse001 main]$ cd java/[hadoop@warehouse001 java]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 org[hadoop@warehouse001 java]$ cd org/[hadoop@warehouse001 org]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 apache[hadoop@warehouse001 org]$ cd apache/[hadoop@warehouse001 apache]$ lltotal 4drwxr-xr-x 3 hadoop hadoop 4096 Jun  3  2018 hadoop[hadoop@warehouse001 apache]$ cd hadoop/[hadoop@warehouse001 hadoop]$ lltotal 4drwxr-xr-x 5 hadoop hadoop 4096 Jun  3  2018 examples[hadoop@warehouse001 hadoop]$ cd examples/[hadoop@warehouse001 examples]$ lltotal 204-rw-r--r-- 1 hadoop hadoop  2897 Jun  3  2018 AggregateWordCount.java-rw-r--r-- 1 hadoop hadoop  3016 Jun  3  2018 AggregateWordHistogram.java-rw-r--r-- 1 hadoop hadoop 21254 Jun  3  2018 BaileyBorweinPlouffe.javadrwxr-xr-x 2 hadoop hadoop  4096 Jun  3  2018 dancing-rw-r--r-- 1 hadoop hadoop 13495 Jun  3  2018 DBCountPageView.java-rw-r--r-- 1 hadoop hadoop  4301 Jun  3  2018 ExampleDriver.java-rw-r--r-- 1 hadoop hadoop  3730 Jun  3  2018 Grep.java-rw-r--r-- 1 hadoop hadoop  7033 Jun  3  2018 Join.java-rw-r--r-- 1 hadoop hadoop  8111 Jun  3  2018 MultiFileWordCount.java-rw-r--r-- 1 hadoop hadoop   853 Jun  3  2018 package.htmldrwxr-xr-x 3 hadoop hadoop  4096 Jun  3  2018 pi-rw-r--r-- 1 hadoop hadoop 12628 Jun  3  2018 QuasiMonteCarlo.java-rw-r--r-- 1 hadoop hadoop 40575 Jun  3  2018 RandomTextWriter.java-rw-r--r-- 1 hadoop hadoop 10573 Jun  3  2018 RandomWriter.java-rw-r--r-- 1 hadoop hadoop  7809 Jun  3  2018 SecondarySort.java-rw-r--r-- 1 hadoop hadoop  8167 Jun  3  2018 Sort.javadrwxr-xr-x 3 hadoop hadoop  4096 Jun  3  2018 terasort-rw-r--r-- 1 hadoop hadoop  3297 Jun  3  2018 WordCount.java-rw-r--r-- 1 hadoop hadoop  6327 Jun  3  2018 WordMean.java-rw-r--r-- 1 hadoop hadoop  7084 Jun  3  2018 WordMedian.java-rw-r--r-- 1 hadoop hadoop  7253 Jun  3  2018 WordStandardDeviation.java#下载到本地[hadoop@warehouse001 examples]$ sz WordCount.javarz?a? zmodem ′???￡  °′ Ctrl+C ???￡Transferring WordCount.java...  100%       3 KB    3 KB/s 00:00:01       0 Errors[hadoop@warehouse001 hadoop]$ find ./ -name *example*.jar./share/hadoop/mapreduce1/hadoop-examples-2.6.0-mr1-cdh5.16.2.jar./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-test-sources.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-sources.jarhadoop jar \./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \wordcount /wordcount/input /wordcount/output1 [hadoop@warehouse001 hadoop]$ hadoop jar \> ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \> wordcount /wordcount/input /wordcount/output1 20/11/24 23:06:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable20/11/24 23:06:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803220/11/24 23:06:52 INFO input.FileInputFormat: Total input paths to process : 120/11/24 23:06:52 INFO mapreduce.JobSubmitter: number of splits:1【了解split切分的规则】20/11/24 23:06:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1606044808202_000120/11/24 23:06:53 INFO impl.YarnClientImpl: Submitted application application_1606044808202_0001【分配的作业id号，web界面会显示】20/11/24 23:06:53 INFO mapreduce.Job: The url to track the job: http://warehouse001:8088/proxy/application_1606044808202_0001/20/11/24 23:06:53 INFO mapreduce.Job: Running job: job_1606044808202_000120/11/24 23:06:58 INFO mapreduce.Job: Job job_1606044808202_0001 running in uber mode : false【了解uber mode】20/11/24 23:06:58 INFO mapreduce.Job:  map 0% reduce 0%20/11/24 23:07:02 INFO mapreduce.Job:  map 100% reduce 0%20/11/24 23:07:07 INFO mapreduce.Job:  map 100% reduce 100%20/11/24 23:07:08 INFO mapreduce.Job: Job job_1606044808202_0001 completed successfully20/11/24 23:07:08 INFO mapreduce.Job: Counters: 49        File System Counters                FILE: Number of bytes read=96                FILE: Number of bytes written=286259                FILE: Number of read operations=0                FILE: Number of large read operations=0                FILE: Number of write operations=0                HDFS: Number of bytes read=160                HDFS: Number of bytes written=58                HDFS: Number of read operations=6                HDFS: Number of large read operations=0                HDFS: Number of write operations=2        Job Counters                 Launched map tasks=1                Launched reduce tasks=1【了解map、reduce个数的规则】                Data-local map tasks=1                Total time spent by all maps in occupied slots (ms)=1783                Total time spent by all reduces in occupied slots (ms)=1958                Total time spent by all map tasks (ms)=1783                Total time spent by all reduce tasks (ms)=1958                Total vcore-milliseconds taken by all map tasks=1783                Total vcore-milliseconds taken by all reduce tasks=1958                Total megabyte-milliseconds taken by all map tasks=1825792                Total megabyte-milliseconds taken by all reduce tasks=2004992        Map-Reduce Framework                Map input records=8                Map output records=11                Map output bytes=92                Map output materialized bytes=96                Input split bytes=111                Combine input records=11                Combine output records=8                Reduce input groups=8                Reduce shuffle bytes=96                Reduce input records=8                Reduce output records=8                Spilled Records=16                Shuffled Maps =1                Failed Shuffles=0                Merged Map outputs=1                GC time elapsed (ms)=89                CPU time spent (ms)=860                Physical memory (bytes) snapshot=489730048                Virtual memory (bytes) snapshot=5554565120                Total committed heap usage (bytes)=466092032        Shuffle Errors                BAD_ID=0                CONNECTION=0                IO_ERROR=0                WRONG_LENGTH=0                WRONG_MAP=0                WRONG_REDUCE=0        File Input Format Counters                 Bytes Read=49        File Output Format Counters                 Bytes Written=58</code></pre><p><img src="https://i.loli.net/2020/11/24/7s82mHTuvPDLntV.png"></p><p>如报错可进入history查看日志</p><h4 id="3-关于挖矿"><a href="#3-关于挖矿" class="headerlink" title="3.关于挖矿"></a>3.关于挖矿</h4><p><a href="https://segmentfault.com/a/1190000015264170">https://segmentfault.com/a/1190000015264170</a></p><p>[hadoop@warehouse001 hadoop]$ top</p><p>top - 23:20:34 up 8 days,  5:42,  1 user,  load average: 0.00, 0.01, 0.05<br>Tasks:  84 total,   1 running,  83 sleeping,   0 stopped,   0 zombie<br>%Cpu(s):  0.2 us,  0.2 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st<br>KiB Mem :  7734044 total,  2442532 free,  2900996 used,  2390516 buff/cache<br>KiB Swap:        0 total,        0 free,        0 used.  4533612 avail Mem </p><p>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND<br> 5931 root      20   0   42156   5532   2704 S   0.3  0.1   0:50.91 aliyun-service<br>29340 hadoop    20   0 2798988 248088  19424 S   0.3  3.2   1:12.20 java          </p><p>cpu过高，杀掉进程后，过段时间挖矿进程会重启</p><p>所以需要调整8088端口</p><p>在yarn-site.xml 新增:</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>$<span class="token entity" title="&#123;">&amp;#123;</span>yarn.resourcemanager.hostname<span class="token entity" title="&#125;">&amp;#125;</span>:7776<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>#重启</p><p>[hadoop@warehouse001 hadoop]$ sbin/stop-yarn.sh </p><p>[hadoop@warehouse001 hadoop]$ sbin/start-yarn.sh</p><p><img src="https://i.loli.net/2020/11/24/uaGfhgHEne2SVLB.png"></p><h4 id="4-jps命令"><a href="#4-jps命令" class="headerlink" title="4.jps命令"></a>4.jps命令</h4><h5 id="4-1-位置"><a href="#4-1-位置" class="headerlink" title="4.1 位置"></a>4.1 位置</h5><p>[hadoop@ruozedata001 hadoop]$ which jps<br>/usr/java/jdk1.8.0_181/bin/jps<br>[hadoop@ruozedata001 hadoop]$ </p><h5 id="4-2-使用"><a href="#4-2-使用" class="headerlink" title="4.2 使用"></a>4.2 使用</h5><p>[hadoop@ruozedata001 hadoop]$ jps<br>21828 NameNode<br>21959 DataNode<br>27577 NodeManager<br>27466 ResourceManager<br>22124 SecondaryNameNode</p><h5 id="4-3-对应的标识文件存储在哪里"><a href="#4-3-对应的标识文件存储在哪里" class="headerlink" title="4.3 对应的标识文件存储在哪里"></a>4.3 对应的标识文件存储在哪里</h5><p>[hadoop@warehouse001 hsperfdata_hadoop]$ ll<br>total 160<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 13863<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 14136<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29050<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29179<br>-rw——- 1 hadoop hadoop 32768 Nov 24 23:36 29340<br>[hadoop@warehouse001 hsperfdata_hadoop]$ pwd<br>/tmp/hsperfdata_hadoop</p><h5 id="4-4-哪个用户可以查看"><a href="#4-4-哪个用户可以查看" class="headerlink" title="4.4 哪个用户可以查看"></a>4.4 哪个用户可以查看</h5><p>[root@warehouse001 ~]# jps<br>14389 Jps<br>13863 ResourceManager<br>14136 NodeManager<br>29050 NameNode<br>29179 DataNode<br>29340 SecondaryNameNode</p><p>root和当前用户</p><h5 id="4-5-出现process-information-unavailable描述"><a href="#4-5-出现process-information-unavailable描述" class="headerlink" title="4.5 出现process information unavailable描述"></a>4.5 出现process information unavailable描述</h5><p>当看见 process information unavailable<br>不能代表进程是存在 或者不存在，要当心，尤其使用jps命令来做脚本状态检测的<br>一般使用经典的 ps -ef | grep xxx命令去查看进程是否存在，<br>这才是真正的状态检测</p><p>但是: 比如spark thriftserver +hive 会启动一个driver 进程 110，<br>默认端口号 10001。由于该程序的内存泄露或者某种bug，导致<br>进程ps是存在的，10001端口号下线了，就不能够对外提供服务。</p><p>总结: 未来做任何程序的状态检测，必须通过端口号来。</p><p>CDH root用户，jps命令查看会有很多的 process information unavailable<br>ps -ef| grep xxx 查看是正确的<br>那么想要看到正常的表述，需要切换对应的用户，<br>比如su - hdfs(有可能你切换不过去，需要/etc/passwd文件的修正)<br>再执行jps命令</p><h5 id="4-6-删除之后是否影响服务重启呢"><a href="#4-6-删除之后是否影响服务重启呢" class="headerlink" title="4.6 删除之后是否影响服务重启呢"></a>4.6 删除之后是否影响服务重启呢</h5><p>经过测试验证，不影响重启</p><h4 id="5-pid文件"><a href="#5-pid文件" class="headerlink" title="5.pid文件"></a>5.pid文件</h4><h5 id="5-1-pid文件的位置"><a href="#5-1-pid文件的位置" class="headerlink" title="5.1 pid文件的位置"></a>5.1 pid文件的位置</h5><p>/tmp </p><p>[hadoop@warehouse001 tmp]$ ll</p><p>-rw-rw-r– 1 hadoop hadoop        6 Nov 22 15:59 hadoop-hadoop-datanode.pid</p><p>[hadoop@warehouse001 sbin]$ pwd<br>/home/hadoop/app/hadoop/sbin</p><p>停止进程脚本:</p><p>hadoop-daemon.sh</p><pre class=" language-shell"><code class="language-shell">    if [ -f $pid ]; then      TARGET_PID=`cat $pid`      if kill -0 $TARGET_PID > /dev/null 2>&1; then        echo stopping $command        kill $TARGET_PID        sleep $HADOOP_STOP_TIMEOUT        if kill -0 $TARGET_PID > /dev/null 2>&1; then          echo "$command did not stop gracefully after $HADOOP_STOP_TIMEOUT seconds: killing with kill -9"          kill -9 $TARGET_PID        fi      else        echo no $command to stop      fi      rm -f $pid    else      echo no $command to stop    fi    ;;  (*)    echo $usage    exit 1    ;;</code></pre><h5 id="5-2-删除之后是否影响服务重启呢"><a href="#5-2-删除之后是否影响服务重启呢" class="headerlink" title="5.2 删除之后是否影响服务重启呢"></a>5.2 删除之后是否影响服务重启呢</h5><p>经过测试验证，影响重启</p><h5 id="5-3-如何修改位置"><a href="#5-3-如何修改位置" class="headerlink" title="5.3 如何修改位置"></a>5.3 如何修改位置</h5><p>[hadoop@ruozedata001 hadoop]$ vi hadoop-env.sh<br>export HADOOP_PID_DIR=/home/hadoop/tmp</p><p>[hadoop@warehouse001 tmp]$ jps<br>17667 NameNode<br>17798 DataNode<br>17963 SecondaryNameNode<br>18077 Jps<br>[hadoop@warehouse001 tmp]$ cat hadoop-hadoop-datanode.pid<br>17798</p><h4 id="6-块-block"><a href="#6-块-block" class="headerlink" title="6.块 block"></a>6.块 block</h4><p>dfs.blocksize   128M<br>大文件  小文件</p><p>一缸水 260ml<br>瓶子的规格 128ml   ==》   dfs.blocksize<br>260/128=2 ….4ml<br>1 128ml<br>2 128ml<br>3 4ml</p><p>一个大文件 260m<br>1 128m<br>2 128m<br>3 4m    –只占用4m不会占用128m</p><p>存储到伪分布式hdfs上是                     3个块                   实际存储260m x 1=260m<br>      集群hdfs上&gt;=3节点  多副本机制3<br>      一个块会被连他自己 复制是3份    3x3= 9个块          实际存储260m x 3=780m</p><p>10个小文件 每个小文件 10m，那么伪分布式：<br>10个块 </p><p>namenode：维护一个文件被切割哪个块，这些块被存放到哪些机器</p><p>10条元数据</p><p>10个小文件 合并 1个文件100m,     那么伪分布式：<br>1个块</p><p>1条元数据</p><p>10个小文件10m VS 1个大文件100m<br>结果： 1个大文件对 nn的存储压力较小</p><p>再举例:<br>假设1亿个小文件，每个小文件10kb, 集群3副本机制，3亿个block，3亿个元数据<br>假如1亿个小文件，合并为1kw个 100m文件 , 集群3副本机制，3kw block，3kw元数据 </p><p>nn维护 3亿个元数据 还是 3kw元数据 的压力，谁轻松？<br>3kw轻松</p><p>元数据是存储在nn进程的内存里 ，那内存是一定的 8g</p><p>所以生产上:<br>尽量规避小文件在hdfs上的存储<br>a.数据传输到hdfs之前，提前合并<br>b.数据已经到hdfs，就定时的  业务低谷期，去合并冷文件<br>          写个脚本 每一天合并<br>          11-1 合并09-30数据<br>          11-2 合并10-1数据</p><p>​          一天卡一天去处理 </p><p>关于split切分的规则</p><p><a href="https://blog.csdn.net/jinywum/article/details/81458359">https://blog.csdn.net/jinywum/article/details/81458359</a></p><p>map、reduce任务个数的规则</p><p><a href="https://blog.csdn.net/zhanglh046/article/details/78567105">https://blog.csdn.net/zhanglh046/article/details/78567105</a></p><p>什么是uber模式</p><p>Uber模式简单地可以理解成JVM重用，该模式是2.x开始引入的；以Uber模式运行MR作业，所有的Map Tasks和Reduce Tasks将会在ApplicationMaster所在的容器（container）中运行，也就是说整个MR作业运行的过程只会启动AM container，因为不需要启动mapper 和 reducer containers，所以AM不需要和远程containers通信，整个过程简单了。</p><p>不是所有的MR作业都可以启用Uber模式，如果我们的MR作业输入的数据量非常小，启动Map container或Reduce container的时间都比处理数据要长，那么这个作业就可以考虑启用Uber模式运行，一般情况下，对小作业启用Uber模式运行会得到2x-3x的性能提升。</p><p>启用uber模式的要求非常严格，代码如下：</p><p>isUber = uberEnabled &amp;&amp; smallNumMapTasks &amp;&amp; smallNumReduceTasks &amp;&amp; smallInput &amp;&amp; smallMemory &amp;&amp; smallCpu &amp;&amp; notChainJob &amp;&amp; isValidUberMaxReduces;</p><ul><li><strong>uberEnabled</strong>：其实就是 mapreduce.job.ubertask.enable 参数的值，默认情况下为 false ；也就是说默认情况不启用Uber模式；</li><li><strong>smallNumMapTasks</strong>：启用Uber模式的作业Map的个数必须小于等于 mapreduce.job.ubertask.maxmaps 参数的值，该值默认为9；也计算说，在默认情况下，如果你想启用Uber模式，作业的Map个数必须小于10；</li><li><strong>smallNumReduceTasks</strong>：同理，Uber模式的作业Reduce的个数必须小于等于mapreduce.job.ubertask.maxreduces，该值默认为1；也计算说，在默认情况下，如果你想启用Uber模式，作业的Reduce个数必须小于等于1；</li><li><strong>smallInput</strong>：不是任何作业都适合启用Uber模式的，输入数据的大小必须小于等于 mapreduce.job.ubertask.maxbytes 参数的值，默认情况是HDFS一个文件块大小；</li><li><strong>smallMemory</strong>：因为作业是在AM所在的container中运行，所以要求我们设置的Map内存（mapreduce.map.memory.mb）和Reduce内存（mapreduce.reduce.memory.mb）必须小于等于 AM所在容器内存大小设置（yarn.app.mapreduce.am.resource.mb）；</li><li><strong>smallCpu</strong>：同理，Map配置的vcores（mapreduce.map.cpu.vcores）个数和 Reduce配置的vcores（mapreduce.reduce.cpu.vcores）个数也必须小于等于AM所在容器vcores个数的设置（yarn.app.mapreduce.am.resource.cpu-vcores）；</li><li><strong>notChainJob</strong>：此外，处理数据的Map class（mapreduce.job.map.class）和Reduce class（mapreduce.job.reduce.class）必须不是 ChainMapper 或 ChainReducer 才行；</li><li><strong>isValidUberMaxReduces</strong>：目前仅当Reduce的个数小于等于1的作业才能启用Uber模式。</li></ul><p>同时满足上面八个条件才能在作业运行的时候启动Uber模式。下面是一个启用Uber模式运行的作业运行成功的日志：</p><p>File System Counters FILE: Number of bytes read=215 FILE: Number of bytes written=505 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=1200 HDFS: Number of bytes written=274907 HDFS: Number of read operations=57 HDFS: Number of large read operations=0 HDFS: Number of write operations=11 Job Counters Launched map tasks=2 Launched reduce tasks=1 Other local map tasks=2 Total time spent by all maps in occupied slots (ms)=3664 Total time spent by all reduces in occupied slots (ms)=2492 TOTAL_LAUNCHED_UBERTASKS=3 NUM_UBER_SUBMAPS=2 NUM_UBER_SUBREDUCES=1 Map-Reduce Framework Map input records=2 Map output records=8 Map output bytes=82 Map output materialized bytes=85 Input split bytes=202 Combine input records=8 Combine output records=6 Reduce input groups=5 Reduce shuffle bytes=0 Reduce input records=6 Reduce output records=5 Spilled Records=12 Shuffled Maps =0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=65 CPU time spent (ms)=1610 Physical memory (bytes) snapshot=1229729792 Virtual memory (bytes) snapshot=5839392768 Total committed heap usage (bytes)=3087532032 File Input Format Counters Bytes Read=50 File Output Format Counters Bytes Written=41</p><p>细心的同学应该会发现里面多了 TOTAL_LAUNCHED_UBERTASKS、NUM_UBER_SUBMAPS 以及 NUM_UBER_SUBREDUCES 信息，以前需要启用Map Task 或 Reduce Task运行的工作直接在AM中运行，所有出现了NUM_UBER_SUBMAPS和原来Map Task个数一样；同理，NUM_UBER_SUBREDUCES 和Reduce Task个数一样。</p><p>作业:</p><p>1.作业输出的内容</p><p>2.拓展 hdfs小文件如何合并</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop01之安装部署</title>
      <link href="2018/11/10/2020-11-21-quannnxu-hadoop1/"/>
      <url>2018/11/10/2020-11-21-quannnxu-hadoop1/</url>
      
        <content type="html"><![CDATA[<h4 id="1-谈谈大数据"><a href="#1-谈谈大数据" class="headerlink" title="1.谈谈大数据"></a>1.谈谈大数据</h4><p>存储                      hdfs  hive  hbase  kudu<br>计算                      SQL 数据处理 、scala/java、spark、flink<br>资源和计算作业的调度分配  yarn</p><h4 id="2-hadoop官网"><a href="#2-hadoop官网" class="headerlink" title="2.hadoop官网"></a>2.hadoop官网</h4><p><a href="https://hadoop.apache.org/">https://hadoop.apache.org</a></p><p>hive.apache.org</p><h4 id="3-hadoop版本选择"><a href="#3-hadoop版本选择" class="headerlink" title="3.hadoop版本选择"></a>3.hadoop版本选择</h4><p>hadoop2.x    hadoop3.x</p><h4 id="4-hadoop认知"><a href="#4-hadoop认知" class="headerlink" title="4.hadoop认知"></a>4.hadoop认知</h4><p>广义: 以apache hadoop软件为主的生态圈 包含hive sqoop hbase kafka spark flink<br>狭义: apache hadoop软件<br>        hdfs          存储<br>        mapreduce     计算<br>        yarn          资源作业的分配调度</p><p>大数据平台： 存储是第一位；存储和计算是相辅相成的</p><h4 id="5-部署前准备"><a href="#5-部署前准备" class="headerlink" title="5.部署前准备"></a>5.部署前准备</h4><p>创建用户<br>useradd hadoop<br>su - hadoop<br>mkdir sourcecode software app log lib data tmp shell</p><p>cd software/</p><p>rz上传tar包</p><p>[hadoop@warehouse001 software]$ ll<br>total 424180<br>-rw-r–r– 1 root root 434354462 Nov 22 09:42 hadoop-2.6.0-cdh5.16.2.tar.gz</p><h4 id="6-jdk部署"><a href="#6-jdk部署" class="headerlink" title="6.jdk部署"></a>6.jdk部署</h4><p>[hadoop@warehouse001 software]$ which java<br>/usr/java/jdk1.8.0_181/bin/java</p><h4 id="7-hdfs安装"><a href="#7-hdfs安装" class="headerlink" title="7.hdfs安装"></a>7.hdfs安装</h4><h5 id="7-1解压"><a href="#7-1解压" class="headerlink" title="7.1解压"></a>7.1解压</h5><p>[hadoop@warehouse001 ~]$ cd software/<br>[hadoop@warehouse001 software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</p><h5 id="7-2软连接"><a href="#7-2软连接" class="headerlink" title="7.2软连接"></a>7.2软连接</h5><p>[hadoop@warehouse001 ~]$ cd app/<br>[hadoop@warehouse001 app]$ ll<br>total 4<br>drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2018 hadoop-2.6.0-cdh5.16.2<br>[hadoop@warehouse001 app]$ ln -s hadoop-2.6.0-cdh5.16.2  hadoop<br>[hadoop@warehouse001 app]$ ll<br>total 4<br>lrwxrwxrwx  1 hadoop hadoop   22 Nov 22 09:44 hadoop -&gt; hadoop-2.6.0-cdh5.16.2<br>drwxr-xr-x 14 hadoop hadoop 4096 Jun  3  2018 hadoop-2.6.0-cdh5.16.2<br>【补充】软连接：<br>    a.版本切换，脚本应用是配置的hadoop，是无感知的 </p><p>b.小盘换大盘<br>    /       系统盘  100G  /ruozedata 80G<br>    /data01 数据盘  2T</p><p>​    ll /             看一下ruozedata文件夹权限<br>​    ll /ruozedata    看一下ruozedata文件夹内容的权限<br>​    mv      /ruozedata /data01/ruozedata<br>​    ln -s   /data01/ruozedata  /ruozedata</p><p>注意的是：<br>权限 rwx  用户用户组</p><h5 id="7-3-解读目录只关注-bin-sbin-etc"><a href="#7-3-解读目录只关注-bin-sbin-etc" class="headerlink" title="7.3 解读目录只关注 bin  sbin  etc"></a>7.3 解读目录只关注 bin  sbin  etc</h5><p>[hadoop@warehouse001 app]$ cd hadoop<br>[hadoop@warehouse001 hadoop]$ ll<br>total 152<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 bin        可执行命令<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 bin-mapreduce1<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 cloudera<br>drwxr-xr-x  6 hadoop hadoop  4096 Jun  3  2018 etc        配置文件<br>drwxr-xr-x  5 hadoop hadoop  4096 Jun  3  2018 examples<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 examples-mapreduce1<br>drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2018 include<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 lib<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 libexec<br>-rw-r–r–  1 hadoop hadoop 85063 Jun  3  2018 LICENSE.txt<br>-rw-r–r–  1 hadoop hadoop 14978 Jun  3  2018 NOTICE.txt<br>-rw-r–r–  1 hadoop hadoop  1366 Jun  3  2018 README.txt<br>drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2018 sbin        启动停止脚本<br>drwxr-xr-x  4 hadoop hadoop  4096 Jun  3  2018 share<br>drwxr-xr-x 18 hadoop hadoop  4096 Jun  3  2018 src</p><h5 id="7-4部署模式"><a href="#7-4部署模式" class="headerlink" title="7.4部署模式"></a>7.4部署模式</h5><p>Local (Standalone) Mode  本地      1台机器  1个单独的java进程 用于debug<br>Pseudo-Distributed Mode  伪分布式  1台机器  多个java进程<br>Fully-Distributed Mode   集群      多台机器 多个java进程 </p><h5 id="7-5修改hadoop-env-sh文件，显性java家目录"><a href="#7-5修改hadoop-env-sh文件，显性java家目录" class="headerlink" title="7.5修改hadoop-env.sh文件，显性java家目录"></a>7.5修改hadoop-env.sh文件，显性java家目录</h5><p>export JAVA_HOME=/usr/java/jdk1.8.0_181</p><h5 id="7-6配置hadoop用户的ssh信任关系"><a href="#7-6配置hadoop用户的ssh信任关系" class="headerlink" title="7.6配置hadoop用户的ssh信任关系"></a>7.6配置hadoop用户的ssh信任关系</h5><p>删除已存在的.ssh，建议生产上mv移走重命名</p><p>[hadoop@warehouse001 ~]$ ssh-keygen<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):<br>Created directory ‘/home/hadoop/.ssh’.<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/hadoop/.ssh/id_rsa.<br>Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>SHA256:dBcBU5BHmP5ZqYc1cy8Bsd1bcHJbgQJ18aESVXW2bUs hadoop@warehouse001<br>The key’s randomart image is:<br>+—[RSA 2048]—-+<br>|         .*@B**+X|<br>|          +o==oBB|<br>|        …o+o.E=|<br>|       . …. O.=|<br>|        S  . * B.|<br>|            = o .|<br>|             . . |<br>|                 |<br>|                 |<br>+—-[SHA256]—–+<br>[hadoop@warehouse001 ~]$<br>[hadoop@warehouse001 ~]$ cd .ssh<br>[hadoop@warehouse001 .ssh]$ ll<br>total 8<br>-rw——- 1 hadoop hadoop 1679 Nov 22 10:29 id_rsa<br>-rw-r–r– 1 hadoop hadoop  401 Nov 22 10:29 id_rsa.pub<br>[hadoop@warehouse001 .ssh]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[hadoop@warehouse001 .ssh]$ chmod 0600 ~/.ssh/authorized_keys</p><p>验证ssh,第一次必须输入yes建立关系,无需输入密码即可打印日期</p><p>[hadoop@warehouse001 .ssh]$ ssh hadoop@warehouse001 date<br>The authenticity of host ‘warehouse001 (172.23.75.57)’ can’t be established.<br>ECDSA key fingerprint is SHA256:XliTXyWZCOu2gk6FPXdhy4QOtYyTzmmKx6UdMuQ1LpY.<br>ECDSA key fingerprint is MD5:3e:e7:27:d3:95:28:f2:dd:b5:71:df:93:c9:83:b6:f3.<br>Are you sure you want to continue connecting (yes/no)? yes<br>Warning: Permanently added ‘warehouse001,172.23.75.57’ (ECDSA) to the list of known hosts.<br>Sun Nov 22 10:31:25 CST 2018</p><p>第二次直接打印</p><p>[hadoop@warehouse001 .ssh]$ ssh hadoop@warehouse001 date<br>Sun Nov 22 10:33:23 CST 2018<br>[hadoop@warehouse001 .ssh]$ </p><h5 id="7-7-配置namenode进程以warehouse001启动"><a href="#7-7-配置namenode进程以warehouse001启动" class="headerlink" title="7.7 配置namenode进程以warehouse001启动"></a>7.7 配置namenode进程以warehouse001启动</h5><p>修改core-site.xml文件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://warehouse001:9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h5 id="7-8-配置secondary-namenode进程以warehouse001启动"><a href="#7-8-配置secondary-namenode进程以warehouse001启动" class="headerlink" title="7.8 配置secondary namenode进程以warehouse001启动"></a>7.8 配置secondary namenode进程以warehouse001启动</h5><p>修改hdfs-site.xml文件</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>warehouse001:50090<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.https-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>warehouse001:50091<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h5 id="7-9配置datanode进程以warehouse001启动"><a href="#7-9配置datanode进程以warehouse001启动" class="headerlink" title="7.9配置datanode进程以warehouse001启动"></a>7.9配置datanode进程以warehouse001启动</h5><p>[hadoop@warehouse001 hadoop]$ vi slaves                </p><p>warehouse001</p><h5 id="7-10格式化"><a href="#7-10格式化" class="headerlink" title="7.10格式化"></a>7.10格式化</h5><p>[hadoop@warehouse001 hadoop]$ sbin/start-dfs.sh<br>20/11/22 11:17:04 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0<br>20/11/22 11:17:04 INFO util.ExitUtil: Exiting with status 0<br>20/11/22 11:17:04 INFO namenode.NameNode: SHUTDOWN_MSG:<br>/<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>****</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong><br>SHUTDOWN_MSG: Shutting down NameNode at warehouse001/172.23.75.57<br>**<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>**</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>/</p><h5 id="7-11启动"><a href="#7-11启动" class="headerlink" title="7.11启动"></a>7.11启动</h5><p>[hadoop@warehouse001 sbin]$ sh start-dfs.sh<br>which: no start-dfs.sh in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)<br>20/11/22 11:29:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Starting namenodes on [warehouse001]<br>warehouse001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-namenode-warehouse001.out<br>localhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-datanode-warehouse001.out<br>Starting secondary namenodes [warehouse001]<br>warehouse001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-secondarynamenode-warehouse001.out<br>20/11/22 11:29:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</p><p>[hadoop@warehouse001 hadoop]$ jps<br>13377 Jps<br>13090 NameNode<br>12098 SecondaryNameNode<br>11944 DataNode</p><p>hdfs存储来说:<br>namenode             名称节点       老大<br>secondary namenode   第二名称节点   老二  每隔1小时 把老大备份一下</p><p>datanode             数据节点       小弟</p><p>最终三个进程以warehouse001 机器名称启动 </p><p>[hadoop@warehouse001 sbin]$ sh start-dfs.sh<br>which: no start-dfs.sh in (/usr/java/jdk1.8.0_181/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)<br>20/11/22 11:54:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Starting namenodes on [warehouse001]<br>warehouse001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-namenode-warehouse001.out<br>warehouse001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-datanode-warehouse001.out<br>Starting secondary namenodes [warehouse001]<br>warehouse001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/hadoop-hadoop-secondarynamenode-warehouse001.out<br>20/11/22 11:54:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</p><p>打开web，云主机需要开启【安全组】</p><p><a href="http://warehouse001:50070/dfshealth.html#tab-overview">http://warehouse001:50070/dfshealth.html#tab-overview</a></p><p>Make the HDFS directories required to execute MapReduce jobs:</p><pre><code>  $ bin/hdfs dfs -mkdir /user  $ bin/hdfs dfs -mkdir /user/&lt;username&gt;</code></pre><p>8.案例<br>[hadoop@warehouse001 hadoop]$bin/hadoop jar <br>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar <br>grep input output ‘dfs[a-z.]+’</p><p>执行成功后</p><p>[hadoop@warehouse001 hadoop]$ bin/hdfs dfs -get output output</p><p>[hadoop@warehouse001 hadoop]$ cd output/<br>[hadoop@warehouse001 output]$ ll<br>total 4<br>-rw-r–r– 1 hadoop hadoop 301 Nov 22 14:44 part-r-00000<br>-rw-r–r– 1 hadoop hadoop   0 Nov 22 14:44 _SUCCESS<br>[hadoop@warehouse001 output]$ cat part-r-00000<br>6       dfs.audit.logger<br>4       dfs.class<br>3       dfs.logger<br>3       dfs.server.namenode.<br>2       dfs.audit.log.maxfilesize<br>2       dfs.audit.log.maxbackupindex<br>2       dfs.period<br>1       dfsmetrics.log<br>1       dfsadmin<br>1       dfs.servers<br>1       dfs.replication<br>1       dfs.namenode.secondary.https<br>1       dfs.namenode.secondary.http<br>1       dfs.log<br>1       dfs.file<br>1       dfs.namenode.http</p><p>但是namenode、datanode、checkpoint(secondarynamenode)官方默认配置如下:<br>dfs.namenode.name.dir –&gt; file://${hadoop.tmp.dir}/dfs/name<br>dfs.datanode.data.dir –&gt; file://${hadoop.tmp.dir}/dfs/data<br>dfs.namenode.checkpoint.dir –&gt; file://${hadoop.tmp.dir}/dfs/namesecondary</p><p>所以配置hadoop.tmp.dir临时目录改为/home/hadoop/tmp，<br>那么namenode、datanode、checkpoint(secondarynamenode)<br>存储也对应变更。</p><p>1.sql行转列、列转行</p><p> 我们首先先通过一个学生成绩表(下面简化了些)来形象了解下行转列</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span>  <span class="token keyword">TABLE</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token punctuation">(</span>UserName <span class="token keyword">VARCHAR</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     Subject <span class="token keyword">VARCHAR</span><span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Score <span class="token keyword">int</span><span class="token punctuation">)</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">;</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'数学'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Nick'</span><span class="token punctuation">,</span> <span class="token string">'生物'</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'语文'</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'数学'</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">;</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'英语'</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">;</span> <span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> mydatabasedb<span class="token punctuation">.</span>StudentScores <span class="token keyword">SELECT</span> <span class="token string">'Kent'</span><span class="token punctuation">,</span> <span class="token string">'生物'</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">;</span><span class="token keyword">select</span>  <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token keyword">select</span> UserName<span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'语文'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'语文'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'数学'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'数学'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'英语'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'英语'</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span><span class="token keyword">case</span> Subject <span class="token keyword">when</span> <span class="token string">'生物'</span> <span class="token keyword">then</span> score <span class="token keyword">else</span> <span class="token number">0</span> <span class="token keyword">end</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token string">'生物'</span><span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>StudentScores<span class="token keyword">group</span> <span class="token keyword">by</span> UserName</code></pre><p><img src="https://i.loli.net/2020/11/22/b3HoLOiN8YnzwFq.png"></p><p>列转行:</p><p><img src="https://i.loli.net/2020/11/23/UCgtDy2PNVxOz1e.png"></p><p>实现:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'语文'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>语文 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student    <span class="token keyword">union</span> <span class="token keyword">all</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'数学'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>数学 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student    <span class="token keyword">union</span> <span class="token keyword">all</span>    <span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token string">'物理'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span>物理 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student<span class="token punctuation">)</span> <span class="token number">a</span> </code></pre><p><img src="https://i.loli.net/2020/11/23/Oy9qUF2gAQt3ez4.png"></p><p>理解：把语文列装换为分数列，本身列头没有了，使用字符来填充列头，代表本身列具备的含义。</p><p>单个示例:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span>  <span class="token string">'语文'</span> <span class="token keyword">as</span> 科目<span class="token punctuation">,</span> 语文 <span class="token keyword">as</span> 分数 <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>student</code></pre><p><img src="https://i.loli.net/2020/11/23/qAjzQyilEUvGY2h.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql的join及练习</title>
      <link href="2018/11/06/2020-11-18-quannnxu-mysql2/"/>
      <url>2018/11/06/2020-11-18-quannnxu-mysql2/</url>
      
        <content type="html"><![CDATA[<h5 id="left-join"><a href="#left-join" class="headerlink" title="left join"></a>left join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/TB9nkIH7EWUMRGh.png"></p><h5 id="right-join"><a href="#right-join" class="headerlink" title="right join"></a>right join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/TB9nkIH7EWUMRGh.png"></p><h5 id="inner-join"><a href="#inner-join" class="headerlink" title="inner join"></a>inner join</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">inner</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id </code></pre><p><img src="https://i.loli.net/2020/11/22/vGtbnOWNh81pYDM.png"></p><h5 id="full-join"><a href="#full-join" class="headerlink" title="full join"></a>full join</h5><p>a表+b表全部</p><p>mysql中没有这个语法,spark,hive支持</p><p>总结:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span><span class="token comment" spellcheck="true">--字段</span><span class="token keyword">from</span><span class="token comment" spellcheck="true">--表</span><span class="token keyword">where</span> xxx<span class="token keyword">group</span> <span class="token keyword">by</span> xxxx <span class="token keyword">having</span> xxxx<span class="token keyword">order</span> <span class="token keyword">by</span> xxxx<span class="token keyword">limit</span> <span class="token number">1</span><span class="token punctuation">;</span>                    <span class="token comment" spellcheck="true">--注意使用顺序</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--创建视图</span><span class="token keyword">create</span> <span class="token keyword">view</span> mydatabase<span class="token punctuation">.</span>tv  <span class="token keyword">as</span> <span class="token keyword">select</span>  <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token keyword">as</span> vid<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name <span class="token keyword">as</span> vname<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">as</span> bid<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name <span class="token keyword">as</span> bname<span class="token keyword">from</span> mydatabase<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> mydatabase<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token comment" spellcheck="true">--这个视图是不存数据的,查询时去执行视图sql</span><span class="token comment" spellcheck="true">--物化视图:是存数据的</span></code></pre><p>作业:</p><p>1.mysql存储引擎myisam和innodb的区别</p><p>2.between A and B</p><p>3.物化视图是什么</p><p>4.sql题</p><hr><p>数据仓库</p><p>离线数仓:</p><p>​        数据源同步    oracle/mysql–&gt;sqoop/datax/kettle–&gt;hive</p><p>​        etl加工            hivesql、spark sql、spark code</p><p>实时数仓:</p><p>​        数据源同步    oracle–&gt;ogg        –&gt;kafaka</p><p>​                                                                                        –&gt;spark streaming/flink–&gt;hbase/kudu/elasticserch</p><p>​                                mysql–&gt;maxwell–&gt;kafaka</p><p>​        ETL加工                                                                  –&gt;spark streaming/flink–&gt;hbase(java)–&gt;oracle/mysql    ETL加工</p><p>数据平台</p><p>数据中台</p><p>数据湖</p><p>单点—————–&gt;分布式存储</p><p>​        数据迁移</p><p>数仓的根本:数据质量,保证数据不丢,哪怕丢了能够补齐,重跑计算</p><p>检测:</p><p>​        比对源端和目标端数据量    100%</p><p>​        抽查源端和目标端数据内容    5%    概率性</p><p>​        明细数据汇总和科目表(汇总表)对比</p><p>业务数据是明细</p><p>1    100</p><p>2    100</p><p>3    50</p><p>科目汇总</p><p>250</p><hr><p>​                源表                                                                                目标表</p><p>​        1    xiaoxing                                                                     1    xiaoxing<br>​        5    xiaoming                                                                    5    xiaoming<br>​        6    lisi                                                                                6    lisi                                                                                        –成功</p><pre class=" language-sql"><code class="language-sql">    <span class="token number">5</span>    xiaoming                            <span class="token boolean">null</span>    <span class="token boolean">null</span>                                                                            <span class="token number">6</span>    lisi                                <span class="token boolean">null</span>    <span class="token boolean">null</span>                                <span class="token comment" spellcheck="true">--源端有 目标没有                            </span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">left</span> <span class="token keyword">join</span> ruozedata<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> <span class="token number">b</span><span class="token punctuation">.</span>id <span class="token operator">is</span> <span class="token boolean">null</span>            <span class="token comment" spellcheck="true">--需要补齐数据</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token boolean">null</span>    <span class="token boolean">null</span>                            <span class="token number">5</span>    xiaoming                                                            <span class="token boolean">null</span>    <span class="token boolean">null</span>                            <span class="token number">6</span>    lisi    <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>id<span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">.</span>name<span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>t1 <span class="token number">a</span> <span class="token keyword">right</span> <span class="token keyword">join</span> ruozedata<span class="token punctuation">.</span>t2 <span class="token number">b</span><span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>id <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>id <span class="token operator">is</span> <span class="token boolean">null</span>            <span class="token comment" spellcheck="true">--需要剔除</span></code></pre><p>练习</p><p>use mydatabasedb;</p><p>–部门表<br>dept部门表(deptno部门编号/dname部门名称/loc地点)<br>create table dept (<br>    deptno numeric(2),<br>    dname varchar(14),<br>    loc varchar(13)<br>);</p><p>insert into dept values (10, ‘ACCOUNTING’, ‘NEW YORK’);<br>insert into dept values (20, ‘RESEARCH’, ‘DALLAS’);<br>insert into dept values (30, ‘SALES’, ‘CHICAGO’);<br>insert into dept values (40, ‘OPERATIONS’, ‘BOSTON’);</p><p>–工资等级表<br>salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)<br>create table salgrade (<br>    grade numeric,<br>    losal numeric,<br>    hisal numeric<br>);</p><p>insert into salgrade values (1, 700, 1200);<br>insert into salgrade values (2, 1201, 1400);<br>insert into salgrade values (3, 1401, 2000);<br>insert into salgrade values (4, 2001, 3000);<br>insert into salgrade values (5, 3001, 9999);</p><p>–员工表<br>emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)<br>工资 ＝ 薪金 ＋ 佣金</p><p>create table emp (<br>    empno numeric(4) not null,<br>    ename varchar(10),<br>    job varchar(9),<br>    mgr numeric(4),<br>    hiredate datetime,<br>    sal numeric(7, 2),<br>    comm numeric(7, 2),<br>    deptno numeric(2)<br>);</p><p>insert into emp values (7369, ‘SMITH’, ‘CLERK’, 7902, ‘1980-12-17’, 800, null, 20);<br>insert into emp values (7499, ‘ALLEN’, ‘SALESMAN’, 7698, ‘1981-02-20’, 1600, 300, 30);<br>insert into emp values (7521, ‘WARD’, ‘SALESMAN’, 7698, ‘1981-02-22’, 1250, 500, 30);<br>insert into emp values (7566, ‘JONES’, ‘MANAGER’, 7839, ‘1981-04-02’, 2975, null, 20);<br>insert into emp values (7654, ‘MARTIN’, ‘SALESMAN’, 7698, ‘1981-09-28’, 1250, 1400, 30);<br>insert into emp values (7698, ‘BLAKE’, ‘MANAGER’, 7839, ‘1981-05-01’, 2850, null, 30);<br>insert into emp values (7782, ‘CLARK’, ‘MANAGER’, 7839, ‘1981-06-09’, 2450, null, 10);<br>insert into emp values (7788, ‘SCOTT’, ‘ANALYST’, 7566, ‘1982-12-09’, 3000, null, 20);<br>insert into emp values (7839, ‘KING’, ‘PRESIDENT’, null, ‘1981-11-17’, 5000, null, 10);<br>insert into emp values (7844, ‘TURNER’, ‘SALESMAN’, 7698, ‘1981-09-08’, 1500, 0, 30);<br>insert into emp values (7876, ‘ADAMS’, ‘CLERK’, 7788, ‘1983-01-12’, 1100, null, 20);<br>insert into emp values (7900, ‘JAMES’, ‘CLERK’, 7698, ‘1981-12-03’, 950, null, 30);<br>insert into emp values (7902, ‘FORD’, ‘ANALYST’, 7566, ‘1981-12-03’, 3000, null, 20);<br>insert into emp values (7934, ‘MILLER’, ‘CLERK’, 7782, ‘1982-01-23’, 1300, null, 10);</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--1. 查询出部门编号为30的所有员工的编号和姓名</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token keyword">where</span> deptno<span class="token operator">=</span><span class="token string">'30'</span><span class="token comment" spellcheck="true">--2.找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token number">10</span> <span class="token operator">and</span> job<span class="token operator">=</span><span class="token string">'MANAGER'</span><span class="token keyword">union</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token number">20</span> <span class="token operator">and</span> job<span class="token operator">=</span><span class="token string">'SALESMAN'</span><span class="token comment" spellcheck="true">--3.查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</span><span class="token keyword">select</span>  <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> salary<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">order</span> <span class="token keyword">by</span> salary <span class="token keyword">desc</span><span class="token punctuation">,</span>hiredate <span class="token comment" spellcheck="true">--4.列出薪金大于1500的各种工作及从事此工作的员工人数。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>job<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> sal<span class="token operator">></span><span class="token number">1500</span><span class="token keyword">group</span> <span class="token keyword">by</span> job<span class="token comment" spellcheck="true">--5.列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno<span class="token keyword">where</span> <span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token operator">=</span><span class="token string">'SALES'</span><span class="token comment" spellcheck="true">--6.查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L  __</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'S%'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'%S'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'%S%'</span><span class="token operator">or</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token operator">like</span> <span class="token string">'_L%'</span><span class="token comment" spellcheck="true">--7.查询每种工作的最高工资、最低工资、人数</span><span class="token keyword">select</span> <span class="token function">max</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> maxsalary<span class="token punctuation">,</span><span class="token function">min</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> minsalary<span class="token punctuation">,</span>job<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> jobcount<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">group</span> <span class="token keyword">by</span> job<span class="token comment" spellcheck="true">--8.列出薪金 高于 公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</span><span class="token keyword">select</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token keyword">case</span> <span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">700</span> <span class="token operator">and</span> <span class="token number">1200</span> <span class="token keyword">then</span> <span class="token number">1</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">1201</span> <span class="token operator">and</span> <span class="token number">1400</span> <span class="token keyword">then</span> <span class="token number">2</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">1404</span> <span class="token operator">and</span> <span class="token number">2000</span> <span class="token keyword">then</span> <span class="token number">3</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">2001</span> <span class="token operator">and</span> <span class="token number">3000</span> <span class="token keyword">then</span> <span class="token number">3</span><span class="token keyword">when</span> t<span class="token punctuation">.</span>salary <span class="token operator">between</span> <span class="token number">3001</span> <span class="token operator">and</span> <span class="token number">9999</span> <span class="token keyword">then</span> <span class="token number">4</span> <span class="token keyword">end</span> <span class="token keyword">as</span> grade<span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>empno <span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname<span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>mgr<span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">+</span>ifnull<span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> salary<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>sal <span class="token operator">></span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">avg</span><span class="token punctuation">(</span><span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token punctuation">)</span> avgsal<span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token comment" spellcheck="true">--9.列出薪金  高于  在部门30工作的  所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</span><span class="token keyword">select</span> <span class="token number">e</span><span class="token punctuation">.</span>ename <span class="token punctuation">,</span><span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token punctuation">,</span><span class="token number">d</span><span class="token punctuation">.</span>dname <span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp <span class="token number">e</span> <span class="token keyword">left</span> <span class="token keyword">join</span> mydatabasedb<span class="token punctuation">.</span>dept <span class="token number">d</span> <span class="token keyword">on</span> <span class="token number">e</span><span class="token punctuation">.</span>deptno <span class="token operator">=</span><span class="token number">d</span><span class="token punctuation">.</span>deptno <span class="token keyword">where</span> <span class="token number">e</span><span class="token punctuation">.</span>sal<span class="token operator">></span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">max</span><span class="token punctuation">(</span>sal<span class="token punctuation">)</span><span class="token keyword">from</span> mydatabasedb<span class="token punctuation">.</span>emp t<span class="token keyword">where</span> deptno <span class="token operator">=</span><span class="token string">'30'</span><span class="token punctuation">)</span></code></pre><p>tips:mysql分组topN</p><p>#哪些部门的哪些职业的薪水和,最高1位的职业是什么?<br>#1.每个部门的每个职业的薪水和</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">view</span> sal<span class="token keyword">as</span> <span class="token keyword">select</span>deptno<span class="token punctuation">,</span>job<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>sal<span class="token operator">+</span>ifnull<span class="token punctuation">(</span>comm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> salary<span class="token keyword">from</span> emp <span class="token keyword">group</span> <span class="token keyword">by</span> deptno<span class="token punctuation">,</span>job<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> sal<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token keyword">from</span> sal <span class="token number">a</span> <span class="token keyword">where</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> sal <span class="token number">b</span><span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>deptno<span class="token operator">=</span><span class="token number">b</span><span class="token punctuation">.</span>deptno <span class="token operator">and</span> <span class="token number">a</span><span class="token punctuation">.</span>salary<span class="token operator">&lt;</span><span class="token number">b</span><span class="token punctuation">.</span>salary        <span class="token comment" spellcheck="true">--自己关联自己,找到比自己大个数为0的,也就是最大的</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">0</span><span class="token keyword">order</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">.</span>deptno<span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql语法</title>
      <link href="2018/11/05/2020-11-16-quanxu-shu-cang-zhi-mysql-yu-fa/"/>
      <url>2018/11/05/2020-11-16-quanxu-shu-cang-zhi-mysql-yu-fa/</url>
      
        <content type="html"><![CDATA[<h4 id="1-字段类型"><a href="#1-字段类型" class="headerlink" title="1.字段类型"></a>1.字段类型</h4><h5 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型:"></a>数值类型:</h5><p>​        int    整数</p><p>​        long    长整型</p><p>​        float    单精度</p><p>​        double    双精度</p><p>​        decimal    钱</p><h5 id="字符串"><a href="#字符串" class="headerlink" title="字符串:"></a>字符串:</h5><p>​        char 字节    定长0-255长度    aaaaxxxxxx    自动补齐</p><p>​        varchar    字符串    变长0-65535字节    aaaa</p><h5 id="日期"><a href="#日期" class="headerlink" title="日期"></a>日期</h5><p>​        date    日期    YYYY-MM-DD</p><p>​        time    时间    HH:MM:SS</p><p>​        datetime    年月日时分秒</p><p>​        timestamp        年月日时分秒</p><h5 id="datetime和timestamp的区别"><a href="#datetime和timestamp的区别" class="headerlink" title="datetime和timestamp的区别:"></a>datetime和timestamp的区别:</h5><h6 id="datetime类型使用8个字节来表示日期和时间。"><a href="#datetime类型使用8个字节来表示日期和时间。" class="headerlink" title="datetime类型使用8个字节来表示日期和时间。"></a>datetime类型使用8个字节来表示日期和时间。</h6><p>支持的常见插入格式为：</p><ol><li>（推荐甚至强制要求必须）‘YYYY-MM-DD HH:MM:SS’或‘YYYYMMDDHHMMSS’格式的字符串表示。这种方式可以表达的范围是‘1000-01-01 00:00:00’~~‘9999-12-31 23:59:59’。</li><li>MySQL中还支持一些不严格的语法格式，任何的标点都可以用来做间隔符。情况与date类型相同，而且时间部分也可以使用任意的分隔符隔开，这与Time类型不同，Time类型只能用‘:’隔开呢。</li><li>使用now()来输入当前系统日期和时间。</li></ol><h6 id="timestamp类型使用4个字节来表示日期和时间。"><a href="#timestamp类型使用4个字节来表示日期和时间。" class="headerlink" title="timestamp类型使用4个字节来表示日期和时间。"></a>timestamp类型使用4个字节来表示日期和时间。</h6><p>支持的常见插入格式为：</p><p>二者主要区别在于取值范围。</p><ol><li><p>timestamp存储需要四个字节，它的取值范围为“1970-01-01 00:00:01” UTC ~ “2038-01-19 03:14:07” （和时区有关）</p></li><li><p>而datetime取值范围为“1000-01-01 00:00:00” ~ “9999-12-31 23:59:59”（和时区无关，怎么存入怎么返回，对程序员友好）</p><p>3、timestamp类型还有一个很大的特殊点，就是时间是根据时区来显示的。<br>例如，在东八区插入的timestamp类型为2009-09-30 14:21:25，在东七区显示时，时间部门就变成了13:21:25，在东九区显示时，时间部门就变成了15:21:25。<br>4、需要显示日期与时间，timestamp类型需要根据不同地区的时区来转换时间，但是，timestamp类型的范围太小，其最大时间为2038-01-19 11:14:07。<br>如果插入时间的比这个大，将会数据库插入0000-00-00 00:00:00。所以需要的时间范围比较大，还是选择dateTime类型比较安全。</p></li></ol><h4 id="2-sql类型"><a href="#2-sql类型" class="headerlink" title="2.sql类型"></a>2.sql类型</h4><p>ddl    数据定义语言    create drop</p><p>dml    数据操作语言    select    insert    update    delete    (增删改查)</p><p>dcl    数据控制语言    grant</p><h4 id="3-建表规范"><a href="#3-建表规范" class="headerlink" title="3.建表规范"></a>3.建表规范</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">auto_increment</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--第一列必须为自增长字段 主键，且无业务意义  架构设计默认规则</span>name <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--业务字段</span>age <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token comment" spellcheck="true">--业务字段</span>create_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建者</span>create_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建时间</span>update_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新者</span>update_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新时间</span><span class="token keyword">primary</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>id<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">--一张表只有一个主键 id，业务字段需要唯一的话，就使用唯一约束来保证</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>规范:<br>1.表名称  字段名称 不要写中文  尽量不要汉语拼音<br>2.统一风格:<br>已经存在表结构设计，风格比如是create_user;不同表的同一字段名称尽量统一,以防后面开发字段混淆.</p><h4 id="4-增删改查"><a href="#4-增删改查" class="headerlink" title="4.增删改查"></a>4.增删改查</h4><p>插入数据</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">(</span>name<span class="token punctuation">,</span>age<span class="token punctuation">)</span> <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'quanxu'</span><span class="token punctuation">,</span><span class="token number">22</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>查询数据:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">;</span></code></pre><pre class=" language-sql"><code class="language-sql"><span class="token keyword">update</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">set</span> age<span class="token operator">=</span><span class="token string">'25'</span> <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'quanxu'</span><span class="token punctuation">;</span></code></pre><p>删除数据:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">delete</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> id<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span></code></pre><p>创建唯一约束:</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">ADD</span> <span class="token keyword">CONSTRAINT</span> employee_un <span class="token keyword">UNIQUE</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>SQL 错误 [1062] [23000]: Duplicate entry ‘quanxu’ for key ‘employee_un’</p><p>违反唯一约束,这时就要去检查是否重复</p><h4 id="5-后面四个字段一定要加上"><a href="#5-后面四个字段一定要加上" class="headerlink" title="5.后面四个字段一定要加上"></a>5.后面四个字段一定要加上</h4><pre class=" language-sql"><code class="language-sql">create_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建者</span>create_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--创建时间</span>update_user <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新者</span>update_time <span class="token keyword">timestamp</span> <span class="token operator">not</span> <span class="token boolean">null</span> <span class="token keyword">default</span> <span class="token keyword">current_timestamp</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--更新时间</span></code></pre><p>离线数仓抽数的基础</p><h4 id="6-字段注释一定要加上"><a href="#6-字段注释一定要加上" class="headerlink" title="6.字段注释一定要加上"></a>6.字段注释一定要加上</h4><p>comment ‘xxxx’</p><pre class=" language-sql"><code class="language-sql">name <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span> <span class="token keyword">comment</span> <span class="token string">'名字'</span><span class="token punctuation">,</span></code></pre><h4 id="7-其他语法"><a href="#7-其他语法" class="headerlink" title="7.其他语法"></a>7.其他语法</h4><ul><li><p>1.where(筛选)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'zhangsan'</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">=</span><span class="token string">'zhangsan'</span> <span class="token operator">and</span> age<span class="token operator">=</span><span class="token number">22</span><span class="token punctuation">;</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age<span class="token operator">=</span><span class="token number">12</span> <span class="token operator">or</span> age<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age<span class="token operator">></span><span class="token number">12</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age <span class="token operator">in</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> age <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre></li><li><p>2.order by (排序)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--默认正序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">asc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--正序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">desc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--倒序</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">order</span> <span class="token keyword">by</span> age <span class="token keyword">desc</span><span class="token punctuation">,</span>name <span class="token keyword">desc</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--多个排序</span></code></pre></li><li><p>3.like(模糊匹配)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'l%'</span>    <span class="token comment" spellcheck="true">--以l开头</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'%o'</span>    <span class="token comment" spellcheck="true">--以o结尾</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> <span class="token string">'%i%'</span>    <span class="token comment" spellcheck="true">--含有i</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">where</span> name <span class="token operator">like</span> '<span class="token comment" spellcheck="true">--p%'    --第三个位为p,_表示占位符</span></code></pre></li><li><p>4.union(合并)</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">union</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span>    <span class="token comment" spellcheck="true">--合并后去重</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span>    <span class="token comment" spellcheck="true">--合并不去重</span></code></pre></li><li><p>5.分组语法</p><pre class=" language-sql"><code class="language-sql">sum    <span class="token comment" spellcheck="true">--求和</span>avg    <span class="token comment" spellcheck="true">--平均数</span>max    <span class="token comment" spellcheck="true">--最大值</span>min    <span class="token comment" spellcheck="true">--最小值</span>count    <span class="token comment" spellcheck="true">--计数</span><span class="token keyword">select</span> <span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">avg</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">max</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">min</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token punctuation">;</span><span class="token keyword">select</span> name<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee <span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token punctuation">;</span><span class="token keyword">select</span>name<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span> sum_age<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token keyword">having</span> sum_age <span class="token operator">></span><span class="token number">30</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">--having相当于过滤</span><span class="token comment" spellcheck="true">--    等同于==></span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span>name<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>age<span class="token punctuation">)</span> sum_age<span class="token punctuation">,</span>    <span class="token comment" spellcheck="true">--子查询</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">from</span> ruozedata<span class="token punctuation">.</span>employee<span class="token keyword">group</span> <span class="token keyword">by</span> name<span class="token punctuation">)</span> <span class="token keyword">as</span> t<span class="token keyword">where</span> sum_age <span class="token operator">></span><span class="token number">30</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">--能使用having尽量使用having,不能使用时再使用子查询</span></code></pre></li></ul><p>tips:</p><h5 id="spark中exists和in有什么区别"><a href="#spark中exists和in有什么区别" class="headerlink" title="spark中exists和in有什么区别?"></a>spark中exists和in有什么区别?</h5><h5 id="一、exists和in的使用方式"><a href="#一、exists和in的使用方式" class="headerlink" title="一、exists和in的使用方式"></a>一、exists和in的使用方式</h5><p>​       1、exists是对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；</p><p>　　2、in是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。</p><p>　　3、如果用not in ，则是内外表都全表扫描，无索引，效率低，可考虑使用not exists，也可使用A left join B on A.id=B.id where B.id is null 进行优化。</p><h5 id="二、EXISTS与IN-的用法异同"><a href="#二、EXISTS与IN-的用法异同" class="headerlink" title="二、EXISTS与IN 的用法异同"></a>二、EXISTS与IN 的用法异同</h5><p>1、IN只能返回一个字段值，但EXITS允许返回多个字段。<br>2、exists是对外表做loop循环，每次loop循环再对内表做查询；IN是将外表和内表做hash连接，相当于多个or条件叠加。exists需要查询数据库，in是内存里遍历比较。</p><h5 id="2、EXISTS与IN-的效率比较"><a href="#2、EXISTS与IN-的效率比较" class="headerlink" title="2、EXISTS与IN 的效率比较"></a>2、EXISTS与IN 的效率比较</h5><p>对于表A，表B：</p><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--用法1</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token number">cc</span> <span class="token operator">in</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">cc</span> <span class="token keyword">from</span> <span class="token number">b</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">--用法2</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">a</span> <span class="token keyword">where</span> <span class="token keyword">exists</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">cc</span><span class="token operator">=</span><span class="token number">a</span><span class="token punctuation">.</span><span class="token number">cc</span><span class="token punctuation">)</span></code></pre><p>1、若表A/B大小相当，那运行效率差异不大；<br>2、若表A（外表）更大，则用法1即in()效率更高；<br>3、若表B（内表）更大，则用法2即exists()效率更高。</p>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql部署</title>
      <link href="2018/11/03/2020-11-14-quanxu-shu-cang-zhi-mysql-bu-shu/"/>
      <url>2018/11/03/2020-11-14-quanxu-shu-cang-zhi-mysql-bu-shu/</url>
      
        <content type="html"><![CDATA[<h3 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h3><p><a href="https://www.bilibili.com/video/BV12b411N7Lv">https://www.bilibili.com/video/BV12b411N7Lv</a><br><a href="https://www.bilibili.com/video/BV1Tt411p7de">https://www.bilibili.com/video/BV1Tt411p7de</a></p><p><a href="https://github.com/Hackeruncle/MySQL">https://github.com/Hackeruncle/MySQL</a><br><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.7.11%20Install.txt">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.7.11%20Install.txt</a></p><h4 id="1-jdk安装"><a href="#1-jdk安装" class="headerlink" title="1.jdk安装"></a>1.jdk安装</h4><p>mkdir -p /usr/java</p><p>tar  -zxvf jdk-8u181-linux-x64.tar.gz -C  /usr/java</p><p>[root@warehouse001 java]# ll<br>total 350520<br>drwxr-xr-x 7   10  143      4096 Jul  7  2018 jdk1.8.0_181【注意赋所属组的权限】</p><p>chmod 775 jdk1.8.0_181</p><p>chown -R root:root jdk1.8.0_181</p><p>vi /etc/profile</p><p>追加:</p><p>export JAVA_HOME=/usr/java/jdk1.8.0_181<br>export PATH=$JAVA_HOME/bin:$PATH</p><h4 id="2-mysql的安装"><a href="#2-mysql的安装" class="headerlink" title="2.mysql的安装"></a>2.mysql的安装</h4><p>要注意文档中的命令是哪个用户,哪个目录执行</p><p>tar -zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz -C /usr/local/</p><p>mv mysql-5.7.11-linux-glibc2.5-x86_64 mysql</p><h5 id="创建my-cnf-见文件"><a href="#创建my-cnf-见文件" class="headerlink" title="创建my.cnf(见文件)"></a>创建my.cnf(见文件)</h5><p>vi /etc/my.cnf</p><pre class=" language-shell"><code class="language-shell">[client]port            = 3306socket          = /usr/local/mysql/data/mysql.sockdefault-character-set=utf8mb4[mysqld]port            = 3306socket          = /usr/local/mysql/data/mysql.sockskip-slave-startskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU's*2 for thread_concurrency#thread_concurrency = 32 #isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id  = 1739basedir     = /usr/local/mysqldatadir     = /usr/local/mysql/datapid-file     = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = ROWlog_bin_trust_function_creators=1log-error  = /usr/local/mysql/data/hostname.errlog-bin = /usr/local/mysql/arch/mysql-binexpire_logs_days = 7innodb_write_io_threads=16relay-log  = /usr/local/mysql/relay_log/relay-logrelay-log-index = /usr/local/mysql/relay_log/relay-log.indexrelay_log_info_file= /usr/local/mysql/relay_log/relay-log.infolog_slave_updates=1gtid_mode=OFFenforce_gtid_consistency=OFF# slaveslave-parallel-type=LOGICAL_CLOCKslave-parallel-workers=4master_info_repository=TABLErelay_log_info_repository=TABLErelay_log_recovery=ON#other logs#general_log =1#general_log_file  = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slavesync_binlog = 500#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:1G;ibdata2:1G:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 4innodb_log_file_size = 1Ginnodb_log_buffer_size = 200M#根据生产需要，调整pool size innodb_buffer_pool_size = 2G#innodb_additional_mem_pool_size = 50M #deprecated in 5.6tmpdir = /usr/local/mysql/tmpinnodb_lock_wait_timeout = 1000#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 2innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 128M[mysql]no-auto-rehashdefault-character-set=utf8mb4[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M</code></pre><p>#根据生产需要，调整pool size </p><p>innodb_buffer_pool_size = 2G</p><p>#innodb_additional_mem_pool_size = 50M #deprecated in 5.6</p><p>tmpdir = /usr/local/mysql/tmp</p><h4 id="3-创建用户组及用户"><a href="#3-创建用户组及用户" class="headerlink" title="3.创建用户组及用户"></a>3.创建用户组及用户</h4><p>groupadd -g 101 dba</p><p>useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</p><p>id mysqladmin</p><p>–uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)</p><p>4.copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量</p><p>cp /etc/skel/.* /usr/local/mysql  ###important</p><h4 id="5-配置环境变量"><a href="#5-配置环境变量" class="headerlink" title="5.配置环境变量"></a>5.配置环境变量</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# vi mysql/.bash_profile# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then        . ~/.bashrcfi# User specific environment and startup programsexport MYSQL_BASE=/usr/local/mysqlexport PATH=$&#123;MYSQL_BASE&#125;/bin:$PATHunset USERNAME#stty erase ^Hset umask to 022umask 022PS1=`uname -n`":"'$USER'":"'$PWD'":>"; export PS1</code></pre><h4 id="6-赋权限和用户组，切换用户mysqladmin，安装"><a href="#6-赋权限和用户组，切换用户mysqladmin，安装" class="headerlink" title="6.赋权限和用户组，切换用户mysqladmin，安装"></a>6.赋权限和用户组，切换用户mysqladmin，安装</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# chown  mysqladmin:dba /etc/my.cnf [root@hadoop39 local]# chmod  640 /etc/my.cnf  [root@hadoop39 local]# chown -R mysqladmin:dba /usr/local/mysql[root@hadoop39 local]# chmod -R 755 /usr/local/mysql </code></pre><h4 id="7-配置服务及开机自启动"><a href="#7-配置服务及开机自启动" class="headerlink" title="7.配置服务及开机自启动"></a>7.配置服务及开机自启动</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 local]# cd /usr/local/mysql#将服务文件拷贝到init.d下，并重命名为mysql[root@hadoop39 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限[root@hadoop39 mysql]# chmod +x /etc/rc.d/init.d/mysql#删除服务[root@hadoop39 mysql]# chkconfig --del mysql#添加服务[root@hadoop39 mysql]# chkconfig --add mysql[root@hadoop39 mysql]# chkconfig --level 345 mysql on</code></pre><p>这步并不能真正实现开机自启动</p><p>这步需要测试</p><pre class=" language-shell"><code class="language-shell">[root@sht-sgmhadoopnn-01 mysql]# vi /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don't# want to do the full Sys V style init stuff.touch /var/lock/subsys/localsu - mysqladmin -c "/etc/init.d/mysql start --federated"</code></pre><h4 id="8-安装libaio及安装mysql的初始db"><a href="#8-安装libaio及安装mysql的初始db" class="headerlink" title="8.安装libaio及安装mysql的初始db"></a>8.安装libaio及安装mysql的初始db</h4><pre class=" language-shell"><code class="language-shell">[root@hadoop39 mysql]# yum -y install libaio[root@hadoop39 mysql]# sudo su - mysqladminhadoop39.ruoze:mysqladmin:/usr/local/mysql:> bin/mysqld \--defaults-file=/etc/my.cnf \--user=mysqladmin \--basedir=/usr/local/mysql/ \--datadir=/usr/local/mysql/data/ \--initialize</code></pre><p>在初始化时如果加上 –initial-insecure，则会创建空密码的 root@localhost 账号，否则会创建带密码的 root@localhost 账号，密码直接写在 log-error 日志文件中<br>（在5.6版本中是放在 ~/.mysql_secret 文件里，更加隐蔽，不熟悉的话可能会无所适从）</p><h4 id="9-查看临时密码"><a href="#9-查看临时密码" class="headerlink" title="9.查看临时密码"></a>9.查看临时密码</h4><p>warehouse001:mysqladmin:/usr/local/mysql/data:&gt;cat hostname.err |grep password</p><p>2020-11-15T09:59:50.686247Z 1 [Note] A temporary password is generated for root@localhost: oifbVz!YZ2.D</p><h4 id="10-启动"><a href="#10-启动" class="headerlink" title="10.启动"></a>10.启动</h4><p>/usr/local/mysql/bin/mysqld_safe –defaults-file=/etc/my.cnf &amp;</p><h4 id="11-登录及修改用户密码"><a href="#11-登录及修改用户密码" class="headerlink" title="11.登录及修改用户密码"></a>11.登录及修改用户密码</h4><pre class=" language-shell"><code class="language-shell">hadoop39.ruoze:mysqladmin:/usr/local/mysql/data:>mysql -uroot -p'kFCqrXeh2y(0'mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.11-logCopyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql> alter user root@localhost identified by 'ruozedata';Query OK, 0 rows affected (0.05 sec)mysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'ruozedata' ;Query OK, 0 rows affected, 1 warning (0.02 sec)mysql> flush privileges;Query OK, 0 rows affected (0.00 sec)mysql> exit;Bye</code></pre><h4 id="12-重启"><a href="#12-重启" class="headerlink" title="12.重启"></a>12.重启</h4><pre class=" language-shell"><code class="language-shell">hadoop39.ruoze:mysqladmin:/usr/local/mysql:> service mysql restarthadoop39.ruoze:mysqladmin:/usr/local/mysql/data:>mysql -uroot -pruozedatamysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.11-log MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql> </code></pre><h3 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3.常用命令"></a>3.常用命令</h3><p>create database ruozedata;<br>grant all privileges on ruozedata.* to quanxu@’%’ identified by ‘ruozedata’;<br>flush privileges;</p><p>【注意】：对于mysql的用户操作，比如权限相关的 ，最后一步必须执行刷新权限。<br>          %代表了 任意的客户端的IP地址 都被允许使用quanxu用户来远程访问</p><p>grant all privileges on ruozedata.* to quanxu@’16.2.3.2’ identified by ‘ruozedata’;</p><p>我已经赋予%权限，在访问的时候为什么还抛错权限  访问错误<br>grant all privileges on ruozedata.* to quanxu@’客户端机器的IP’ identified by ‘ruozedata’;</p><p>grant all privileges on ruozedata.* to quanxu@’192.168.0.%’ identified by ‘ruozedata’;</p><p>139.224.129这个网段的所有ip都允许有权限去访问</p><p>mysql&gt; show databases;<br>mysql&gt; use mysql;<br>mysql&gt; show tables;</p><p>查看表结构<br>mysql&gt; desc db;<br>mysql&gt; show create table DB;<br>mysql&gt; select User,Host,db,Select_priv,Delete_priv from db;</p><p>杀会话<br>mysql&gt; show processlist;<br>mysql&gt; kill 7;</p><p>报错:DBeaver连接时connect time out</p><p>原因:云主机安全组没有加3306端口,阿里云需要配置</p><p><img src="C:\Users\10090\AppData\Roaming\Typora\typora-user-images\image-20201116175652491.png" alt="image-20201116175652491"></p>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.26早课</title>
      <link href="2018/10/30/2020-11-26-quannnxu-zao-ke/"/>
      <url>2018/10/30/2020-11-26-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.Linux的hosts文件，我们应该注意什么</p><p>2.Windows系统的hosts会找吗？</p><p>3.一般shell，代码这些我们与机器通信，是hostname吗？</p><p>4.后台执行脚本或命令，前后加什么?</p><p>5.执行xxx.sh脚本需要什么权限，哪两种方式执行</p><p>6.做软连接语法是什么? 主要应用场景哪两个? 同时需要注意什么?</p><p>7.crontab 编辑和查看参数是什么?  五个 * ，分别代表什么</p><p>8.作业  rundeck视频要看看做做，不要忘记</p><p>9.Linux现在遇见两个经典错误，链接拒绝，权限受限，会排查解决了吗？</p><p>10.mysql部署简述流程</p><p>11.对用户执行完权限相关操作，最后一句命令是什么?</p><p>12.%代表什么</p><p>13.创建用户和设置密码，那句sql会背吗？</p><p>14.vi简述编辑流程</p><p>15.Linux命令里带有大写 R的命令，哪两个?</p><p>16.rwxr–r–  数字多少</p><p>17.一个log文件很大，1G，找ERROR怎么办</p><p>18.个人环境变量文件在哪，怎样生效</p><p>19.su - 做什么事</p><p>20.三种方式切换用户的家目录</p><p>21.一个文件差不多1W行，我要去vi编辑一个参数xxx，但是我不知道在多少行，怎么快速找到?</p><p>1.Linux的hosts文件，我们应该注意什么<br>配置ip与机器名</p><p>2.Windows系统的hosts会找吗？<br>C:\Windows\System32\drivers\etc\hosts</p><p>3.一般shell，代码这些我们与机器通信，是hostname吗？<br>是的,hosts配置后hostname代表ip</p><p>4.后台执行脚本或命令，前后加什么?<br>nohub    …    &amp;</p><p>5.执行xxx.sh脚本需要什么权限，哪两种方式执行<br>./xxx.sh<br>sh xxx.sh</p><p>6.做软连接语法是什么? 主要应用场景哪两个? 同时需要注意什么?<br>ln -s<br>a.版本切换，脚本应用是配置的hadoop，是无感知的<br>b.小盘换大盘<br>注意的是：<br>权限 rwx  用户用户组</p><p>7.crontab 编辑和查看参数是什么?  五个 * ，分别代表什么<br>crontab -e </p><hr><p>分    时 日 月 周</p><p>8.作业  rundeck视频要看看做做，不要忘记<br><a href="https://www.bilibili.com/video/BV1Tb411c7nW?from=search&amp;seid=1376855633580937330">https://www.bilibili.com/video/BV1Tb411c7nW?from=search&amp;seid=1376855633580937330</a></p><p>9.Linux现在遇见两个经典错误，链接拒绝，权限受限，会排查解决了吗？<br>连接错误先ping ip,再telnet ip port,分析错误原因检查配置文件是否配置ip,防火墙是否关闭,安全组是否开启</p><p>10.mysql部署简述流程</p><p>11.对用户执行完权限相关操作，最后一句命令是什么?<br>flush privileges;</p><p>12.%代表什么<br>通配符,表示一串字符串</p><p>13.创建用户和设置密码，那句sql会背吗？<br>create user ‘UserName’@’%’ by ‘passwd’;<br>grant all privileges on <em>.</em> to ‘UserName’@’%’identified by ‘passwd’;<br>flush privileges;</p><p>14.vi简述编辑流程<br>i键进入编辑模式,插入内容,esc退出编辑模式,shift:进入尾行模式,wq保存退出</p><p>15.Linux命令里带有大写 R的命令，哪两个?<br>chmod    chown</p><p>16.rwxr–r–  数字多少<br>744</p><p>17.一个log文件很大，1G，找ERROR怎么办<br>cat xx.log | grep -C ERROR</p><p>18.个人环境变量文件在哪，怎样生效<br>~/.bash_profile<br>~/.bashrc<br>source生效</p><p>19.su - 做什么事<br>切换用户</p><p>20.三种方式切换用户的家目录<br>cd<br>cd ~<br>cd /home/username</p><p>21.一个文件差不多1W行，我要去vi编辑一个参数xxx，但是我不知道在多少行，怎么快速找到?<br>用/+关键字可以快速查找关键字。<br>被查到的关键字以高亮方式显示。如果根据关键字查到的有多个，可以通过按键“N”，快速定位到下一个高亮关键字。</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.25早课</title>
      <link href="2018/10/29/2020-11-25-quannnxu-zao-ke/"/>
      <url>2018/10/29/2020-11-25-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.高危命令有哪些</p><p>2.带R的命令哪些    </p><p>3.带r的命令哪些</p><p>4.块128m,三副本机制，一个文件180m，需要多少块，多少存储空间？</p><p>5.boss让我打开A服务器的 只知道名称好像含有xxx服务的，web界面，请问怎么做？</p><p>6.在杀死进程时，我们应该是否要确认 这个进程就是我们可以杀的？那么全局杀死所有 名称xxx的 进程，命令是什么？</p><p>7.which的执行结果输出，是来自哪个系统环境变量呢？</p><p>8.tar命令的压缩 、解压的命令参数 ？</p><p>9.谈谈你们对命令帮助 ，应该怎么看呢？会看吗？</p><p>10.查看系统负载、内存、磁盘 命令</p><p>1.高危命令有哪些<br>rm -rf;kill -9;&gt;</p><p>2.带R的命令哪些<br>chmod    chown    </p><p>3.带r的命令哪些<br>rm;cp等</p><p>4.块128m,三副本机制，一个文件180m，需要多少块，多少存储空间？<br>6个块;540m空间</p><p>5.boss让我打开A服务器的 只知道名称好像含有xxx服务的，web界面，请问怎么做？<br>先ps -ef | grep xxxx查看进程号<br>netstat -nlp 进程号    查看端口号<br>再ip+端口号进入web</p><p>6.在杀死进程时，我们应该是否要确认 这个进程就是我们可以杀的？那么全局杀死所有 名称xxx的 进程，命令是什么？<br>需要确认<br>kill -9 $(pgrep -f xxx)</p><p>7.which的执行结果输出，是来自哪个系统环境变量呢？<br>环境变量PATH中保存了命令的目录，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。</p><p>8.tar命令的压缩 、解压的命令参数 ？<br>tar -zxvf 解压<br>tar -zcvf 压缩</p><p>9.谈谈你们对命令帮助 ，应该怎么看呢？会看吗？<br>–help    查看参数和解释 【】为可选参数</p><p>10.查看系统负载、内存、磁盘 命令<br>top</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.24早课</title>
      <link href="2018/10/28/2020-11-24-quannnxu-zao-ke/"/>
      <url>2018/10/28/2020-11-24-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.个人环境变量文件，默认在哪里？推荐使用哪个呢？怎样生效？生效完成后，习惯做一件事是执行什么命令？</p><p>2.历史命令，执行第55行；清空历史命令</p><p>3.假如su/ssh 无法登录机器，这个用户是不是有可能在/etc/passwd文件里，做了禁止？</p><p>4.一个普通用户，想要瞬间临时获取root的最大权限，请问在哪个配置文件修改什么内容？博客写了吗？  在执行命令时，sudo命令加在前面？</p><p>5.vi编辑文件，想要从外部文件内容粘贴复制到这个文件，必须提前做一件什么事？否则数据丢失</p><p>6.window 和 Linux 去测试端口号的命令，会先部署吗？会使用吗？</p><p>7.netstat 一个服务时，显示  127.0.0.1:8899  ，外部window 或者其他服务器进行 访问，能通吗？</p><p>8.netstat 一个服务时，显示  hostname:8899  ，外部window 或者其他服务器进行 访问，假如不通，我们应该要调整什么？</p><p>9.rwx分别代表数字多少？  rwx–x-w- ,请问权限数字多少？</p><p>10.简述vi编辑文件的流程</p><p>1.个人环境变量文件，默认在哪里？推荐使用哪个呢？怎样生效？生效完成后，习惯做一件事是执行什么命令？<br>该用户的家目录<br>.bash_profile        .bashrc【推荐】<br>source生效<br>which xxx    (查看是否成功)</p><p>2.历史命令，执行第55行；清空历史命令<br>!55<br>history -c</p><p>3.假如su/ssh 无法登录机器，这个用户是不是有可能在/etc/passwd文件里，做了禁止？<br>是<br>mysqladmin:x:514:101::/usr/local/mysql:/bin/bash<br>apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin<br>ruoze:x:1004:1005::/home/ruoze:/usr/bin/false 没提示</p><p>4.一个普通用户，想要瞬间临时获取root的最大权限，请问在哪个配置文件修改什么内容？在执行命令时，sudo命令加在前面？<br>vi /etc/sudoers<br>hadoop   ALL=(root)      NOPASSWD:ALL<br>sudo xxx</p><p>5.vi编辑文件，想要从外部文件内容粘贴复制到这个文件，必须提前做一件什么事？否则数据丢失<br>先进入编辑模式—&gt;i键</p><p>6.window 和 Linux 去测试端口号的命令，会先部署吗？会使用吗？<br>先ping ip<br>再telnet ip port</p><p>7.netstat 一个服务时，显示  127.0.0.1:8899  ，外部window 或者其他服务器进行 访问，能通吗？<br>不能    127.0.0.1只可本机访问</p><p>8.netstat 一个服务时，显示  hostname:8899  ，外部window 或者其他服务器进行 访问，假如不通，我们应该要调整什么？<br>检查防火墙、安全组、/etc/httpd是否配置ip</p><p>9.rwx分别代表数字多少？  rwx–x-w- ,请问权限数字多少？<br>421    712</p><p>10.简述vi编辑文件的流程<br>i键进入编辑模式,插入,esc退出编辑模式,shift:进入尾行模式,wq保存退出</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.23早课</title>
      <link href="2018/10/26/2020-11-23-quannnxu-zao-ke/"/>
      <url>2018/10/26/2020-11-23-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.查看当前目录</p><p>2.隐藏文件、文件夹以什么开始，怎样查看</p><p>3.ls -l 等价于什么？</p><p>4.级联创建文件夹</p><p>5.创建文件哪几种方式</p><p>6.cp和mv谁快？ 能不能在cp 、mv过程，顺便修改名称呢？</p><p>7.查看文件的大小哪两种命令？</p><p>8.查看文件夹的大小的命令？</p><p>9.ll 命令执行完成后，文件、文件夹展示一长串信息，有哪些？</p><p>10.绝对路径和相对路径，区别是什么？</p><p>11.root的家目录在哪？普通用户的默认家目录在哪？</p><p>12.切换到用户的家目录，哪三种方式？</p><p>13.切换到 上一层目录 与 上一次目录，命令分别是什么？</p><p>14.清除屏幕信息</p><p>15.查看文件内容 哪几个命令？</p><p>16.实时查看文件内容，-f -F区别是什么？</p><p>17.谈谈你对  如何定位ERROR的 理解？</p><ol start="18"><li><blockquote><blockquote><blockquote><p>区别是什么？</p></blockquote></blockquote></blockquote></li></ol><p>19.设置变量  key= value  ,这样写对吗？如何引用呢？</p><p>20.全局环境变量文件在哪？ 个人环境变量文件(推荐哪个)在哪？如何生效？</p><p>1.查看当前目录<br>pwd</p><p>2.隐藏文件、文件夹以什么开始，怎样查看<br>以. 开头的；ls -a或ll -a</p><p>3.ls -l 等价于什么？<br>ll</p><p>4.级联创建文件夹<br>mkdir -p</p><p>5.创建文件哪几种方式<br>touch<br>vi</p><p>6.cp和mv谁快？ 能不能在cp 、mv过程，顺便修改名称呢？<br>mv 快<br>可以修改</p><p>7.查看文件的大小哪两种命令？<br>du -sh<br>ll -h</p><p>8.查看文件夹的大小的命令？<br>du -sh</p><p>9.ll 命令执行完成后，文件、文件夹展示一长串信息，有哪些？<br>权限，所属用户和用户组，创建/修改时间,文件大小<br>drwxrwxr-x 3 hadoop hadoop 4096 Nov 22 09:44 app</p><p>10.绝对路径和相对路径，区别是什么？<br>绝对路径以/开头<br>绝对路径可以精确引用，而相对路径则是模糊应用概念，只是在目标目录下找到引用</p><p>11.root的家目录在哪？普通用户的默认家目录在哪?<br>/root<br>/home/xxx</p><p>12.切换到用户的家目录，哪三种方式？<br>cd<br>cd /home/xx<br>cd ~</p><p>13.切换到 上一层目录 与 上一次目录，命令分别是什么？<br>cd  ..<br>cd -</p><p>14.清除屏幕信息<br>clear</p><p>15.查看文件内容 哪几个命令？<br>cat<br>less<br>more</p><p>16.实时查看文件内容，-f -F区别是什么？<br>-F 会一直try</p><p>17.谈谈你对  如何定位ERROR的 理解？<br>cat xx.log|grep -C 5 ERROR&gt;20201122.log</p><ol start="18"><li><blockquote><blockquote><blockquote><p>区别是什么？<br>覆盖</p></blockquote></blockquote><blockquote><p>追加</p></blockquote></blockquote></li></ol><p>19.设置变量  key= value  ,这样写对吗？如何引用呢？<br>不对 等号前后不能有空格<br>${key}</p><p>20.全局环境变量文件在哪？ 个人环境变量文件(推荐哪个)在哪？如何生效？<br>/etc/profile<br>~/.bashrc<br>source  ～/.bashrc</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.19早课</title>
      <link href="2018/10/25/2020-11-19-quannnxu-zao-ke/"/>
      <url>2018/10/25/2020-11-19-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.which  whereis命令区别</p><p>2.su  sudo命令区别</p><p>3.全局搜索</p><p>4.压缩解压  哪两个命令</p><p>5.vi三种模式是什么。如何切换</p><p>6.谈谈哪些命令是操作文件夹的</p><p>7.谈谈哪些命令是操作文件的</p><p>8.哪两个命令是 大R</p><p>9.谈谈左连接理解</p><p>10.mysql的 full join 是怎样实现的，在数据质量的 数据量校验 发挥作用，是否理解？</p><p>1.which  whereis命令区别<br>which 查看可执行文件的位置。  whereis 查看文件的位置。</p><p>2.su  sudo命令区别<br>u切换用户，sudo临时获取root权限</p><p>3.全局搜索<br>find / -name ‘<em>xxx</em>‘</p><p>4.压缩解压  哪两个命令<br>tar -zcvf 压缩<br>tar -zxvf 解压</p><p>5.vi三种模式是什么。如何切换<br>编辑模式    –&gt; i键<br>命令行模式    –&gt; esc<br>尾行模式    –&gt; shift:</p><p>6.谈谈哪些命令是操作文件夹的<br>mkdir/find/mv等</p><p>7.谈谈哪些命令是操作文件的<br>vi/cp/mv/cat/less/more/tail/echo/touch等</p><p>8.哪两个命令是 大R<br>chmod    chown</p><p>9.谈谈左连接理解<br>left join （左连接）：返回包括左表中的所有记录和右表中连接字段相等的记录。</p><p>10.mysql的 full join 是怎样实现的，在数据质量的 数据量校验 发挥作用，是否理解？<br>select * from A a<br>left join B b on a.id=b.id<br>union<br>select * from A a<br>right join B b on a.id=b.id</p><p>full join后的数据量和目标表、源表的数据量一致</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.18早课</title>
      <link href="2018/10/23/2020-11-18-quannnxu-zao-ke/"/>
      <url>2018/10/23/2020-11-18-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.which 命令作用是什么<br>which 　　查看可执行文件的位置。</p><ol start="2"><li>whereis  xxx命令<br>whereis 　查看文件的位置</li></ol><p>3.简述vi清空文件的步骤<br>先进入编辑模式,gg跳转首行,dG清空当前及以下行</p><p>4.谈谈  权限 相关的<br>r-4–&gt;读权限,w-2–&gt;写权限,x-1–&gt;执行权限<br>chmod 修改权限<br>chown 修改所属组</p><p>5.谈谈你对命令帮助的<br>–help    查看参数和可选参数,以及参数用法</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.17早课</title>
      <link href="2018/10/22/2020-11-17-quannnxu-zao-ke/"/>
      <url>2018/10/22/2020-11-17-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.查看硬盘命令，一般分为哪两种盘</p><p>2.查看内存命令，剩余内存怎么看</p><p>3.使用top命令，可以查看哪些数据</p><p>4.which  命令，命令出现的位置和命令使用时，其实在哪配置的</p><p>5.下载命令</p><p>6.上传命令</p><p>7.tar压缩解压 </p><p>8.定时调度查看，编辑语法</p><p>9.set -u是万能的吗？假如K=  空，也是致命的?</p><p>10.rw-r-x-wx   权限数字多少</p><p>最好先自己思考一下,再看后面的答案.</p><p>1.查看硬盘命令，一般分为哪两种盘<br>df -h<br>分为系统盘和数据盘</p><p>2.查看内存命令，剩余内存怎么看<br>free -m<br>剩余空间:total-used/toatl<br>预留内存最好在15%左右</p><p>3.使用top命令，可以查看哪些数据<br>查看cpu使用情况<br>load average: 0.00, 0.01, 0.05        1m 5m 15m的平均负载<br>经验值:10 生产上尽量控制在10,否则服务器就认为卡<br>PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND<br> 1466 root      10 -10  131500  13332  10160 S   0.3  0.2   3:18.22 AliYunDun<br>    1 root      20   0   43384   3776   2576 S   0.0  0.0   0:01.34 systemd     </p><p>4.which  命令，命令出现的位置和命令使用时，其实在哪配置的<br>配置在环境变量里</p><p>5.下载命令<br>rz</p><p>6.上传命令<br>sz</p><p>7.tar压缩解压<br>tar -zcvf xxx.tar.gz ./*    压缩<br>tar -zxvf xxx.tar.gz -C /xxx/    解压(指定目录)</p><p>8.定时调度查看，编辑语法<br>crontab -e</p><ul><li><ul><li><ul><li><ul><li><ul><li>分 时 日 月 周</li></ul></li></ul></li></ul></li></ul></li></ul><p>9.set -u是万能的吗？假如K=  空，也是致命的?<br>不是万能的,变量不可赋空</p><p>10.rw-r-x-wx   权限数字多少<br>653</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.16早课</title>
      <link href="2018/10/21/2020-11-16-quannnxu-zao-ke/"/>
      <url>2018/10/21/2020-11-16-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.简述mysql安装流程<br>创建my.cnf<br>创建用户组及用户<br>配置环境变量<br>赋权限和用户组，切换用户mysqladmin，安装<br>配置服务及开机自启动<br>安装libaio及安装mysql的初始db<br>查看临时密码<br>启动<br>登录及修改用户密码<br>重启</p><p>2.MySQL默认配置文件在哪<br>/etc/my.cnf</p><p>3.建表规范谈谈看<br>create table ruozedata.rz(<br>id int(11)  not null auto_increment, –第一列必须是自增长id    </p><p>name varchar(255),<br>age  int(3),            –comment ‘xxx’  业务字段加上注释</p><p>create_user varchar(255),<br>create_time timestamp not null default current_timestamp,</p><p>update_user varchar(255),<br>update_time timestamp not null default current_timestamp on update current_timestamp,<br>primary key(id)  主键= 唯一约束 +not null非空约束<br>);</p><p>4.模糊查询第三个字符为P<br>select * from table where 字段 like ‘__p%’(_占位符)</p><p>5.合并去重  语法<br>select * from A<br>union<br>select * from B</p><p>6.mysql默认端口号多少<br>3306</p><p>7.测试服务通不通  怎么办<br>先ping ip<br>再telnet ip port</p><p>8.常用错误有哪些<br>Command not found<br>permission defined<br>connect refused</p><p>9.命令找不到  怎么办<br>检查安装包安装了没有<br>环境变量是否配置</p><p>10.高危命令有哪些<br> rm -rf; &gt;; kill -9</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.13早课</title>
      <link href="2018/10/19/2020-11-13-quannnxu-zao-ke/"/>
      <url>2018/10/19/2020-11-13-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.全局环境变量在哪，怎么生效<br>/etc/profile    source或者. 生效</p><p>2.个人环境变量文件 默认在哪？ 一般使用哪个？ 个人环境变量文件一定在默认位置吗？（一般存放在当前用户的家目录）<br><del>/.bash_profile 和</del>/.bashrc位置<br>一般使用/.bashrc</p><p>3.声明环境变量  必须类似这样  export  JAVA_HOME = /usr/java/jdk1.8，是否有问题？<br>exprot JAVA_HOME=/usr/java/jdk1.8<br>export PATH=$JAVA_HOME/bin:$PATH【推荐】<br>=前后不能有空格</p><p>4.在环境变量文件 PATH=$JAVA_HOME/bin:$PATH    这样写法一般在最后，是声明JAVA的bin文件夹的可执行文件 java，这样敲命令 java，就能够找到</p><p>5.发现用户无法登录或者 无法执行命令 ，有可能什么文件导致？<br>/etc/passwd<br>文件中用户的最后一段为 /bin/false<br>mysqladmin:x:514:101::/usr/local/mysql:/bin/bash</p><p>6.同事给你交接服务器，就简单交接，请问拿到服务器，一般操作什么？<br>查看一下常规操作有哪些，搜索一下相应的软件包在什么地方<br>find / -name ‘<em>xxx</em>‘ 查看部署内容和位置<br>history</p><p>7.高危命令哪些<br>rm -fr</p><blockquote></blockquote><p>kill -9 (今天)</p><p>在我看来 生产上操作 修改 尤其不是自己掌管的服务配置文件，是不是高危？要不要提前备份呢？<br>是高危,需要备份</p><p>8.在生产上常见错误是什么 ，当前讲了两个<br>commond not found和permission defined</p><p>9.sudo  su 分别做什么<br>sudo su - jepson  这样执行是否OK？<br>sudo普通用户临时切换root的权限，su切换用户<br>可以执行</p><p>10.命令帮助是否会看，看什么？<br>–help查看[可选项option]和必填项以及后面的英文解释,【】里可选参数</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.12早课</title>
      <link href="2018/10/18/2020-11-12-quannnxu-zao-ke/"/>
      <url>2018/10/18/2020-11-12-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.经典错误1是什么  大概什么导致<br>command not found<br>要不就是没有安装，yum安装或者tar解压安装<br>要不就是安装好了，没有配置环境变量  到PATH里配置</p><p>2.经典错误2是什么  大概什么导致<br>permission denied<br>首先分析出 你去访问是什么用户===》 文件或者文件夹是什么用户对应什么权限、什么用户组对应什么权限、其他用户对应什么权限，一一比对<br>是否可以满足访问的需求：如  读、写、执行<br>如果不满足，则：<br>2.1 修改对应权限 chmod<br>2.2 修改所属用户或者用户组  chown<br>2.3 切换对应的 所属用户来操作（一般所属的用户的权限是 rwx）</p><ol start="3"><li>rwx分别代表什么<br>读 写 执行 421</li></ol><p>4.r–r—-x  数字多少<br>441</p><p>5.切换用户命令是什么，带-  有什么作用<br>su    切换该用户并执行该用户的环境变量</p><p>6.临时最大权限的命令是什么，该怎么配置？<br>sudo   编辑/etc/sudoers  添加ruozedata(用户名)  ALL=(ALL)       ALL</p><p>7.vi 简述编辑流程的快捷键<br>i    –&gt;编辑模式<br>编辑内容<br>esc    –&gt;退出编辑模式<br>shift+:    –&gt;尾行模式<br>wq    –&gt;保存退出</p><p>9.vi清空文件内容，最关键的两个命令是什么<br>gg dG        gg(跳转到首行首字)    Dg(删除当前行及以下所有行)</p><p>10.高危命令哪几个？<br>rm -rf ,&gt;</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础03</title>
      <link href="2018/10/17/2020-11-13-quannnxu-linux3/"/>
      <url>2018/10/17/2020-11-13-quannnxu-linux3/</url>
      
        <content type="html"><![CDATA[<h2 id="1-查看磁盘空间"><a href="#1-查看磁盘空间" class="headerlink" title="1. 查看磁盘空间"></a>1. 查看磁盘空间</h2><p>[root@warehouse001 ~]# df -h<br>Filesystem      Size  Used Avail Use% Mounted on<br>/dev/vda1        40G   11G   27G  29% /<br>devtmpfs        3.7G     0  3.7G   0% /dev<br>tmpfs           3.7G     0  3.7G   0% /dev/shm<br>tmpfs           3.7G  496K  3.7G   1% /run<br>tmpfs           3.7G     0  3.7G   0% /sys/fs/cgroup<br>tmpfs           756M     0  756M   0% /run/user/0<br>[root@warehouse001 ~]# </p><p>/dev/vda1        40G   11G   27G  29% /    系统盘</p><p>/dev/vdb1        2T    0            2T    0%    /data01        数据盘</p><h3 id="2-查看内存"><a href="#2-查看内存" class="headerlink" title="2. 查看内存"></a>2. 查看内存</h3><p>free -m<br>[root@warehouse001 ~]# free -m<br>              total        used        free      shared  buff/cache   available<br>Mem:    7552         800         162           0        6590        6442<br>Swap:        0           0           0</p><p>剩余空间:total-used/toatl</p><p>预留内存最好在15%左右</p><p>swap    因为内存不够,使用部分磁盘空间来充当内存使用,虽然可以解决内存紧缺的问题,但是效率不高.尤其大数据,swap哪怕设置了大小,也尽量设置惰性使用.</p><h3 id="3-机器负载"><a href="#3-机器负载" class="headerlink" title="3.机器负载"></a>3.机器负载</h3><p>load average: 0.00, 0.01, 0.05<br>                1m    5m        15m    </p><pre><code>经验值:10 生产上尽量控制在10,否则服务器就认为卡</code></pre><p>a.计算程序hivesql,spark,flink密集计算是不是要调优<br>b.是不是被挖矿了<br>6240 root      10 -10  131208  13072  10192 S   1.0  0.2   9:18.02 AliYunDun<br>                                                                                 cpu<br>查看负载和cpu<br>c.硬件问题    内存条损坏    最后一招,万能重启,检测是不是硬件问题</p><h3 id="4-安装"><a href="#4-安装" class="headerlink" title="4.安装"></a>4.安装</h3><p>yum install</p><p>yum remove</p><p>yum search xxx 搜索应用</p><h3 id="5-查看进程"><a href="#5-查看进程" class="headerlink" title="5.查看进程"></a>5.查看进程</h3><p>ps -ef | grep xxx(进程名称)<br>[root@warehouse001 ~]# ps -ef | grep httpd<br>root     11615 11102  0 16:02 pts/1    00:00:00 grep –color=auto httpd<br>            pid      父id</p><h3 id="6-端口号"><a href="#6-端口号" class="headerlink" title="6.端口号"></a>6.端口号</h3><p>netstat -nlp | grep 0<br>tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      3845/sshd<br>总结:<br>a.有进程pid,不一定有端口号<br>b.服务的通信交流,其实就是要ip+端口号(交互)<br>如:那个机器上运行xxx服务,打开web页面<br>ip<br>ps -ef | grep xxx 找到pid</p><p>netstat -nlp | grep pid</p><p><a href="https://139.224.129.11:80/">https://139.224.129.11:80</a></p><p>windows:提前安装telnet客户端,控制面板卸载程序里面打开启用或关闭windows功能勾选telnet选项</p><p>[root@warehouse001 ~]# telnet 139.224.129.11 22<br>Trying 139.224.129.11…<br>Connected to 139.224.129.11.<br>Escape character is ‘^]’.<br>SSH-2.0-OpenSSH_7.4</p><p>先ping ip再telnet ip+port</p><h3 id="7-卸载"><a href="#7-卸载" class="headerlink" title="7.卸载"></a>7.卸载</h3><p>yum remove<br>–nodeps                         do not verify package dependencies<br>不检查包的依赖性</p><p>connection refused    连接拒绝<br>connection time out</p><p>杀死进程<br>kill -9 pid 【高危命令】<br>误杀造成生产事故<br>kill -9 $(pgrep -f 匹配字符) 全局杀</p><p>[root@warehouse001 ~]# netstat -nlp | grep ssh<br>tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 :::22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 139.224.129.11:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>22端口号服务器可以对外</p><p>tcp        0      0 localhost:22              0.0.0.0:*               LISTEN      3845/sshd  </p><p>tcp        0      0 127.0.0.1:22              0.0.0.0:*               LISTEN      3845/sshd </p><p>localhost,127.0.0.1代表本机,外部不可访问</p><p>在/etc/httpd里面配置</p><h3 id="8-下载wget"><a href="#8-下载wget" class="headerlink" title="8.下载wget"></a>8.下载wget</h3><h3 id="9-压缩和解压"><a href="#9-压缩和解压" class="headerlink" title="9.压缩和解压"></a>9.压缩和解压</h3><p>zip -r 报名.zip 压缩目录<br>unzip 包名</p><p>tar -czvf xxx.tar.gz xxx/*<br>tar -xzvf xxx.tar.gz</p><h3 id="10-command-not-found"><a href="#10-command-not-found" class="headerlink" title="10.command not found"></a>10.command not found</h3><p>没有安装<br>没有配置环境变量</p><p>[root@warehouse001 ~]# which java1<br>/usr/bin/which: no java1 in (/usr/java/jdk1.8.0_45/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)<br>/etc/profile<br>exprot JAVA_HOME=/XXX<br>export PATH=$JAVA_HOME/bin:$PATH【推荐】</p><h3 id="11-定时"><a href="#11-定时" class="headerlink" title="11.定时"></a>11.定时</h3><p>crontab -e    编辑一个定时器文件</p><p>`* * * * * sleep 10s;date &gt;&gt; /root/ruoze.log</p><p>分 时 日 月 周(最低粒度是分)<br>每隔10s打印一次</p><pre class=" language-shell"><code class="language-shell">#/bin/bashset -u for ((i=1;i<=6;i++));do    date    sleep 10sdone</code></pre><p>`* * * * *  z10s.sh &gt;&gt; /root/ruoze.log</p><h3 id="12-后台执行脚本"><a href="#12-后台执行脚本" class="headerlink" title="12.后台执行脚本"></a>12.后台执行脚本</h3><p>nohub … &amp; 后台执行<br>nohub /root/ruoze.sh &gt;&gt; ruoze.log 2&gt;&amp;1 &amp;</p><p>rm -rf </p><p>#!/bin/bash<br>set -u</p><p>row=0<br>#arry=(t1 t2)<br>arry=(t1)</p><p>while(( ${row} &lt;2))<br>do<br>    tablename=$(arry[${row}])<br>    echo ${tablename}</p><pre><code>row=$(($&#123;row&#125;+1))</code></pre><p>done</p><p>拿到别人的脚本千万不要太自信,千万不要直接在生产操作<br>假如修改先去开发环境测试验证,不要在生产环境随意执行<br>set -u 只会在参数不存在时报错,参数为空时不会报错</p><p>生产一定要有敬畏之心 宁愿少做也不要做错 做之前必须开发环境走一下 哪怕生产有临时变动操作,请及时知会领导,让领导复核</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.11早课</title>
      <link href="2018/10/16/2020-11-11-quannnxu-zao-ke/"/>
      <url>2018/10/16/2020-11-11-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.vi三种模式<br>命令行模式、编辑模式、最后行模式</p><p>2.高危命令有哪两个？<br>rm -rf，&gt;(覆盖)</p><p>3.实时查看 大小F 区别<br>-f   假如文件被移除 然后重命名 就无法再监控到文件<br>-F   假如文件被移除 然后重命名 会不断的retry尝试 去监控文件，直到监控到位</p><p>4.管道符 是什么 ，过滤是什么<br>管道符 |     一个命令的标准输出传送到另一个命令的标准输入中<br>过滤 grep</p><p>5.别名语法是什么<br>alias</p><p>6.个人的  用谁   .bash_profile    .bashrc<br>用.bashrc</p><p>7.tab按一次 ，2次分别什么意思<br>tab键一次，只有1个 命令自动补全<br>按二次，会把当前匹配到的 所有 打印出来，再挑选</p><p>8.history -c 是干什么<br>清空历史命令(堡垒机无效哦)</p><p>9.chmod -R 777 /  这个命令生产上是否允许执行？<br>不允许,会把根目录下所有文件所有用户都可以读写执了</p><p>10.除了ftp sftp上传下载工具，还有什么命令？<br>rz(上传) sz(下载)<br>默认系统不自带，安装命令是<br>sudo yum install lrzsz -y</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11.10早课</title>
      <link href="2018/10/15/2020-11-10-quannnxu-zao-ke/"/>
      <url>2018/10/15/2020-11-10-quannnxu-zao-ke/</url>
      
        <content type="html"><![CDATA[<p>1.隐藏文件文件夹怎么标识，如何查看<br>隐藏文件以.开头        ls -a或者ll -a</p><p>2.级联创建文件夹参数是什么<br>-p</p><p>3.移动复制什么区别，是否可以改名<br>复制最终产生两份文件    速度慢<br>移动最终只有一份文件    速度快<br>都可以改名</p><p>4.rwx分别代表数字多少<br>r-&gt;4 w-&gt;2 x-&gt;1</p><p>5.rwxr–r-x  数字多少<br>rwx -&gt;7     r– -&gt;4     r-x -&gt;5<br>所以就是745</p><p>6.如上 三组，分别代表什么<br>所属用户,所属用户组其他成员,其他用户组成员</p><p>7.查看命令帮助会不?  要看什么<br>命令 –help，看usage用法还有option选项<br>【】为可选参数</p><p>8.发现一个用户登录不上或者无法执行命令  是什么文件问题<br>/etc/passwd</p><p>9.切换用户 带  -，标识什么<br>切换该用户并执行该用户的环境变量</p><p>10.临时获取root权限 ，会配置吗？ 执行要加什么？<br>使用root用户在/etc/sudoers 给用户加上ALL，如:ruozedata  ALL=(ALL)       ALL<br>执行加sudo</p>]]></content>
      
      
      <categories>
          
          <category> linux练习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux练习题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础02</title>
      <link href="2018/10/15/2020-11-08-quannnxu-linux2/"/>
      <url>2018/10/15/2020-11-08-quannnxu-linux2/</url>
      
        <content type="html"><![CDATA[<h2 id="1-rm-删除"><a href="#1-rm-删除" class="headerlink" title="1.rm 删除"></a>1.rm 删除</h2><pre class=" language-shell"><code class="language-shell">rm -rf  #强制删除-f #直接不提示 直接删除</code></pre><p>场景:</p><p>LOG_PATH=””</p><p>业务逻辑判断 赋值 /xxxx/yyyy</p><p>​    漏了一种</p><p>rm -rf ${LOG_PATH}$/*  ==&gt; rm -rf /*</p><p>如何避免:</p><p>每次删除之前,都判断${LOG_PATH}$目录是否存在</p><p>set -u参数    在脚本的第一行#!bin/bash,在第二行写(赋值为空报错)</p><h2 id="2-添加用户"><a href="#2-添加用户" class="headerlink" title="2.添加用户"></a>2.添加用户</h2><p>[root@hadoop01 ~]# useradd quanxu<br>[root@hadoop01 ~]# id quanxu<br>uid=502(quanxu) gid=503(quanxu) 组=503(quanxu)</p><pre class=" language-shell"><code class="language-shell">cat /etc/passwd    #查看密码cat /etc/group    #查看用户组</code></pre><p>切换用户</p><pre class=" language-shell"><code class="language-shell">su - quanxu</code></pre><p>删除用户</p><pre class=" language-shell"><code class="language-shell">userdel quanxu</code></pre><p>删除用户,那么该用户所属的用户组加入没有其他用户则一起删除;否则有其他用户则不能删除</p><p>[root@hadoop01 quanxu]# rm -f /home/quanxu/.bash*</p><p>[root@hadoop01 quanxu]# su - quanxu<br>-bash-4.1$ </p><p>当前用户的所属家目录的个人环境变量不存在</p><p>一个用户默认创建的家目录是在/home/用户名,但是后期可以修改</p><pre class=" language-shell"><code class="language-shell">usermod -d /tmp #单独使用时，只是把保存在/etc/passwd这个配置文件当中的源目录名改成指定的新目录名，并不会把源目录下的内容移动到新目录下，如果要把源目录下的内容移动新目录下，则要和-m选项一起使用，才会把源目录下的内容移动到新目录下。</code></pre><pre class=" language-shell"><code class="language-shell">usermod -a -G dw qx  #添加用户qx  到组dw 里</code></pre><pre class=" language-shell"><code class="language-shell">usermod -g dw xx     #修改用户所属的群组</code></pre><pre class=" language-shell"><code class="language-shell">groupsmems -g xx -l #查看组的所有成员</code></pre><p>用户切换不了查看/etc/passwd文件</p><p>sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin #不可登录</p><p>quanxu:x:502:503::/home/quanxu:/bin/bash/false #禁止切换</p><pre class=" language-shell"><code class="language-shell">sudo #普通用户临时使用root的最大权限</code></pre><h2 id="4-权限"><a href="#4-权限" class="headerlink" title="4.权限"></a>4.权限</h2><pre class=" language-shell"><code class="language-shell">drwxr-xr-x.  22 root root  4096 7月  10 07:58 vard    rwx    r-x    r-x #第一个字符d代表文件夹 -文件#接下来三组的三个字母 分别是代表 读r 4,写w 2,执行x 1,-没有权限 0rwx:7        r-x:5        r-x :5#所属用户    所属组的成员    其他组的成员777 rwxrwxrwx755 rwxr-xr-x-rw-rw-r--. 1 quanxu quanxu   15 7月  11 02:29 quan.logchmod 640 quan.log    #所属成员可读可写,所属组其他成员可读,其他组成员不可读写执</code></pre><h2 id="5-大小"><a href="#5-大小" class="headerlink" title="5.大小"></a>5.大小</h2><p>文件大小:ll -h</p><p>文件或文件夹大小:du -sh</p><h2 id="6-搜索-find"><a href="#6-搜索-find" class="headerlink" title="6.搜索    find"></a>6.搜索    find</h2><p>find / -name ‘<em>hadoop</em>‘</p><p>history    查看历史操作</p><p>ps -ef    查看进程</p><h2 id="7-vi编辑"><a href="#7-vi编辑" class="headerlink" title="7.vi编辑"></a>7.vi编辑</h2><p>良好习惯:</p><p>vi 编辑生产配置文件</p><p>先cp conf conf20201108</p><p>粘贴的坑:</p><p>必须先进入编辑模式,否则第一行内容丢失,不完整</p><p>在命令行模式,常用的快捷方式:</p><pre class=" language-shell"><code class="language-shell">dd    删除当前行dG    删除当前行及以下所有行gg    跳转到第一行的第一个字母G    跳转到最后一行的第一个字母</code></pre><p>shift+$ 行尾</p><p>如何通过vi命令进行清空文件内容:</p><p>dd dG</p><p>i</p><p>复制粘贴</p><p>esc</p><p>shfit+:wq</p><pre class=" language-shell"><code class="language-shell">#尾行模式下set nu    #显示行号set nonu    #恢复正常</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础01</title>
      <link href="2018/10/13/2020-11-04-quannnxu-linux/"/>
      <url>2018/10/13/2020-11-04-quannnxu-linux/</url>
      
        <content type="html"><![CDATA[<p>一.Linux基础命令</p><p>pwd 查看当前路径</p><p>[root@hadoop01 ~]# pwd</p><pre class=" language-shell"><code class="language-shell">/root/ #代表根ls #查看ls #显示的文件夹 文件的名称ls -l #权限 用户用户组 时间 => ll</code></pre><pre class=" language-shell"><code class="language-shell">ls -l -a #显示隐藏文件#以.开头的文件或文件夹就是隐藏文件ls -l -h #仅仅查看文件的大小ls -l -r -t #按时间排序 => ls -lrtmkdir #创建文件夹mkdir 1 2 3 #并联创建文件夹mkdir -p 4/5/6 #串联创建文件夹cd #切换目录cd ../ #退回上层目录cd ../../  #退回上两层目录#回到当前用户家目录cd 回车cd ~cd - #回退到上一次目录</code></pre><pre class=" language-shell"><code class="language-shell">目录绝对目录  全路径相对目录  相对路径</code></pre><h2 id="1-clear-清理屏幕"><a href="#1-clear-清理屏幕" class="headerlink" title="1.clear 清理屏幕"></a>1.clear 清理屏幕</h2><h2 id="2-mv-始终是一份-快"><a href="#2-mv-始终是一份-快" class="headerlink" title="2.mv 始终是一份 快"></a>2.mv 始终是一份 快</h2><p>cp 两份 慢</p><h2 id="3-创建文件"><a href="#3-创建文件" class="headerlink" title="3.创建文件"></a>3.创建文件</h2><pre class=" language-shell"><code class="language-shell">#创建一个空文件 touch 1.logvi 2.log#i键        进入编辑模式#esc        退出编辑模式#shift+:     从命令行模式进入尾行模式#\> 覆盖        [高危命令]#\>> 追加</code></pre><h2 id="4-查看文件命令"><a href="#4-查看文件命令" class="headerlink" title="4.查看文件命令"></a>4.查看文件命令</h2><p>cat 文件内容一下全部显示</p><p>more 文件内容一页页往下 翻,按空格键,ctrl+b回退</p><p>less 一行一行滚动,上下左右键</p><h2 id="5-tail实时查看文件内容"><a href="#5-tail实时查看文件内容" class="headerlink" title="5.tail实时查看文件内容"></a>5.tail实时查看文件内容</h2><p>-f 假如文件移除 然后重命名 就无法再监控到</p><p>-F 文件删除 然后重命名 会不断的retry</p><p>tail -100f 1.log 查看后一百行</p><h2 id="6-log4j-规则-大小100M-保留10份"><a href="#6-log4j-规则-大小100M-保留10份" class="headerlink" title="6.log4j    规则:大小100M,保留10份"></a>6.log4j    规则:大小100M,保留10份</h2><p>erp.log<br>               mv erp.log erp.log1<br>               touch erp.log</p><pre><code>   erp.log  空的---》100m   erp.log1 100m        mv erp.log erp.log2            touch erp.log   erp.log  空的   erp.log1 100m   erp.log2 100m   log---&gt;flume--&gt;kafka</code></pre><p>文件内容特别多 如何快速定位到ERROR、关键词信息<br>cat CloudAgent.log | grep ERROR</p><p>cat CloudAgent.log | grep -A 10 ERROR 后10行<br>cat CloudAgent.log | grep -B 10 ERROR 前10行<br>cat CloudAgent.log | grep -C 10 ERROR 前后各10行 20行 【推荐】<br>| 管道符<br>grep过滤</p><p>如:</p><pre class=" language-shell"><code class="language-shell">cat CloudAgent.log | grep -C 20 ERROR > 20201107error.logmore 20201107error.log</code></pre><p>通过编辑去搜索<br>    vi xxx.log<br>    shift+:<br>    /ERROR 回车<br>    n键寻找 </p><p>将日志文件 下载到window电脑，进行搜索 定位 分析  【推荐】<br>坑: 假如CloudAgent.log 原文件很大，那么从生产下载到公司网络 是不是要走外网带宽10M的，<br>想问 会不会影响 公司服务？<br>建议： 假如下载大文件，业务高峰或者工作日白天 尽量不要做，非要做，那就【限速】（FTP设置）</p><h2 id="5-上传下载"><a href="#5-上传下载" class="headerlink" title="5.上传下载"></a>5.上传下载</h2><p>yum install -y lrzsz<br>sz xxx.log 下载 Linux–》window<br>rz 直接回车 相反的</p><h2 id="6-别名-alias"><a href="#6-别名-alias" class="headerlink" title="6.别名 alias"></a>6.别名 alias</h2><p>ls -l ==&gt;ll </p><h2 id="7-环境变量"><a href="#7-环境变量" class="headerlink" title="7.环境变量"></a>7.环境变量</h2><p>全局:/etc/profile</p><p>个人:~/.bash_profile</p><p>​        ~/.bashrc 【推荐】</p><p>加好后 source一下,在新的窗口生效</p><p>场景：<br>      ssh 远程执行B机器 启动服务命令 抛错， java command not found<br>          直接登录B机器 命令是找到的  which java找到</p><p>配置环境变量文件在 .bash_profile 是不正确的，<br>     应该配置在 .bashrc文件里。</p><h2 id="8-创建用户和设置密码"><a href="#8-创建用户和设置密码" class="headerlink" title="8.创建用户和设置密码"></a>8.创建用户和设置密码</h2><p>useradd jj    </p><p>passwd jj</p><h2 id="9-tab"><a href="#9-tab" class="headerlink" title="9.tab"></a>9.tab</h2><p>按一次 匹配只有一串的 就补齐这串<br>按2次 打印出匹配的所有字符</p><h2 id="10-历史命令-history"><a href="#10-历史命令-history" class="headerlink" title="10.历史命令 history"></a>10.历史命令 history</h2><p>alias ll=’rm -rf /*’</p><p>history -c  清空</p><p>堡垒机仍然会被记录</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java基础01</title>
      <link href="2018/08/13/java-day01/"/>
      <url>2018/08/13/java-day01/</url>
      
        <content type="html"><![CDATA[<p>JDK包含JRE和java工具</p><h4 id="1-hello-world解析"><a href="#1-hello-world解析" class="headerlink" title="1.hello world解析"></a>1.hello world解析</h4><h4 id="2-语言基础"><a href="#2-语言基础" class="headerlink" title="2.语言基础"></a>2.语言基础</h4><h5 id="1-变量"><a href="#1-变量" class="headerlink" title="1.变量"></a>1.变量</h5><p>​    int a=1;        在内存中申请一个空间来储存变量</p><p>​        A.类型    B.名称    C.这个区域的值可以不断变化(a=2)        </p><p>什么时候定义变量?</p><p>整数类型:  byte,short,int,long</p><p>浮点:float,double</p><p>字符:char</p><p>布尔</p><p>class,interface,[]</p><pre class=" language-javvva"><code class="language-javvva">package com.company;public class Main &#123;   /*   1.程序入口,个类有了一个main,他就可以独立运行   2.这个main方法是被谁调用的?JVM   3.javac编译之后生成一个.class的文件.java Main.class    */   public static void main(String[] args) &#123;   // write your code//        byte b1,b2,b3;//        //byte取值范围?类型提升从byte提升到int.//        b1=5;//        b2=3;//        b3=b1+b2;     编译报错,原因b3超出byte的字节长度,-128~127       byte b1,b2;       int b3;       b1=5;       b2=3;       b3=b1+b2;//        byte b;//        b=5+111000;     超出长度       //int i=3;       System.out.println(b3);   &#125;&#125;</code></pre><h5 id="2-常量"><a href="#2-常量" class="headerlink" title="2.常量"></a>2.常量</h5><p>变量的特殊形式,它的值不变,关键字final,名称定义成大写</p><p><img src="https://i.loli.net/2020/11/29/gkPscImORNYtLCQ.png"></p><p>1.整数,小数</p><p>2.布尔</p><p>3.字符(串)</p><p>4.null</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
